Started by timer
[EnvInject] - Loading node environment variables.
Building remotely on testserver3 (BUILD-SANITY) in workspace /var/lib/jenkins/workspace/build_sanity
Extended Email Publisher is currently disabled in project settings
[build_sanity] $ /bin/sh -xe /tmp/hudson562420983290569818.sh
+ set +e
+ project_dir=P17-GA-2.3.0-NYNJ-e2e
+ mkdir P17-GA-2.3.0-NYNJ-e2e
mkdir: cannot create directory â€˜P17-GA-2.3.0-NYNJ-e2eâ€™: File exists
+ cd P17-GA-2.3.0-NYNJ-e2e
+ DIAMANTI_RPM=/var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/dcx.rpm
+ TEST_RPM=diamanti-test-pkg.tar.gz
+ rm -f e2e /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/dcx.rpm auto_tb7_inventory.json dctl e2e_param.json diamanti-test-pkg.tar.gz
+ export DCTL_CONFIG=/var/lib/jenkins/sanity/auto_tb7
+ DCTL_CONFIG=/var/lib/jenkins/sanity/auto_tb7
+ mkdir -p /var/lib/jenkins/sanity/auto_tb7
+ export KUBECONFIG=/var/lib/jenkins/sanity/auto_tb7/.dctl.d/kubeconfig
+ KUBECONFIG=/var/lib/jenkins/sanity/auto_tb7/.dctl.d/kubeconfig
+ '[' false = false ']'
++ wget -q -O - http://bldserv1:8080/job/Project17-GA-2.3.0/lastSuccessfulBuild/artifact/artifacts/rpm/
++ head -n 1
++ grep -Po 'diamanti-cx.*?rpm'
+ rpm=diamanti-cx-2.3.0-25.x86_64.rpm
+ wget --quiet http://bldserv1:8080/job/Project17-GA-2.3.0/lastSuccessfulBuild/artifact/artifacts/rpm/diamanti-cx-2.3.0-25.x86_64.rpm -O /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/dcx.rpm
+ wget --quiet http://bldserv1:8080/job/Project17-GA-2.3.0/lastSuccessfulBuild/artifact/artifacts/test/diamanti-test-pkg.tar.gz
+ tar xfz diamanti-test-pkg.tar.gz
+ cd diamanti-test-pkg/bin
+ export PATH=/var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin
+ PATH=/var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin
+ export INVENTORY=../qa_tb/auto_tb7_inventory.json
+ INVENTORY=../qa_tb/auto_tb7_inventory.json
++ rpm -qp --queryformat '%{VERSION}' /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/dcx.rpm
+ ver=2.3.0
++ rpm -qp --queryformat '%{RELEASE}' /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/dcx.rpm
+ rel=25
+ iter=0
++ cat /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/bin/version.txt
+ old_run='Sanity|Daily-23-0'
+ [[ Sanity|Daily-23-0 == *25* ]]
+ dailyskiplist='Network.PingWithDnsName|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.Multiple|PerfTier.CreateDeleteValidateQosWithCustomMaxBwLimit|PerfTier.NetworkThroughputValidationCustomPerfTierWithMaxBWLimit|DirectionalOnePort|StorageKubeSnapshot.CreateSnapshotAndLCV|Rbac.SecureAuthLinuxLDAP|Benchmarking.WriteThroughputRemoteVolumes'
+ weeklyskiplist='Nvme|ReplicationController.NetworkPodEvacuation|RemoteStorage.IPMIBasedTargetReboot|MaxSize|ResyncTwoTargetReboot|RebootDestinationNodeDuringResync|RebootSourceNodeDuringResync|Pod.UnhealthyNode|Storage.StressWithFioPodsWithReboot|Qos|Rbac|Rbac.MultipleSecureAuthsWindowsLinux|Storage.StressWithFioPods|Snapshot.LimitTestMaxSnapshotsPerLocalVolumeOnANode|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.FunctionTestsRemoteLAuth|Rbac.FunctionTestsRemoteWAuth|Network.ValidateTrafficAfterIperfClientRcPodEvacuation|Nfs.NfsSrvrAndClientOnSameCluster|Quorum|Cluster.DataValidationOnQHMVolumesAfterNodeRemoveAndAdd|StorageKubeSnapshot.CreateSnapshotAndLCV
'
++ date +%a
+ DAYOFWEEK=Wed
+ '[' Wed == Fri ']'
+ '[' Wed == Sat ']'
+ focus='Sanity|Daily'
+ skiplist='Network.PingWithDnsName|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.Multiple|PerfTier.CreateDeleteValidateQosWithCustomMaxBwLimit|PerfTier.NetworkThroughputValidationCustomPerfTierWithMaxBWLimit|DirectionalOnePort|StorageKubeSnapshot.CreateSnapshotAndLCV|Rbac.SecureAuthLinuxLDAP|Benchmarking.WriteThroughputRemoteVolumes'
+ failfast=true
+ echo 'Sanity|Daily-25-0'
+ rm -rf /var/lib/jenkins/.kube
+ tee console_ouput.txt
+ e2e --tb=../qa_tb/auto_tb7_inventory.json --ginkgo.failFast=true '--ginkgo.focus=Sanity|Daily' '--ginkgo.skip=Network.PingWithDnsName|StorageKubeSnapshot.PatternVerificationOnLCV|Rbac.Multiple|PerfTier.CreateDeleteValidateQosWithCustomMaxBwLimit|PerfTier.NetworkThroughputValidationCustomPerfTierWithMaxBWLimit|DirectionalOnePort|StorageKubeSnapshot.CreateSnapshotAndLCV|Rbac.SecureAuthLinuxLDAP|Benchmarking.WriteThroughputRemoteVolumes' --rpm=/var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/dcx.rpm --install-method=install --check-storage-resources --check-services-timestamp
Environment variable DIAMANTI_TEST_DIR is not set. Setting it to /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg
Default log file location : /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/logs/e2e_log2019-11-13_06-29-12.log
Default command log file location : /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/logs/e2e_command_log2019-11-13_06-29-12.log
    DEBUG: 2019/11/13 06:29:12 Removing leading and trailing spaces from Ipmi IP of all nodes.
    DEBUG: 2019/11/13 06:29:12 Check all nodes are ready to run test(s):
    DEBUG: 2019/11/13 06:29:12 172.16.6.153 is ready to run test
    DEBUG: 2019/11/13 06:29:12 172.16.6.154 is ready to run test
    DEBUG: 2019/11/13 06:29:12 172.16.6.155 is ready to run test
    DEBUG: 2019/11/13 06:29:29 Testbed type used is to get list of Tcs is: D10
Running Suite: DWS e2e Suite run 1 of 1
=======================================
Random Seed: [1m1573655352[0m - Will randomize all specs
Will run [1m143[0m of [1m416[0m specs

    DEBUG: 2019/11/13 06:29:29 Planning to run tests on following nodes :
    DEBUG: 2019/11/13 06:29:29 172.16.6.153
    DEBUG: 2019/11/13 06:29:29 172.16.6.154
    DEBUG: 2019/11/13 06:29:29 172.16.6.155
    DEBUG: 2019/11/13 06:29:29 Checking existance of loopback device(s) on all cluster nodes
    DEBUG: 2019/11/13 06:29:40 Saving cluster info of primary and secondary clusters: 
    DEBUG: 2019/11/13 06:29:41 Checking if cluster already exists 
    DEBUG: 2019/11/13 06:29:43 OS is centos. Package diamanti-cx is installed on 172.16.6.153
    DEBUG: 2019/11/13 06:29:44 Version of the old dctl : 2.3.0
    DEBUG: 2019/11/13 06:29:44 Version of the rpm which we are going to install: 2.3.0
    DEBUG: 2019/11/13 06:29:44 Method used :install. RPM path: /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/dcx.rpm
    DEBUG: 2019/11/13 06:29:44 Output from 172.16.6.153

CentOS Linux 7 (Core)
Kernel 3.10.0-957.el7.x86_64 on an x86_64

    DEBUG: 2019/11/13 06:29:59 [33m Uninstalling diamanti-cx-2.3.0-25.x86_64 from 172.16.6.153 [0m
    DEBUG: 2019/11/13 06:29:59 [33m Uninstalling diamanti-cx-2.3.0-25.x86_64 from 172.16.6.155 [0m
    DEBUG: 2019/11/13 06:29:59 [33m Uninstalling diamanti-cx-2.3.0-25.x86_64 from 172.16.6.154 [0m
diamanti: Stopping services on uninstall...
diamanti: Stopping services on uninstall...
diamanti: Stopping services on uninstall...
warning: /etc/kubernetes/controller-manager saved as /etc/kubernetes/controller-manager.rpmsave
warning: /etc/diamanti/prometheus/prometheus.yml saved as /etc/diamanti/prometheus/prometheus.yml.rpmsave
warning: /etc/diamanti/convoy.conf saved as /etc/diamanti/convoy.conf.rpmsave
diamanti: Cleaning up on uninstall...
warning: /etc/kubernetes/controller-manager saved as /etc/kubernetes/controller-manager.rpmsave
warning: /etc/diamanti/prometheus/prometheus.yml saved as /etc/diamanti/prometheus/prometheus.yml.rpmsave
warning: /etc/diamanti/convoy.conf saved as /etc/diamanti/convoy.conf.rpmsave
diamanti: Cleaning up on uninstall...
warning: /etc/kubernetes/controller-manager saved as /etc/kubernetes/controller-manager.rpmsave
warning: /etc/diamanti/prometheus/prometheus.yml saved as /etc/diamanti/prometheus/prometheus.yml.rpmsave
warning: /etc/diamanti/convoy.conf saved as /etc/diamanti/convoy.conf.rpmsave
diamanti: Cleaning up on uninstall...
Preparing...                                                            (100%)#                                 (100%)##                                (100%)###                               (100%)####                              (100%)#####                             (100%)######                            (100%)#######                           (100%)########                          (100%)#########                         (100%)##########                        (100%)###########                       (100%)############                      (100%)#############                     (100%)##############                    (100%)###############                   (100%)################                  (100%)#################                 (100%)##################                (100%)###################               (100%)####################              (100%)#####################             (100%)######################            (100%)#######################           (100%)########################          (100%)#########################         (100%)##########################        (100%)###########################       (100%)############################      (100%)#############################     (100%)##############################    (100%)###############################   (100%)################################  (100%)################################# (100%)################################# [100%]
Updating / installing...
   1:diamanti-cx-2.3.0-25                                               (  1%)#                                 (  4%)##                                (  7%)###                               ( 10%)####                              ( 13%)#####                             ( 16%)######                            ( 19%)#######                           ( 22%)########                          ( 25%)#########                         ( 28%)##########                        ( 31%)###########                       ( 34%)############                      ( 37%)#############                     ( 40%)##############                    ( 43%)###############                   ( 46%)################                  ( 49%)#################                 ( 51%)##################                ( 54%)###################               ( 57%)####################              ( 60%)#####################             ( 63%)######################            ( 66%)#######################           ( 69%)########################          ( 72%)#########################         ( 75%)##########################        ( 78%)###########################       ( 81%)############################      ( 84%)#############################     ( 87%)##############################    ( 90%)###############################   ( 93%)################################  ( 96%)################################# ( 99%)################################# [100%]
Setting up configuration for firmware installation
Installing Firmware component 1 
Firmware component 1 already up-to-date
Installing Firmware component 2 
Firmware component 2 already up-to-date
Please powercycle for changes to take effect
    DEBUG: 2019/11/13 06:33:19 Doing sync on 172.16.6.155
    DEBUG: 2019/11/13 06:33:20 Doing sync on 172.16.6.153
    DEBUG: 2019/11/13 06:33:21 Doing sync on 172.16.6.154

Output from  172.16.6.155
Preparing...                                                            (100%)#                                 (100%)##                                (100%)###                               (100%)####                              (100%)#####                             (100%)######                            (100%)#######                           (100%)########                          (100%)#########                         (100%)##########                        (100%)###########                       (100%)############                      (100%)#############                     (100%)##############                    (100%)###############                   (100%)################                  (100%)#################                 (100%)##################                (100%)###################               (100%)####################              (100%)#####################             (100%)######################            (100%)#######################           (100%)########################          (100%)#########################         (100%)##########################        (100%)###########################       (100%)############################      (100%)#############################     (100%)##############################    (100%)###############################   (100%)################################  (100%)################################# (100%)################################# [100%]
Updating / installing...
   1:diamanti-cx-2.3.0-25                                               (  1%)#                                 (  4%)##                                (  7%)###                               ( 10%)####                              ( 13%)#####                             ( 16%)######                            ( 19%)#######                           ( 22%)########                          ( 25%)#########                         ( 28%)##########                        ( 31%)###########                       ( 34%)############                      ( 37%)#############                     ( 40%)##############                    ( 43%)###############                   ( 46%)################                  ( 49%)#################                 ( 51%)##################                ( 54%)###################               ( 57%)####################              ( 60%)#####################             ( 63%)######################            ( 66%)#######################           ( 69%)########################          ( 72%)#########################         ( 75%)##########################        ( 78%)###########################       ( 81%)############################      ( 84%)#############################     ( 87%)##############################    ( 90%)###############################   ( 93%)################################  ( 96%)################################# ( 99%)################################# [100%]
Setting up configuration for firmware installation
Installing Firmware component 1 
Firmware component 1 already up-to-date
Installing Firmware component 2 
Firmware component 2 already up-to-date
Please powercycle for changes to take effect

CentOS Linux 7 (Core)
Kernel 3.10.0-957.el7.x86_64 on an x86_64


Output from  172.16.6.154
Preparing...                                                            (100%)#                                 (100%)##                                (100%)###                               (100%)####                              (100%)#####                             (100%)######                            (100%)#######                           (100%)########                          (100%)#########                         (100%)##########                        (100%)###########                       (100%)############                      (100%)#############                     (100%)##############                    (100%)###############                   (100%)################                  (100%)#################                 (100%)##################                (100%)###################               (100%)####################              (100%)#####################             (100%)######################            (100%)#######################           (100%)########################          (100%)#########################         (100%)##########################        (100%)###########################       (100%)############################      (100%)#############################     (100%)##############################    (100%)###############################   (100%)################################  (100%)################################# (100%)################################# [100%]
Updating / installing...
   1:diamanti-cx-2.3.0-25                                               (  1%)#                                 (  4%)##                                (  7%)###                               ( 10%)####                              ( 13%)#####                             ( 16%)######                            ( 19%)#######                           ( 22%)########                          ( 25%)#########                         ( 28%)##########                        ( 31%)###########                       ( 34%)############                      ( 37%)#############                     ( 40%)##############                    ( 43%)###############                   ( 46%)################                  ( 49%)#################                 ( 51%)##################                ( 54%)###################               ( 57%)####################              ( 60%)#####################             ( 63%)######################            ( 66%)#######################           ( 69%)########################          ( 72%)#########################         ( 75%)##########################        ( 78%)###########################       ( 81%)############################      ( 84%)#############################     ( 87%)##############################    ( 90%)###############################   ( 93%)################################  ( 96%)################################# ( 99%)################################# [100%]
Setting up configuration for firmware installation
Installing Firmware component 1 
Firmware component 1 already up-to-date
Installing Firmware component 2 
Firmware component 2 already up-to-date
Please powercycle for changes to take effect

CentOS Linux 7 (Core)
Kernel 3.10.0-957.el7.x86_64 on an x86_64

    DEBUG: 2019/11/13 06:33:22 Waiting for nodes to come up, will wait upto 800 seconds
...............................    DEBUG: 2019/11/13 06:38:44 Nodes are up, waiting for armada to start
......
    DEBUG: 2019/11/13 06:39:45 Old rpm verison was = 2.3.0
    DEBUG: 2019/11/13 06:39:45 Current rpm verison is = 2.3.0
    DEBUG: 2019/11/13 06:39:45 Starting tests
    DEBUG: 2019/11/13 06:39:45 Cluster Spec Node list is [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/13 06:39:45 Getting dns domain name
    DEBUG: 2019/11/13 06:39:45 FQDN : autotb7.eng.diamanti.com
    DEBUG: 2019/11/13 06:39:45 Generating certificates for the cluster: (Name: autotb7, VIP: 172.16.19.55, FQDN: autotb7.eng.diamanti.com)
    DEBUG: 2019/11/13 06:39:45 Clean up existing certs if any:
    DEBUG: 2019/11/13 06:39:45 Generate unique CA name with current date
    DEBUG: 2019/11/13 06:39:45 Integrate CA name in file
    DEBUG: 2019/11/13 06:39:45 Generate CA certs
    DEBUG: 2019/11/13 06:39:46 Create a CSR to generate a certificate using FQDN, VIP, Cluster Name for a server certs
    DEBUG: 2019/11/13 06:39:46 Generate server certificate:
    DEBUG: 2019/11/13 06:39:46 Getting CertificateAuthority from /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/ca.pem file
    DEBUG: 2019/11/13 06:39:46 Getting ServerCertificate from /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/server.pem file
    DEBUG: 2019/11/13 06:39:46 Getting ServerPrivateKey from /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/server-key.pem file
    DEBUG: 2019/11/13 06:39:46 Creating the cluster
    DEBUG: 2019/11/13 06:40:01 Please import "/var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/ca.pem" certificate to your client machine
    DEBUG: 2019/11/13 06:40:01 Sleeping for 60 sec
    DEBUG: 2019/11/13 06:41:01 Save cluster configuration: 
    DEBUG: 2019/11/13 06:41:02 Login to cluster
    DEBUG: 2019/11/13 06:41:02 Polling for cluster login for 300 seconds.
    DEBUG: 2019/11/13 06:41:03 Checking in a loop for cluster status
    DEBUG: 2019/11/13 06:41:03 Found '3' nodes
    DEBUG: 2019/11/13 06:41:03 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/13 06:41:03 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/13 06:41:03 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/13 06:41:03 Creating network default
    DEBUG: 2019/11/13 06:41:03 Creating network blue
    DEBUG: 2019/11/13 06:41:03 Add default tag to default network
    DEBUG: 2019/11/13 06:41:14 Labeled all nodes with node=node$

    DEBUG: 2019/11/13 06:41:14 Getting cluster ID
    DEBUG: 2019/11/13 06:41:14 Created test cluster: 709335e9-0623-11ea-a994-a4bf01194d67
    DEBUG: 2019/11/13 06:41:14 Deleting all LCVs, volumes, snapshots from previous cluster if any.
    DEBUG: 2019/11/13 06:41:14 Recording timestamp of all services on all nodes
    DEBUG: 2019/11/13 06:41:22 Overwritting e2e parameter : ExpectedBasicVnicUsageCount
    DEBUG: 2019/11/13 06:41:24 Checking if given pods are in Running state
    DEBUG: 2019/11/13 06:41:25 Checking if given pods are in Running state
    DEBUG: 2019/11/13 06:41:26 Checking if given pods are in Running state
    DEBUG: 2019/11/13 06:41:26 Checking if given pods are in Running state
    DEBUG: 2019/11/13 06:41:27 Checking if given pods are in Running state
    DEBUG: 2019/11/13 06:41:27 Updating inventory struct
    DEBUG: 2019/11/13 06:41:28 Creating storage classes
    DEBUG: 2019/11/13 06:41:39 rpm=diamanti-cx-2.3.0-25.x86_64
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.Basic Management Sanity QOS_Cli-1.0 QOS_Cli-1.7 QOS_Cli-1.8 QOS_Cli-1.9 QOS_Cli-2.0 QOS_Cli-2.1 QOS_Cli-2.2 QOS_Cli-4.0 QOS_Cli-4.1 QOS_Cli-4.2 Multizone[0m [90mBasic Perf-tier testcases[0m 
  [1mdelete the same perf-tier again[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:72[0m
[BeforeEach] Basic Perf-tier testcases
  /gocode/main/test/e2e/tests/perf-tier.go:36
    DEBUG: 2019/11/13 06:41:39 Login to cluster
    DEBUG: 2019/11/13 06:41:40 Checking basic Vnic usage
    DEBUG: 2019/11/13 06:41:40 Updating inventory struct
    DEBUG: 2019/11/13 06:41:41 Checking stale storage resources
    DEBUG: 2019/11/13 06:41:41 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 06:41:41 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:41:41 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:41:48 Creating storage classes
    DEBUG: 2019/11/13 06:41:58 START_TEST PerfTier.Basic
[It] delete the same perf-tier again
  /gocode/main/test/e2e/tests/perf-tier.go:72
    DEBUG: 2019/11/13 06:41:58 Create perf-tier.
    DEBUG: 2019/11/13 06:41:59 Delete the perf-tier.
    DEBUG: 2019/11/13 06:41:59 Try to delete the same perf-tier again.
[AfterEach] Basic Perf-tier testcases
  /gocode/main/test/e2e/tests/perf-tier.go:42
    DEBUG: 2019/11/13 06:41:59 END_TEST PerfTier.Basic Time-taken : 0.335107962
    DEBUG: 2019/11/13 06:41:59 Checking stale storage resources
    DEBUG: 2019/11/13 06:41:59 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:41:59 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:41:59 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.598 seconds][0m
PerfTier.Basic Management Sanity QOS_Cli-1.0 QOS_Cli-1.7 QOS_Cli-1.8 QOS_Cli-1.9 QOS_Cli-2.0 QOS_Cli-2.1 QOS_Cli-2.2 QOS_Cli-4.0 QOS_Cli-4.1 QOS_Cli-4.2 Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:26[0m
  Basic Perf-tier testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:27[0m
    delete the same perf-tier again
    [90m/gocode/main/test/e2e/tests/perf-tier.go:72[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mEndpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5[0m [90mEndpoint Basic testcases[0m 
  [1mCreate the endpoint with same name again[0m
  [37m/gocode/main/test/e2e/tests/endpoint.go:53[0m
[BeforeEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:31
    DEBUG: 2019/11/13 06:41:59 Login to cluster
    DEBUG: 2019/11/13 06:41:59 Checking basic Vnic usage
    DEBUG: 2019/11/13 06:41:59 Updating inventory struct
    DEBUG: 2019/11/13 06:42:00 Checking stale storage resources
    DEBUG: 2019/11/13 06:42:00 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:42:00 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 06:42:00 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:42:08 Creating storage classes
    DEBUG: 2019/11/13 06:42:18 START_TEST Endpoint.Basic
[It] Create the endpoint with same name again
  /gocode/main/test/e2e/tests/endpoint.go:53
    DEBUG: 2019/11/13 06:42:18 Create a endpoint.
    DEBUG: 2019/11/13 06:42:18 Try to create endpoint with the same name again.
    DEBUG: 2019/11/13 06:42:18 Endpoint create command failed: failed to run commmand 'dctl  -o json endpoint create ep1 -ns default -n blue -l custom-endpoint=true', status:&{{Status } {  0} Failure Endpoint "default/ep1" already exists AlreadyExists 0xc0001c41e0 409}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "Endpoint \"default/ep1\" already exists",
 "reason": "AlreadyExists",
 "details": {
  "name": "default/ep1",
  "kind": "Endpoint"
 },
 "code": 409
}



    DEBUG: 2019/11/13 06:42:18 Delete the endpoint.
[AfterEach] Endpoint Basic testcases
  /gocode/main/test/e2e/tests/endpoint.go:37
    DEBUG: 2019/11/13 06:42:18 END_TEST Endpoint.Basic Time-taken : 0.285318495
    DEBUG: 2019/11/13 06:42:18 Checking stale storage resources
    DEBUG: 2019/11/13 06:42:18 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:42:18 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:42:18 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.505 seconds][0m
Endpoint.Basic Daily N_Endpoint-1.4 N_Endpoint-1.5
[90m/gocode/main/test/e2e/tests/endpoint.go:23[0m
  Endpoint Basic testcases
  [90m/gocode/main/test/e2e/tests/endpoint.go:30[0m
    Create the endpoint with same name again
    [90m/gocode/main/test/e2e/tests/endpoint.go:53[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14[0m [90mCheck if a pod's IP is pingable from other pod[0m 
  [1mCheck if a pod's IP is pingable from other pod using private network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:869[0m
[BeforeEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:848
    DEBUG: 2019/11/13 06:42:18 START_TEST Network.PingBetweenTwoPods
    DEBUG: 2019/11/13 06:42:18 Login to cluster
    DEBUG: 2019/11/13 06:42:19 Checking basic Vnic usage
    DEBUG: 2019/11/13 06:42:19 Updating inventory struct
    DEBUG: 2019/11/13 06:42:20 Checking stale storage resources
    DEBUG: 2019/11/13 06:42:20 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:42:20 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:42:20 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 06:42:27 Creating storage classes
[It] Check if a pod's IP is pingable from other pod using private network
  /gocode/main/test/e2e/tests/network-pod.go:869
    DEBUG: 2019/11/13 06:42:37 Creating private network : blue
    DEBUG: 2019/11/13 06:42:37 Creating 2 pods of docker.io/redis:3.0.5 image with network : blue and qos : high
    DEBUG: 2019/11/13 06:42:43 IP address ( 172.16.180.4 ) of e2etest-pod-1 is between 172.16.180.4 and 172.16.180.253

    DEBUG: 2019/11/13 06:42:43 IP address ( 172.16.180.5 ) of e2etest-pod-2 is between 172.16.180.4 and 172.16.180.253

    DEBUG: 2019/11/13 06:42:43 Trying to ping the e2etest-pod-2's IP from pod e2etest-pod-1
    DEBUG: 2019/11/13 06:42:44 172.16.180.5 is pingable from pod e2etest-pod-1 (172.16.180.4)
    DEBUG: 2019/11/13 06:42:44 Deleting pods : 
    DEBUG: 2019/11/13 06:42:53 Deleting private network : blue
[AfterEach] Check if a pod's IP is pingable from other pod
  /gocode/main/test/e2e/tests/network-pod.go:857
    DEBUG: 2019/11/13 06:42:53 END_TEST Network.PingBetweenTwoPods Time-taken : 35.099127096
    DEBUG: 2019/11/13 06:42:53 Checking stale storage resources
    DEBUG: 2019/11/13 06:42:54 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:42:54 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:42:54 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:35.210 seconds][0m
Network.PingBetweenTwoPods Daily N_Basic-1.10 N_Basic-1.11 N_Basic-1.12 N_Basic-1.13 N_Basic-1.14
[90m/gocode/main/test/e2e/tests/network-pod.go:842[0m
  Check if a pod's IP is pingable from other pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:843[0m
    Check if a pod's IP is pingable from other pod using private network
    [90m/gocode/main/test/e2e/tests/network-pod.go:869[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.RebootTestFsckCheck Daily S_Reboot-2.3[0m [90mreboot test to check file system errors with fsck command on local storage volumes[0m 
  [1mreboot test with fsck check on local storage[0m
  [37m/gocode/main/test/e2e/tests/volume.go:1479[0m
[BeforeEach] reboot test to check file system errors with fsck command on local storage volumes
  /gocode/main/test/e2e/tests/volume.go:1465
    DEBUG: 2019/11/13 06:42:54 START_TEST LocalStorage.RebootTestFsckCheck
    DEBUG: 2019/11/13 06:42:54 Login to cluster
    DEBUG: 2019/11/13 06:42:54 Checking basic Vnic usage
    DEBUG: 2019/11/13 06:42:54 Updating inventory struct
    DEBUG: 2019/11/13 06:42:55 Checking stale storage resources
    DEBUG: 2019/11/13 06:42:55 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:42:55 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:42:55 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 06:43:03 Creating storage classes
[It] reboot test with fsck check on local storage
  /gocode/main/test/e2e/tests/volume.go:1479
    DEBUG: 2019/11/13 06:43:13 Creating 8 volumes of random sizes between 1GiB and 200GiB
    DEBUG: 2019/11/13 06:43:14 Attching volumes locally.
    DEBUG: 2019/11/13 06:43:53 Create XFS on odd no volumes and EXT4 on even no volumes.
    DEBUG: 2019/11/13 06:44:01 Volume2uuid_mapping: 
test-vol1 eba2bfcb-0623-11ea-97a7-a4bf01194d67
test-vol2 ebc15e7e-0623-11ea-97a7-a4bf01194d67
test-vol3 ebdf4001-0623-11ea-97a7-a4bf01194d67
test-vol4 ebfcb06b-0623-11ea-97a7-a4bf01194d67
test-vol5 ec19f1ba-0623-11ea-97a7-a4bf01194d67
test-vol6 ec37d424-0623-11ea-97a7-a4bf01194d67
test-vol7 ec56766d-0623-11ea-97a7-a4bf01194d67
test-vol8 ec73c741-0623-11ea-97a7-a4bf01194d67

    DEBUG: 2019/11/13 06:44:20 Running fio job on all the volumes
    DEBUG: 2019/11/13 06:44:20 FIO Command : sudo /usr/local/bin/fio --ioengine=sync --direct=1 --runtime=300 --time_based --iodepth=16 --group_reporting --blocksize_range=4K-1024K --rw=randwrite --disable_lat=1 --disable_clat=1 --disable_slat=1 --disable_bw_measurement=1 --name=job1 --filename=/mnt/test-vol1/file --name=job2 --filename=/mnt/test-vol2/file --name=job3 --filename=/mnt/test-vol3/file --name=job4 --filename=/mnt/test-vol4/file --name=job5 --filename=/mnt/test-vol5/file --name=job6 --filename=/mnt/test-vol6/file --name=job7 --filename=/mnt/test-vol7/file --name=job8 --filename=/mnt/test-vol8/file
    DEBUG: 2019/11/13 06:49:22 Getting cluster quorum nodes
    DEBUG: 2019/11/13 06:49:22 Powering OFF the node appserv54
    DEBUG: 2019/11/13 06:49:22 Node 172.16.6.154 took 0 seconds to power off
    DEBUG: 2019/11/13 06:49:22 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/13 06:49:22 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:49:31 Polling to check until node: appserv54 goes down
    DEBUG: 2019/11/13 06:50:41 Powering ON the node appserv54
    DEBUG: 2019/11/13 06:50:42 Node 172.16.6.154 took 1 seconds to power on
    DEBUG: 2019/11/13 06:50:42 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/13 06:50:42 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:51:01 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:51:18 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:51:35 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:51:52 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:52:09 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:52:26 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:52:43 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:53:00 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 06:53:04 appserv54 is pingable from local machine
    DEBUG: 2019/11/13 06:53:04 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/13 06:53:34 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/13 06:53:34 Found '3' nodes
    DEBUG: 2019/11/13 06:53:34 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/13 06:53:34 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/13 06:54:21 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/13 06:54:31 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/13 06:54:33 Getting cluster quorum nodes
    DEBUG: 2019/11/13 06:55:33 Updating inventory struct
    DEBUG: 2019/11/13 06:55:34 Doing fsck on all the volumes
    DEBUG: 2019/11/13 06:55:35 output : fsck from util-linux 2.23.2
/dev/nvme8n1: recovering journal
/dev/nvme8n1: clean, 12/67248 files, 39671/268800 blocks


    DEBUG: 2019/11/13 06:55:36 output : fsck from util-linux 2.23.2
/dev/nvme2n1: recovering journal
/dev/nvme2n1: clean, 12/353408 files, 201535/1411200 blocks


    DEBUG: 2019/11/13 06:55:36 output : fsck from util-linux 2.23.2
/dev/nvme7n1: recovering journal
/dev/nvme7n1: clean, 12/638976 files, 334007/2553600 blocks


    DEBUG: 2019/11/13 06:55:37 output : fsck from util-linux 2.23.2
/dev/nvme1n1: recovering journal
/dev/nvme1n1: clean, 12/925696 files, 470535/3696000 blocks


    DEBUG: 2019/11/13 06:55:37 output : fsck from util-linux 2.23.2
/dev/nvme5n1: recovering journal
/dev/nvme5n1: clean, 12/1210048 files, 604320/4838400 blocks


    DEBUG: 2019/11/13 06:55:38 output : fsck from util-linux 2.23.2
/dev/nvme3n1: recovering journal
/dev/nvme3n1: clean, 12/1496208 files, 735978/5980800 blocks


    DEBUG: 2019/11/13 06:55:38 output : fsck from util-linux 2.23.2
/dev/nvme6n1: recovering journal
/dev/nvme6n1: clean, 12/1782368 files, 868805/7123200 blocks


    DEBUG: 2019/11/13 06:55:39 output : fsck from util-linux 2.23.2
/dev/nvme4n1: recovering journal
/dev/nvme4n1: clean, 12/2072576 files, 1002557/8275200 blocks


    DEBUG: 2019/11/13 06:55:39 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/13 06:55:44 Volume2uuid_mapping: 
test-vol1 eba2bfcb-0623-11ea-97a7-a4bf01194d67
test-vol2 ebc15e7e-0623-11ea-97a7-a4bf01194d67
test-vol3 ebdf4001-0623-11ea-97a7-a4bf01194d67
test-vol4 ebfcb06b-0623-11ea-97a7-a4bf01194d67
test-vol5 ec19f1ba-0623-11ea-97a7-a4bf01194d67
test-vol6 ec37d424-0623-11ea-97a7-a4bf01194d67
test-vol7 ec56766d-0623-11ea-97a7-a4bf01194d67
test-vol8 ec73c741-0623-11ea-97a7-a4bf01194d67

    DEBUG: 2019/11/13 06:55:44 Volume to uuid mapping. After reboot: test-vol1 eba2bfcb-0623-11ea-97a7-a4bf01194d67
test-vol2 ebc15e7e-0623-11ea-97a7-a4bf01194d67
test-vol3 ebdf4001-0623-11ea-97a7-a4bf01194d67
test-vol4 ebfcb06b-0623-11ea-97a7-a4bf01194d67
test-vol5 ec19f1ba-0623-11ea-97a7-a4bf01194d67
test-vol6 ec37d424-0623-11ea-97a7-a4bf01194d67
test-vol7 ec56766d-0623-11ea-97a7-a4bf01194d67
test-vol8 ec73c741-0623-11ea-97a7-a4bf01194d67

    DEBUG: 2019/11/13 06:55:44 Comparing the UUID before and after reboot for all volumes
    DEBUG: 2019/11/13 06:55:44 Mounting all volumes
    DEBUG: 2019/11/13 06:55:49 Unmounting all volumes
    DEBUG: 2019/11/13 06:55:53 Detach & Delete all volumes
[AfterEach] reboot test to check file system errors with fsck command on local storage volumes
  /gocode/main/test/e2e/tests/volume.go:1474
    DEBUG: 2019/11/13 06:56:38 END_TEST LocalStorage.RebootTestFsckCheck Time-taken : 824.140443016
    DEBUG: 2019/11/13 06:56:38 Checking stale storage resources
    DEBUG: 2019/11/13 06:56:38 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:56:38 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:56:38 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:824.254 seconds][0m
LocalStorage.RebootTestFsckCheck Daily S_Reboot-2.3
[90m/gocode/main/test/e2e/tests/volume.go:1455[0m
  reboot test to check file system errors with fsck command on local storage volumes
  [90m/gocode/main/test/e2e/tests/volume.go:1458[0m
    reboot test with fsck check on local storage
    [90m/gocode/main/test/e2e/tests/volume.go:1479[0m
[90m------------------------------[0m
[0mHostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2[0m [90mCreate single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted[0m 
  [1mShould create multiple host-networks, check if endpoints got created, delete host network, check if endpoints deleted[0m
  [37m/gocode/main/test/e2e/tests/network.go:510[0m
[BeforeEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:499
    DEBUG: 2019/11/13 06:56:38 START_TEST HostNetwork.Basic
    DEBUG: 2019/11/13 06:56:38 Login to cluster
    DEBUG: 2019/11/13 06:56:38 Checking basic Vnic usage
    DEBUG: 2019/11/13 06:56:38 Updating inventory struct
    DEBUG: 2019/11/13 06:56:39 Checking stale storage resources
    DEBUG: 2019/11/13 06:56:39 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 06:56:39 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 06:56:39 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 06:56:47 Creating storage classes
[It] Should create multiple host-networks, check if endpoints got created, delete host network, check if endpoints deleted
  /gocode/main/test/e2e/tests/network.go:510
    DEBUG: 2019/11/13 06:57:00 Creating perf-tier : template
    DEBUG: 2019/11/13 06:57:00 Disabling helm feature
    DEBUG: 2019/11/13 06:57:01 Checking basic Vnic usage
    DEBUG: 2019/11/13 06:57:08 Updating non default network as host network
    DEBUG: 2019/11/13 06:57:08 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/13 06:57:08 Waiting for allocation of VF to endpoints of network : blue
    DEBUG: 2019/11/13 06:57:41 VF enp129s5f3d2 allocated to endpoint blue.appserv53 successfully
    DEBUG: 2019/11/13 06:57:41 VF enp129s1f4 allocated to endpoint blue.appserv54 successfully
    DEBUG: 2019/11/13 06:57:41 VF enp129s7f5 allocated to endpoint blue.appserv55 successfully
    DEBUG: 2019/11/13 06:57:41 Updating default network as host network
    DEBUG: 2019/11/13 06:57:41 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/13 06:57:41 Waiting for allocation of VF to endpoints of network : default
    DEBUG: 2019/11/13 06:57:42 VF enp129s3 allocated to endpoint default.appserv53 successfully
    DEBUG: 2019/11/13 06:57:42 VF enp129s7f7d2 allocated to endpoint default.appserv54 successfully
    DEBUG: 2019/11/13 06:57:42 VF enp129s5f2d2 allocated to endpoint default.appserv55 successfully
    DEBUG: 2019/11/13 06:57:43 Checking enpoint are provisioned and attached or not for hostnetwork : blue
    DEBUG: 2019/11/13 06:57:43 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/13 06:57:43 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/13 06:57:46 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/13 06:58:16 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/13 06:58:19 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/13 06:58:19 tx_bytes on host-network interface enp129s1f4 when data transfer starts is : 1875673321 bytes
    DEBUG: 2019/11/13 06:58:50 tx_bytes on host-network interface enp129s1f4 after waiting time is : 36708463213 bytes
    DEBUG: 2019/11/13 06:58:50 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/13 06:58:50 Deleting pods:
    DEBUG: 2019/11/13 06:58:50 Deleting pods : 
    DEBUG: 2019/11/13 06:58:57 Deleting services: 
    DEBUG: 2019/11/13 06:58:57 Deleting service(s)
    DEBUG: 2019/11/13 06:58:57 Checking enpoint are provisioned and attached or not for hostnetwork : default
    DEBUG: 2019/11/13 06:58:58 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/13 06:58:58 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/13 06:59:01 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/13 06:59:31 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/13 06:59:33 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/13 06:59:34 tx_bytes on host-network interface enp129s7f7d2 when data transfer starts is : 1877648945 bytes
    DEBUG: 2019/11/13 07:00:04 tx_bytes on host-network interface enp129s7f7d2 after waiting time is : 36261430799 bytes
    DEBUG: 2019/11/13 07:00:04 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/13 07:00:04 Deleting pods:
    DEBUG: 2019/11/13 07:00:04 Deleting pods : 
    DEBUG: 2019/11/13 07:00:17 Deleting services: 
    DEBUG: 2019/11/13 07:00:17 Deleting service(s)
    DEBUG: 2019/11/13 07:00:17 Deleting host-network blue
    DEBUG: 2019/11/13 07:00:20 Endpoint blue.appserv53 deleted successfully
    DEBUG: 2019/11/13 07:00:20 Endpoint blue.appserv54 deleted successfully
    DEBUG: 2019/11/13 07:00:21 Endpoint blue.appserv55 deleted successfully
    DEBUG: 2019/11/13 07:00:21 Endpoints created by host-network got deleted
    DEBUG: 2019/11/13 07:00:21 Deleting host-network default
    DEBUG: 2019/11/13 07:00:25 Endpoint default.appserv53 deleted successfully
    DEBUG: 2019/11/13 07:00:25 Endpoint default.appserv54 deleted successfully
    DEBUG: 2019/11/13 07:00:25 Endpoint default.appserv55 deleted successfully
    DEBUG: 2019/11/13 07:00:25 Endpoints created by host-network got deleted
    DEBUG: 2019/11/13 07:00:25 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:00:25 VNICS usage is : 0
    DEBUG: 2019/11/13 07:00:25 After deleting host networks, vnics released by host-network endpoints
    DEBUG: 2019/11/13 07:00:25 Creating networks deleted by this TC
    DEBUG: 2019/11/13 07:00:25 Deleting the perf-tier : template
    DEBUG: 2019/11/13 07:00:25 Enabling helm feature.
[AfterEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:505
    DEBUG: 2019/11/13 07:00:36 END_TEST HostNetwork.Basic Time-taken : 237.727883479
    DEBUG: 2019/11/13 07:00:36 Checking stale storage resources
    DEBUG: 2019/11/13 07:00:36 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:00:36 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:00:36 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:237.851 seconds][0m
HostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2
[90m/gocode/main/test/e2e/tests/network.go:490[0m
  Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  [90m/gocode/main/test/e2e/tests/network.go:491[0m
    Should create multiple host-networks, check if endpoints got created, delete host network, check if endpoints deleted
    [90m/gocode/main/test/e2e/tests/network.go:510[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.BasicPod Sanity N_Basic-2.4 N_Basic-2.0 Qos Multizone[0m [90mNetwork Basic tests with pods[0m 
  [1mNetwork Basic pod test with 8 pods[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:82[0m
[BeforeEach] Network Basic tests with pods
  /gocode/main/test/e2e/tests/network-pod.go:68
    DEBUG: 2019/11/13 07:00:36 START_TEST Network.BasicPod
    DEBUG: 2019/11/13 07:00:36 Login to cluster
    DEBUG: 2019/11/13 07:00:36 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:00:36 Updating inventory struct
    DEBUG: 2019/11/13 07:00:37 Checking stale storage resources
    DEBUG: 2019/11/13 07:00:37 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:00:37 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:00:37 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:00:45 Creating storage classes
[It] Network Basic pod test with 8 pods
  /gocode/main/test/e2e/tests/network-pod.go:82
    DEBUG: 2019/11/13 07:00:55 Pick up appserv53 as serverNode and appserv54 as clientNode
    DEBUG: 2019/11/13 07:00:55 Creating 8 pairs of iperf client-server pod.
    DEBUG: 2019/11/13 07:00:55 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/13 07:00:56 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/13 07:00:56 Creating iperf server pod: iperf-serverhigh2
    DEBUG: 2019/11/13 07:00:56 Creating service with name: iperf-serverhigh2
    DEBUG: 2019/11/13 07:00:56 Creating iperf server pod: iperf-serverhigh3
    DEBUG: 2019/11/13 07:00:56 Creating service with name: iperf-serverhigh3
    DEBUG: 2019/11/13 07:00:57 Creating iperf server pod: iperf-serverhigh4
    DEBUG: 2019/11/13 07:00:57 Creating service with name: iperf-serverhigh4
    DEBUG: 2019/11/13 07:00:57 Creating iperf server pod: iperf-serverhigh5
    DEBUG: 2019/11/13 07:00:57 Creating service with name: iperf-serverhigh5
    DEBUG: 2019/11/13 07:00:57 Creating iperf server pod: iperf-serverhigh6
    DEBUG: 2019/11/13 07:00:58 Creating service with name: iperf-serverhigh6
    DEBUG: 2019/11/13 07:00:58 Creating iperf server pod: iperf-serverhigh7
    DEBUG: 2019/11/13 07:00:58 Creating service with name: iperf-serverhigh7
    DEBUG: 2019/11/13 07:00:58 Creating iperf server pod: iperf-serverhigh8
    DEBUG: 2019/11/13 07:00:59 Creating service with name: iperf-serverhigh8
    DEBUG: 2019/11/13 07:01:29 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/13 07:01:29 Creating iperf Client pod: iperf-clienthigh2
    DEBUG: 2019/11/13 07:01:30 Creating iperf Client pod: iperf-clienthigh3
    DEBUG: 2019/11/13 07:01:30 Creating iperf Client pod: iperf-clienthigh4
    DEBUG: 2019/11/13 07:01:30 Creating iperf Client pod: iperf-clienthigh5
    DEBUG: 2019/11/13 07:01:31 Creating iperf Client pod: iperf-clienthigh6
    DEBUG: 2019/11/13 07:01:31 Creating iperf Client pod: iperf-clienthigh7
    DEBUG: 2019/11/13 07:01:31 Creating iperf Client pod: iperf-clienthigh8
    DEBUG: 2019/11/13 07:01:32 Checking if given pods are in Running state
    DEBUG: 2019/11/13 07:01:32 Checking if given pods are in Running state
    DEBUG: 2019/11/13 07:01:35 Validate resource reservation counts of VNICs, Bandwidth, Network in cluster status
    DEBUG: 2019/11/13 07:01:36 Cluster Node: appserv53. Used vnic: 8
    DEBUG: 2019/11/13 07:01:36 Cluster Node: appserv53. Used Bandwidth: 4000000000
    DEBUG: 2019/11/13 07:01:36 Node : appserv53, Actual bandwith : 4000000000, Expected bandwidth :4000000000
    DEBUG: 2019/11/13 07:01:36 Cluster Node: appserv54. Used vnic: 9
    DEBUG: 2019/11/13 07:01:37 Cluster Node: appserv54. Used Bandwidth: 4000000000
    DEBUG: 2019/11/13 07:01:37 Node : appserv54, Actual bandwith : 4000000000, Expected bandwidth :4000000000
    DEBUG: 2019/11/13 07:01:37 network :default, Ipcnt :16
    DEBUG: 2019/11/13 07:01:37 Validating distribution of pods on NIF IDs 0,2 on appserv54
    DEBUG: 2019/11/13 07:01:38 Validating distribution of pods on NIF IDs 0,2 on appserv53
    DEBUG: 2019/11/13 07:01:38 Validating if bandwidth is honored or not for server pods:
    DEBUG: 2019/11/13 07:01:38 Sleeping 180 seconds, before validating qos: 
    DEBUG: 2019/11/13 07:04:38 QoS honored for pod: iperf-serverhigh1
    DEBUG: 2019/11/13 07:04:39 QoS honored for pod: iperf-serverhigh2
    DEBUG: 2019/11/13 07:04:39 QoS honored for pod: iperf-serverhigh3
    DEBUG: 2019/11/13 07:04:39 QoS honored for pod: iperf-serverhigh4
    DEBUG: 2019/11/13 07:04:40 QoS honored for pod: iperf-serverhigh5
    DEBUG: 2019/11/13 07:04:40 QoS honored for pod: iperf-serverhigh6
    DEBUG: 2019/11/13 07:04:41 QoS honored for pod: iperf-serverhigh7
    DEBUG: 2019/11/13 07:04:41 QoS honored for pod: iperf-serverhigh8
    DEBUG: 2019/11/13 07:04:41 Validating if bandwidth is honored or not for client pods:
    DEBUG: 2019/11/13 07:04:42 QoS honored for pod: iperf-clienthigh1
    DEBUG: 2019/11/13 07:04:42 QoS honored for pod: iperf-clienthigh2
    DEBUG: 2019/11/13 07:04:43 QoS honored for pod: iperf-clienthigh3
    DEBUG: 2019/11/13 07:04:43 QoS honored for pod: iperf-clienthigh4
    DEBUG: 2019/11/13 07:04:44 QoS honored for pod: iperf-clienthigh5
    DEBUG: 2019/11/13 07:04:44 QoS honored for pod: iperf-clienthigh6
    DEBUG: 2019/11/13 07:04:45 QoS honored for pod: iperf-clienthigh7
    DEBUG: 2019/11/13 07:04:45 QoS honored for pod: iperf-clienthigh8
    DEBUG: 2019/11/13 07:04:45 Measuring throughput. It must be around 18G.
    DEBUG: 2019/11/13 07:04:45 Node: appserv53. Expected Throughput: 18000000000. RX Throughput: 17695098760. TX Throughput: 0
    WARN : 2019/11/13 07:05:50 BUG: Error: Failed to get expected network throughput on node: appserv53. Expected Throughput: 18000000000. RX Throughput: 17695098760. TX Throughput: 0
    DEBUG: 2019/11/13 07:05:50 Node: appserv54. Expected Throughput: 18000000000. RX Throughput: 0. TX Throughput: 18292343426
    DEBUG: 2019/11/13 07:05:50 Delete iperf client
    DEBUG: 2019/11/13 07:05:50 Deleting pods : 
    DEBUG: 2019/11/13 07:06:14 Delete iperf service
    DEBUG: 2019/11/13 07:06:15 Delete iperf servers
    DEBUG: 2019/11/13 07:06:15 Deleting pods : 
    DEBUG: 2019/11/13 07:07:10 Checking if vnics, network, bandwidth reservations are released 
    DEBUG: 2019/11/13 07:07:10 Cluster Node: appserv53. Used vnic: 0
    DEBUG: 2019/11/13 07:07:10 Cluster Node: appserv53. Used Bandwidth: 0
    DEBUG: 2019/11/13 07:07:10 Node : appserv53, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/13 07:07:10 Cluster Node: appserv54. Used vnic: 1
    DEBUG: 2019/11/13 07:07:11 Cluster Node: appserv54. Used Bandwidth: 0
    DEBUG: 2019/11/13 07:07:11 Node : appserv54, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/13 07:07:11 network :default, Ipcnt :0
[AfterEach] Network Basic tests with pods
  /gocode/main/test/e2e/tests/network-pod.go:77
    DEBUG: 2019/11/13 07:07:11 END_TEST Network.BasicPod Time-taken : 394.996230318
    DEBUG: 2019/11/13 07:07:11 Checking stale storage resources
    DEBUG: 2019/11/13 07:07:11 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:07:11 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:07:11 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:395.116 seconds][0m
Network.BasicPod Sanity N_Basic-2.4 N_Basic-2.0 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:62[0m
  Network Basic tests with pods
  [90m/gocode/main/test/e2e/tests/network-pod.go:63[0m
    Network Basic pod test with 8 pods
    [90m/gocode/main/test/e2e/tests/network-pod.go:82[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos template with min iops > max iops.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:204[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/13 07:07:11 Login to cluster
    DEBUG: 2019/11/13 07:07:11 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:07:11 Updating inventory struct
    DEBUG: 2019/11/13 07:07:12 Checking stale storage resources
    DEBUG: 2019/11/13 07:07:12 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:07:12 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:07:12 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:07:20 Creating storage classes
    DEBUG: 2019/11/13 07:07:32 START_TEST PerfTier.NegativeTests
[It] tries creating qos template with min iops > max iops.
  /gocode/main/test/e2e/tests/perf-tier.go:204
    DEBUG: 2019/11/13 07:07:32 Try creating qos template with min iops > max iops.
    ERROR: 2019/11/13 07:07:32  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 1G -i 50k -I 20k', status:&{{Status } {  0} Failure Performance tier validation failed, error: [spec.storage-iops: Invalid value: 20000: Maximum storage IOPS cannot be less than minimum storage IOPS] BadRequest <nil> 400}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "Performance tier validation failed, error: [spec.storage-iops: Invalid value: 20000: Maximum storage IOPS cannot be less than minimum storage IOPS]",
 "reason": "BadRequest",
 "code": 400
}



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/13 07:07:32 END_TEST PerfTier.NegativeTests Time-taken: 0.095204918
    DEBUG: 2019/11/13 07:07:32 Checking stale storage resources
    DEBUG: 2019/11/13 07:07:32 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:07:32 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:07:32 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:21.581 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos template with min iops > max iops.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:204[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mInvalid gateway test[0m
  [37m/gocode/main/test/e2e/tests/network.go:66[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:34
    DEBUG: 2019/11/13 07:07:32 START_TEST Network.NegativeTests
    DEBUG: 2019/11/13 07:07:32 Login to cluster
    DEBUG: 2019/11/13 07:07:33 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:07:33 Updating inventory struct
    DEBUG: 2019/11/13 07:07:34 Checking stale storage resources
    DEBUG: 2019/11/13 07:07:34 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:07:34 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:07:34 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:07:41 Creating storage classes
[It] Invalid gateway test
  /gocode/main/test/e2e/tests/network.go:66
    DEBUG: 2019/11/13 07:07:52 Try to create network with invalid gateway.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:47
    DEBUG: 2019/11/13 07:07:52 END_TEST Network.NegativeTests Time-taken : 19.231196849
    DEBUG: 2019/11/13 07:07:52 Checking stale storage resources
    DEBUG: 2019/11/13 07:07:52 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:07:52 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:07:52 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.343 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:25[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:26[0m
    Invalid gateway test
    [90m/gocode/main/test/e2e/tests/network.go:66[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.EditView Daily Rbac_Local_Basic-2.0[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view volume(s)[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:554[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:528
    DEBUG: 2019/11/13 07:07:52 START_TEST Rbac.EditView
    DEBUG: 2019/11/13 07:07:52 Login to cluster
    DEBUG: 2019/11/13 07:07:52 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:07:52 Updating inventory struct
    DEBUG: 2019/11/13 07:07:53 Checking stale storage resources
    DEBUG: 2019/11/13 07:07:53 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:07:53 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:07:53 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:08:01 Creating storage classes
    DEBUG: 2019/11/13 07:08:11 User Logout
[It] User can edit/view volume(s)
  /gocode/main/test/e2e/tests/rbac.go:554
    DEBUG: 2019/11/13 07:08:12 Creating group, user with role(s)
    DEBUG: 2019/11/13 07:08:12 Creating group jacksgroup with volume-edit role(s)
    DEBUG: 2019/11/13 07:08:12 Creating jack user in jacksgroup group
    DEBUG: 2019/11/13 07:08:13 Login as jack user
    DEBUG: 2019/11/13 07:08:13 Creating the volume with volume-edit role.
    DEBUG: 2019/11/13 07:08:13 Listing volumes
    DEBUG: 2019/11/13 07:08:13 Editing group role(s)
    DEBUG: 2019/11/13 07:08:14 Editing jacksgroup group with volume-view role(s)
    DEBUG: 2019/11/13 07:08:14 Login as jack user
    DEBUG: 2019/11/13 07:08:14 Listing volumes
    DEBUG: 2019/11/13 07:08:14 Creating the volume with volume-view role.
    DEBUG: 2019/11/13 07:08:15 Editing group role(s)
    DEBUG: 2019/11/13 07:08:15 Editing jacksgroup group with volume-edit role(s)
    DEBUG: 2019/11/13 07:08:15 Login as jack user
    DEBUG: 2019/11/13 07:08:16 Deleting volume
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:539
    DEBUG: 2019/11/13 07:08:37 User Logout
    DEBUG: 2019/11/13 07:08:38 END_TEST Rbac.EditView Time-taken : 46.138573244
    DEBUG: 2019/11/13 07:08:38 Checking stale storage resources
    DEBUG: 2019/11/13 07:08:38 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:08:38 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:08:38 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:46.251 seconds][0m
Rbac.EditView Daily Rbac_Local_Basic-2.0
[90m/gocode/main/test/e2e/tests/rbac.go:518[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:520[0m
    User can edit/view volume(s)
    [90m/gocode/main/test/e2e/tests/rbac.go:554[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.IperfPodsWithRestartPolicyNever Daily N_Basic-2.3 MutliZone[0m [90mCreating iperf pods with restart policy Never[0m 
  [1mCreate iperf client-server pairs with restart policy Never.[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:2414[0m
[BeforeEach] Creating iperf pods with restart policy Never
  /gocode/main/test/e2e/tests/network-pod.go:2402
    DEBUG: 2019/11/13 07:08:38 START_TEST Network.IperfPodsWithRestartPolicyNever
    DEBUG: 2019/11/13 07:08:38 Login to cluster
    DEBUG: 2019/11/13 07:08:38 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:08:39 Updating inventory struct
    DEBUG: 2019/11/13 07:08:39 Checking stale storage resources
    DEBUG: 2019/11/13 07:08:39 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:08:39 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:08:39 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:08:47 Creating storage classes
[It] Create iperf client-server pairs with restart policy Never.
  /gocode/main/test/e2e/tests/network-pod.go:2414
    DEBUG: 2019/11/13 07:08:57 Creating 8 pairs of iperf client-server pod.
    DEBUG: 2019/11/13 07:08:57 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/13 07:09:00 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/13 07:09:00 Creating iperf server pod: iperf-serverhigh2
    DEBUG: 2019/11/13 07:09:03 Creating service with name: iperf-serverhigh2
    DEBUG: 2019/11/13 07:09:03 Creating iperf server pod: iperf-serverhigh3
    DEBUG: 2019/11/13 07:09:05 Creating service with name: iperf-serverhigh3
    DEBUG: 2019/11/13 07:09:05 Creating iperf server pod: iperf-serverhigh4
    DEBUG: 2019/11/13 07:09:08 Creating service with name: iperf-serverhigh4
    DEBUG: 2019/11/13 07:09:08 Creating iperf server pod: iperf-serverhigh5
    DEBUG: 2019/11/13 07:09:11 Creating service with name: iperf-serverhigh5
    DEBUG: 2019/11/13 07:09:11 Creating iperf server pod: iperf-serverhigh6
    DEBUG: 2019/11/13 07:09:14 Creating service with name: iperf-serverhigh6
    DEBUG: 2019/11/13 07:09:14 Creating iperf server pod: iperf-serverhigh7
    DEBUG: 2019/11/13 07:09:16 Creating service with name: iperf-serverhigh7
    DEBUG: 2019/11/13 07:09:16 Creating iperf server pod: iperf-serverhigh8
    DEBUG: 2019/11/13 07:09:19 Creating service with name: iperf-serverhigh8
    DEBUG: 2019/11/13 07:09:49 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/13 07:09:52 Creating iperf Client pod: iperf-clienthigh2
    DEBUG: 2019/11/13 07:10:02 Creating iperf Client pod: iperf-clienthigh3
    DEBUG: 2019/11/13 07:10:05 Creating iperf Client pod: iperf-clienthigh4
    DEBUG: 2019/11/13 07:10:07 Creating iperf Client pod: iperf-clienthigh5
    DEBUG: 2019/11/13 07:10:19 Creating iperf Client pod: iperf-clienthigh6
    DEBUG: 2019/11/13 07:10:31 Creating iperf Client pod: iperf-clienthigh7
    DEBUG: 2019/11/13 07:10:38 Creating iperf Client pod: iperf-clienthigh8
    DEBUG: 2019/11/13 07:10:49 Validating resource reservation for a cluster having 18 vnics
    DEBUG: 2019/11/13 07:10:49 Validating Vnics reservation in cluster
    DEBUG: 2019/11/13 07:10:49 Validating bandwidth reservation in cluster
    DEBUG: 2019/11/13 07:10:50 Cluster Node: appserv53. Used Bandwidth: 3000000000
    DEBUG: 2019/11/13 07:10:50 Node : appserv53, Actual bandwith : 3000000000, Expected bandwidth :3000000000
    DEBUG: 2019/11/13 07:10:50 Cluster Node: appserv54. Used Bandwidth: 2500000000
    DEBUG: 2019/11/13 07:10:50 Node : appserv54, Actual bandwith : 2500000000, Expected bandwidth :2500000000
    DEBUG: 2019/11/13 07:10:51 Cluster Node: appserv55. Used Bandwidth: 2500000000
    DEBUG: 2019/11/13 07:10:51 Node : appserv55, Actual bandwith : 2500000000, Expected bandwidth :2500000000
    DEBUG: 2019/11/13 07:10:51 Validation network reservation in cluster
    DEBUG: 2019/11/13 07:10:51 Sleeping for 720 sec
    DEBUG: 2019/11/13 07:22:51 Checking if all client pods are in Complete state/Succeeded phase
    DEBUG: 2019/11/13 07:22:54 Deleting pods : 
    DEBUG: 2019/11/13 07:23:14 Deleting service(s)
    DEBUG: 2019/11/13 07:23:15 Waiting 60 sec for resource release
    DEBUG: 2019/11/13 07:24:15 Validating resource reservation for a cluster having 2 vnics
    DEBUG: 2019/11/13 07:24:15 Validating Vnics reservation in cluster
    DEBUG: 2019/11/13 07:24:16 Validating bandwidth reservation in cluster
    DEBUG: 2019/11/13 07:24:16 Cluster Node: appserv53. Used Bandwidth: 0
    DEBUG: 2019/11/13 07:24:16 Node : appserv53, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/13 07:24:16 Cluster Node: appserv54. Used Bandwidth: 0
    DEBUG: 2019/11/13 07:24:16 Node : appserv54, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/13 07:24:16 Cluster Node: appserv55. Used Bandwidth: 0
    DEBUG: 2019/11/13 07:24:16 Node : appserv55, Actual bandwith : 0, Expected bandwidth :0
    DEBUG: 2019/11/13 07:24:16 Validation network reservation in cluster
[AfterEach] Creating iperf pods with restart policy Never
  /gocode/main/test/e2e/tests/network-pod.go:2409
    DEBUG: 2019/11/13 07:24:16 END_TEST Network.IperfPodsWithRestartPolicyNever Time-taken : 938.345535134
    DEBUG: 2019/11/13 07:24:16 Checking stale storage resources
    DEBUG: 2019/11/13 07:24:16 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:24:16 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:24:16 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:938.447 seconds][0m
Network.IperfPodsWithRestartPolicyNever Daily N_Basic-2.3 MutliZone
[90m/gocode/main/test/e2e/tests/network-pod.go:2396[0m
  Creating iperf pods with restart policy Never
  [90m/gocode/main/test/e2e/tests/network-pod.go:2397[0m
    Create iperf client-server pairs with restart policy Never.
    [90m/gocode/main/test/e2e/tests/network-pod.go:2414[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mLocalStorage.RebootInitiator Daily S_Basic-2.0 S_Basic-2.1 S_Reboot-2.0 S_Reboot-2.2 S_Reboot-2.6[0m [90mreboot initiator node after running IO and verify data before and after reboot[0m 
  [1mreboot initiator node after running IO and verify data before and after reboot[0m
  [37m/gocode/main/test/e2e/tests/volume.go:2451[0m
[BeforeEach] reboot initiator node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2439
    DEBUG: 2019/11/13 07:24:16 START_TEST LocalStorage.RebootInitiator
    DEBUG: 2019/11/13 07:24:16 Login to cluster
    DEBUG: 2019/11/13 07:24:17 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:24:17 Updating inventory struct
    DEBUG: 2019/11/13 07:24:18 Checking stale storage resources
    DEBUG: 2019/11/13 07:24:18 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:24:18 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:24:18 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:24:26 Creating storage classes
[It] reboot initiator node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2451
    DEBUG: 2019/11/13 07:24:36 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/13 07:24:42 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/13 07:24:42 Creating 4 volumes of random sizes
    DEBUG: 2019/11/13 07:24:42 Mirror Count: 1
    DEBUG: 2019/11/13 07:24:43 Attaching all 4 volumes
    DEBUG: 2019/11/13 07:25:03 Initiator node : appserv55
    DEBUG: 2019/11/13 07:25:03 Nodes to reboot :[appserv55]
    DEBUG: 2019/11/13 07:25:06 Running WRITE fio job on node : appserv55
    DEBUG: 2019/11/13 07:25:06 FIO Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"pqrs\"-12 --verify_interval=4096 --runtime=120 --blocksize=64K --iodepth=16  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1

    DEBUG: 2019/11/13 07:27:07 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/13 07:27:07 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"pqrs\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=64K --iodepth=16  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/13 07:27:28 Getting cluster quorum nodes
    DEBUG: 2019/11/13 07:27:28 Powering OFF the node appserv55
    DEBUG: 2019/11/13 07:27:28 Node 172.16.6.155 took 0 seconds to power off
    DEBUG: 2019/11/13 07:27:28 Ensuring that appserv55 node is unreachable: 
    DEBUG: 2019/11/13 07:27:28 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:27:37 Polling to check until node: appserv55 goes down
    DEBUG: 2019/11/13 07:29:29 Powering ON the node appserv55
    DEBUG: 2019/11/13 07:29:30 Node 172.16.6.155 took 1 seconds to power on
    DEBUG: 2019/11/13 07:29:30 Checking if node appserv55 is reachable or not: 
    DEBUG: 2019/11/13 07:29:30 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:29:49 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:30:06 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:30:23 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:30:40 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:30:57 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:31:14 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:31:31 Executing ping command: ping  -c 5 -W 5 appserv55
    DEBUG: 2019/11/13 07:31:35 appserv55 is pingable from local machine
    DEBUG: 2019/11/13 07:31:35 Checking ssh port is up or not on node: appserv55
    DEBUG: 2019/11/13 07:32:15 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/13 07:32:15 Found '3' nodes
    DEBUG: 2019/11/13 07:32:15 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/13 07:32:15 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/13 07:32:15 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/13 07:33:16 After power cycle/reboot, updating timestamp of node : appserv55
    DEBUG: 2019/11/13 07:33:18 Getting cluster quorum nodes
    DEBUG: 2019/11/13 07:34:18 Updating inventory struct
    DEBUG: 2019/11/13 07:34:19 Waiting for nodes to come up, will wait upto 800 seconds
    DEBUG: 2019/11/13 07:34:31 Nodes are up, waiting for armada to start
.
    DEBUG: 2019/11/13 07:35:41 Waiting for the nodes to go into Ready state
    DEBUG: 2019/11/13 07:35:41 Found '3' nodes
    DEBUG: 2019/11/13 07:35:41 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/13 07:35:41 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/13 07:35:41 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/13 07:35:42 Waiting for volumes to come into Attached state after rebooting cluster nodes.
    DEBUG: 2019/11/13 07:35:42 Detaching all 4 volumes
    DEBUG: 2019/11/13 07:35:44 Re-attaching all 4 volumes
    DEBUG: 2019/11/13 07:36:04 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/13 07:36:07 Comparing the device path & uuid on initiator before and after reboot
    DEBUG: 2019/11/13 07:36:10 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/13 07:36:10 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"pqrs\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=64K --iodepth=16  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/13 07:36:28 Successfully completed Verification on all the volumes
    DEBUG: 2019/11/13 07:36:28 Detach & Delete all volumes
[AfterEach] reboot initiator node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2446
    DEBUG: 2019/11/13 07:37:09 END_TEST LocalStorage.RebootInitiator Time-taken : 772.574839758
    DEBUG: 2019/11/13 07:37:09 Checking stale storage resources
    DEBUG: 2019/11/13 07:37:09 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:37:09 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:37:09 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:772.684 seconds][0m
LocalStorage.RebootInitiator Daily S_Basic-2.0 S_Basic-2.1 S_Reboot-2.0 S_Reboot-2.2 S_Reboot-2.6
[90m/gocode/main/test/e2e/tests/volume.go:2432[0m
  reboot initiator node after running IO and verify data before and after reboot
  [90m/gocode/main/test/e2e/tests/volume.go:2434[0m
    reboot initiator node after running IO and verify data before and after reboot
    [90m/gocode/main/test/e2e/tests/volume.go:2451[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries creating qos template with invalid value for max bandwidth.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:218[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/13 07:37:09 Login to cluster
    DEBUG: 2019/11/13 07:37:10 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:37:10 Updating inventory struct
    DEBUG: 2019/11/13 07:37:10 Checking stale storage resources
    DEBUG: 2019/11/13 07:37:11 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:37:11 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:37:11 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:37:18 Creating storage classes
    DEBUG: 2019/11/13 07:37:30 START_TEST PerfTier.NegativeTests
[It] tries creating qos template with invalid value for max bandwidth.
  /gocode/main/test/e2e/tests/perf-tier.go:218
    ERROR: 2019/11/13 07:37:31  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 1G -i 50k -B invalidBW@3!', output:, error:Error: Invalid --max-network-bw/-B specification. Input should be in K, M, G format



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/13 07:37:31 END_TEST PerfTier.NegativeTests Time-taken: 0.056469658
    DEBUG: 2019/11/13 07:37:31 Checking stale storage resources
    DEBUG: 2019/11/13 07:37:31 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:37:31 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:37:31 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:21.545 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries creating qos template with invalid value for max bandwidth.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:218[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mPerfTier.Basic Management Sanity QOS_Cli-1.0 QOS_Cli-1.7 QOS_Cli-1.8 QOS_Cli-1.9 QOS_Cli-2.0 QOS_Cli-2.1 QOS_Cli-2.2 QOS_Cli-4.0 QOS_Cli-4.1 QOS_Cli-4.2 Multizone[0m [90mBasic Perf-tier testcases[0m 
  [1mCreate qos template with min bw, min iops, max iops.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:121[0m
[BeforeEach] Basic Perf-tier testcases
  /gocode/main/test/e2e/tests/perf-tier.go:36
    DEBUG: 2019/11/13 07:37:31 Login to cluster
    DEBUG: 2019/11/13 07:37:31 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:37:31 Updating inventory struct
    DEBUG: 2019/11/13 07:37:32 Checking stale storage resources
    DEBUG: 2019/11/13 07:37:32 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:37:32 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:37:32 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:37:40 Creating storage classes
    DEBUG: 2019/11/13 07:37:50 START_TEST PerfTier.Basic
[It] Create qos template with min bw, min iops, max iops.
  /gocode/main/test/e2e/tests/perf-tier.go:121
    DEBUG: 2019/11/13 07:37:50 Creating perf-tier with minimum bandwidth, minimum iops, maximum iops: 
    DEBUG: 2019/11/13 07:37:50 Deleting perf-tier: 
[AfterEach] Basic Perf-tier testcases
  /gocode/main/test/e2e/tests/perf-tier.go:42
    DEBUG: 2019/11/13 07:37:50 END_TEST PerfTier.Basic Time-taken : 0.253343346
    DEBUG: 2019/11/13 07:37:50 Checking stale storage resources
    DEBUG: 2019/11/13 07:37:50 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:37:50 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:37:50 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.590 seconds][0m
PerfTier.Basic Management Sanity QOS_Cli-1.0 QOS_Cli-1.7 QOS_Cli-1.8 QOS_Cli-1.9 QOS_Cli-2.0 QOS_Cli-2.1 QOS_Cli-2.2 QOS_Cli-4.0 QOS_Cli-4.1 QOS_Cli-4.2 Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:26[0m
  Basic Perf-tier testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:27[0m
    Create qos template with min bw, min iops, max iops.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:121[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries to create perf tier with invalid Bandwidth.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:190[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/13 07:37:50 Login to cluster
    DEBUG: 2019/11/13 07:37:51 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:37:51 Updating inventory struct
    DEBUG: 2019/11/13 07:37:52 Checking stale storage resources
    DEBUG: 2019/11/13 07:37:52 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:37:52 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:37:52 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:37:59 Creating storage classes
    DEBUG: 2019/11/13 07:38:09 START_TEST PerfTier.NegativeTests
[It] tries to create perf tier with invalid Bandwidth.
  /gocode/main/test/e2e/tests/perf-tier.go:190
    DEBUG: 2019/11/13 07:38:09 Try to create perf-tier with invalid Bandwidth.
    ERROR: 2019/11/13 07:38:09  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b -500M -i 50k', output:, error:Error: Invalid --network-bw/-b specification. Input should be in K, M, G format



[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/13 07:38:09 END_TEST PerfTier.NegativeTests Time-taken: 0.060072573
    DEBUG: 2019/11/13 07:38:09 Checking stale storage resources
    DEBUG: 2019/11/13 07:38:09 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:38:09 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:38:09 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:18.955 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries to create perf tier with invalid Bandwidth.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:190[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9[0m [90mPing an external IP from from a pod[0m 
  [1mPing external IP from a pod created using endpoint of public network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:538[0m
[BeforeEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:487
    DEBUG: 2019/11/13 07:38:09 START_TEST Network.PingExternalIP
    DEBUG: 2019/11/13 07:38:09 Login to cluster
    DEBUG: 2019/11/13 07:38:10 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:38:10 Updating inventory struct
    DEBUG: 2019/11/13 07:38:11 Checking stale storage resources
    DEBUG: 2019/11/13 07:38:11 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:38:11 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:38:11 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:38:18 Creating storage classes
[It] Ping external IP from a pod created using endpoint of public network
  /gocode/main/test/e2e/tests/network-pod.go:538
    DEBUG: 2019/11/13 07:38:28 Creating endpoint test-ep 
    DEBUG: 2019/11/13 07:38:28 Creating a pod with  docker.io/redis:3.0.5 image & the endpoint 
    DEBUG: 2019/11/13 07:38:31 IP address ( 172.16.179.6 ) of e2e-test-pod is between 172.16.179.4 and 172.16.179.253

    DEBUG: 2019/11/13 07:38:31 Trying to ping google-public-dns-a.google.com from pod e2e-test-pod
    DEBUG: 2019/11/13 07:38:31 google-public-dns-a.google.com is pingable from pod e2e-test-pod (172.16.179.6)
    DEBUG: 2019/11/13 07:38:31 Network gateway is 172.16.179.1
    DEBUG: 2019/11/13 07:38:31 Matching default gateway of pod e2e-test-pod with 172.16.179.1 
    DEBUG: 2019/11/13 07:38:32 Default gateway of e2e-test-pod is 172.16.179.1
    DEBUG: 2019/11/13 07:38:32 Deleting the pod: e2e-test-pod
    DEBUG: 2019/11/13 07:38:47 Delete Endpoint: test-ep
[AfterEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:496
    DEBUG: 2019/11/13 07:38:47 END_TEST Network.PingExternalIP Time-taken : 38.084154307
    DEBUG: 2019/11/13 07:38:47 Checking stale storage resources
    DEBUG: 2019/11/13 07:38:47 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:38:47 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:38:47 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:38.189 seconds][0m
Network.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9
[90m/gocode/main/test/e2e/tests/network-pod.go:480[0m
  Ping an external IP from from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:481[0m
    Ping external IP from a pod created using endpoint of public network
    [90m/gocode/main/test/e2e/tests/network-pod.go:538[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mNetwork.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2[0m [90mNetwork Negative testcases[0m 
  [1mCreate network with same subnet of existing network but with different Vlan[0m
  [37m/gocode/main/test/e2e/tests/network.go:136[0m
[BeforeEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:34
    DEBUG: 2019/11/13 07:38:47 START_TEST Network.NegativeTests
    DEBUG: 2019/11/13 07:38:47 Login to cluster
    DEBUG: 2019/11/13 07:38:48 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:38:48 Updating inventory struct
    DEBUG: 2019/11/13 07:38:49 Checking stale storage resources
    DEBUG: 2019/11/13 07:38:49 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:38:49 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:38:49 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:38:56 Creating storage classes
[It] Create network with same subnet of existing network but with different Vlan
  /gocode/main/test/e2e/tests/network.go:136
    DEBUG: 2019/11/13 07:39:06 Trying to create network with same subnet ( start addr : 172.16.180.4, end addr : 172.16.180.253) and different VLAN.
[AfterEach] Network Negative testcases
  /gocode/main/test/e2e/tests/network.go:47
    DEBUG: 2019/11/13 07:39:06 END_TEST Network.NegativeTests Time-taken : 19.024369418
    DEBUG: 2019/11/13 07:39:06 Checking stale storage resources
    DEBUG: 2019/11/13 07:39:07 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:39:07 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:39:07 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.139 seconds][0m
Network.NegativeTests Daily N_Negative-1.0 N_Negative-1.1 N_Negative-1.2
[90m/gocode/main/test/e2e/tests/network.go:25[0m
  Network Negative testcases
  [90m/gocode/main/test/e2e/tests/network.go:26[0m
    Create network with same subnet of existing network but with different Vlan
    [90m/gocode/main/test/e2e/tests/network.go:136[0m
[90m------------------------------[0m
[0mPerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone[0m [90mPerf-tier negative testcases[0m 
  [1mtries deleting best-effort QoS class.[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:230[0m
[BeforeEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:151
    DEBUG: 2019/11/13 07:39:07 Login to cluster
    DEBUG: 2019/11/13 07:39:07 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:39:07 Updating inventory struct
    DEBUG: 2019/11/13 07:39:08 Checking stale storage resources
    DEBUG: 2019/11/13 07:39:08 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:39:08 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:39:08 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:39:16 Creating storage classes
    DEBUG: 2019/11/13 07:39:26 START_TEST PerfTier.NegativeTests
[It] tries deleting best-effort QoS class.
  /gocode/main/test/e2e/tests/perf-tier.go:230
[AfterEach] Perf-tier negative testcases
  /gocode/main/test/e2e/tests/perf-tier.go:157
    DEBUG: 2019/11/13 07:39:26 END_TEST PerfTier.NegativeTests Time-taken: 0.079501451
    DEBUG: 2019/11/13 07:39:26 Checking stale storage resources
    DEBUG: 2019/11/13 07:39:26 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:39:26 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:39:26 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:19.224 seconds][0m
PerfTier.NegativeTests Management Daily QOS_Cli-1.1 QOS_Cli-1.2 QOS_Cli-1.3 QOS_Cli-1.4 QOS_Cli-1.5 QOS_Cli-1.6 QOS_Cli-2.3 QOS_Cli-2.4  QOS_Cli-4.3 QOS_Cli-4.4 QOS_Cli-4.5 QOS_Cli-4.6 QOS_Cli-4.7   Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:142[0m
  Perf-tier negative testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:143[0m
    tries deleting best-effort QoS class.
    [90m/gocode/main/test/e2e/tests/perf-tier.go:230[0m
[90m------------------------------[0m
[0mMirroring.OnlinePlexDeleteWithAddPlex Daily SM_PlexDelete-1.2[0m [90mCreate mirrored volumes and delete plexes from these volumes and add new plex[0m 
  [1mCreate mirrored volumes and delete plexes from these volumes and add new plex[0m
  [37m/gocode/main/test/e2e/tests/mirroring.go:2149[0m
[BeforeEach] Create mirrored volumes and delete plexes from these volumes and add new plex
  /gocode/main/test/e2e/tests/mirroring.go:2134
    DEBUG: 2019/11/13 07:39:26 START_TEST Mirroring.OnlinePlexDeleteWithAddPlex
    DEBUG: 2019/11/13 07:39:26 Login to cluster
    DEBUG: 2019/11/13 07:39:26 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:39:26 Updating inventory struct
    DEBUG: 2019/11/13 07:39:27 Checking stale storage resources
    DEBUG: 2019/11/13 07:39:27 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:39:27 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:39:27 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:39:35 Creating storage classes
[It] Create mirrored volumes and delete plexes from these volumes and add new plex
  /gocode/main/test/e2e/tests/mirroring.go:2149
    DEBUG: 2019/11/13 07:39:45 Creating 8 volumes. Mirror Count: 3:
    DEBUG: 2019/11/13 07:39:45 Mirror Count: 3
    DEBUG: 2019/11/13 07:39:46 Attaching volumes: 
    DEBUG: 2019/11/13 07:40:25 Running write fio job on all volumes: 
    DEBUG: 2019/11/13 07:40:26 Running Write IOs on node : appserv54 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:40:26 Running Write IOs on node : appserv55 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:40:26 Running Write IOs on node : appserv53 
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --runtime=120 --blocksize=4K --direct=1 --time_based  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/13 07:42:27 Running read fio job on all volumes: 
    DEBUG: 2019/11/13 07:42:29 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:29 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:29 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:30 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:30 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:30 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:31 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/13 07:42:31 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:31 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/13 07:42:32 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:42:32 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:43:41 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:43 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:43 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:44 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:44 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:44 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:45 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:43:45 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/13 07:43:45 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:47 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/13 07:43:47 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:44:57 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/13 07:44:57 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/13 07:44:58 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/13 07:44:58 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/13 07:44:58 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/13 07:44:59 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/13 07:44:59 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/13 07:45:00 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/13 07:45:00 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:45:01 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/13 07:45:02 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:46:12 Removing a plex from each volume. Expected PlexCount: 2
    DEBUG: 2019/11/13 07:47:08 Removing a plex from each volume. Expected PlexCount: 1
    DEBUG: 2019/11/13 07:48:09 Adding new plex to each volume. Expected PlexCount 2: 
    DEBUG: 2019/11/13 07:48:11 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:11 Volume name & Plex : test-vol1.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:13 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/13 07:48:13 Volume: test-vol1. Resync offset: 78

    DEBUG: 2019/11/13 07:48:14 Volume: test-vol1. Resync offset: 89

    DEBUG: 2019/11/13 07:48:14 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:14 Volume name & Plex : test-vol2.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:15 Volume "test-vol2" has index "1" in embedded.
    DEBUG: 2019/11/13 07:48:16 Volume: test-vol2. Resync offset: 29

    DEBUG: 2019/11/13 07:48:17 Volume: test-vol2. Resync offset: 32

    DEBUG: 2019/11/13 07:48:17 Volume name & Plex : test-vol3.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:18 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/13 07:48:19 Volume: test-vol3. Resync offset: 36

    DEBUG: 2019/11/13 07:48:20 Volume: test-vol3. Resync offset: 39

    DEBUG: 2019/11/13 07:48:20 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:20 Volume name & Plex : test-vol4.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:21 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/13 07:48:22 Volume: test-vol4. Resync offset: 37

    DEBUG: 2019/11/13 07:48:22 Volume: test-vol4. Resync offset: 40

    DEBUG: 2019/11/13 07:48:22 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:23 Volume name & Plex : test-vol5.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:24 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/13 07:48:24 Volume: test-vol5. Resync offset: 32

    DEBUG: 2019/11/13 07:48:25 Volume: test-vol5. Resync offset: 33

    DEBUG: 2019/11/13 07:48:25 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:48:25 Volume name & Plex : test-vol6.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:27 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/13 07:48:27 Volume: test-vol6. Resync offset: 35

    DEBUG: 2019/11/13 07:48:28 Volume: test-vol6. Resync offset: 36

    DEBUG: 2019/11/13 07:48:28 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:28 Volume name & Plex : test-vol7.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:29 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/13 07:48:30 Volume: test-vol7. Resync offset: 38

    DEBUG: 2019/11/13 07:48:31 Volume: test-vol7. Resync offset: 39

    DEBUG: 2019/11/13 07:48:31 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:48:31 Volume name & Plex : test-vol8.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:32 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/13 07:48:33 Volume: test-vol8. Resync offset: 37

    DEBUG: 2019/11/13 07:48:34 Volume: test-vol8. Resync offset: 39

    DEBUG: 2019/11/13 07:48:34 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:48:34 Number of volumes : 8
    DEBUG: 2019/11/13 07:48:34 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/13 07:48:34 Volume name & Plex : test-vol8.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:35 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/13 07:48:36 Volume: test-vol8. Resync offset: 43

    DEBUG: 2019/11/13 07:48:37 Volume: test-vol8. Resync offset: 44

    DEBUG: 2019/11/13 07:48:37 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:48:37 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/13 07:48:37 Volume name & Plex : test-vol4.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:38 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/13 07:48:39 Volume: test-vol4. Resync offset: 94

    DEBUG: 2019/11/13 07:48:39 Volume: test-vol4. Resync offset: 96

    DEBUG: 2019/11/13 07:48:39 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:39 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/13 07:48:40 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:40 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:40 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/13 07:48:40 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/13 07:48:40 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:40 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:40 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/13 07:48:40 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/13 07:48:40 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:40 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:40 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/13 07:48:40 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/13 07:48:40 Volume name & Plex : test-vol6.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:41 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/13 07:48:42 Volume: test-vol6. Resync offset: 68

    DEBUG: 2019/11/13 07:48:43 Volume: test-vol6. Resync offset: 70

    DEBUG: 2019/11/13 07:48:43 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:43 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/13 07:48:43 Volume name & Plex : test-vol5.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:44 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/13 07:48:45 Volume: test-vol5. Resync offset: 86

    DEBUG: 2019/11/13 07:48:45 Volume: test-vol5. Resync offset: 89

    DEBUG: 2019/11/13 07:48:45 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:48:45 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/13 07:48:46 Volume name & Plex : test-vol7.p0. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:47 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/13 07:48:48 Volume: test-vol7. Resync offset: 72

    DEBUG: 2019/11/13 07:48:48 Volume: test-vol7. Resync offset: 73

    DEBUG: 2019/11/13 07:48:48 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:48:48 Adding new plex to each volume. Expected PlexCount 3: 
    DEBUG: 2019/11/13 07:48:51 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:51 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:51 Volume name & Plex : test-vol1.p2. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:52 Volume "test-vol1" has index "0" in embedded.
    DEBUG: 2019/11/13 07:48:53 Volume: test-vol1. Resync offset: 50

    DEBUG: 2019/11/13 07:48:53 Volume: test-vol1. Resync offset: 58

    DEBUG: 2019/11/13 07:48:53 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:53 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:53 Volume name & Plex : test-vol2.p2. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:55 Volume "test-vol2" has index "1" in embedded.
    DEBUG: 2019/11/13 07:48:55 Volume: test-vol2. Resync offset: 50

    DEBUG: 2019/11/13 07:48:56 Volume: test-vol2. Resync offset: 55

    DEBUG: 2019/11/13 07:48:56 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:56 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:56 Volume name & Plex : test-vol3.p2. Plex State : Resyncing
    DEBUG: 2019/11/13 07:48:58 Volume "test-vol3" has index "2" in embedded.
    DEBUG: 2019/11/13 07:48:58 Volume: test-vol3. Resync offset: 29

    DEBUG: 2019/11/13 07:48:59 Volume: test-vol3. Resync offset: 32

    DEBUG: 2019/11/13 07:48:59 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:48:59 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:48:59 Volume name & Plex : test-vol4.p2. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:00 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/13 07:49:01 Volume: test-vol4. Resync offset: 31

    DEBUG: 2019/11/13 07:49:02 Volume: test-vol4. Resync offset: 33

    DEBUG: 2019/11/13 07:49:02 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:02 Volume name & Plex : test-vol5.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:03 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/13 07:49:04 Volume: test-vol5. Resync offset: 39

    DEBUG: 2019/11/13 07:49:04 Volume: test-vol5. Resync offset: 41

    DEBUG: 2019/11/13 07:49:04 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:05 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:05 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:49:05 Volume name & Plex : test-vol6.p2. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:06 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/13 07:49:07 Volume: test-vol6. Resync offset: 21

    DEBUG: 2019/11/13 07:49:07 Volume: test-vol6. Resync offset: 23

    DEBUG: 2019/11/13 07:49:07 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:07 Volume name & Plex : test-vol7.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:09 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/13 07:49:09 Volume: test-vol7. Resync offset: 12

    DEBUG: 2019/11/13 07:49:10 Volume: test-vol7. Resync offset: 13

    DEBUG: 2019/11/13 07:49:10 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:10 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:10 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:11 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/13 07:49:12 Volume: test-vol8. Resync offset: 4

    DEBUG: 2019/11/13 07:49:13 Volume: test-vol8. Resync offset: 5

    DEBUG: 2019/11/13 07:49:13 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:13 Number of volumes : 8
    DEBUG: 2019/11/13 07:49:13 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/13 07:49:13 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:13 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:14 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/13 07:49:15 Volume: test-vol8. Resync offset: 9

    DEBUG: 2019/11/13 07:49:16 Volume: test-vol8. Resync offset: 10

    DEBUG: 2019/11/13 07:49:16 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:16 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/13 07:49:16 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:16 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:49:16 Volume name & Plex : test-vol4.p2. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:17 Volume "test-vol4" has index "3" in embedded.
    DEBUG: 2019/11/13 07:49:18 Volume: test-vol4. Resync offset: 85

    DEBUG: 2019/11/13 07:49:18 Volume: test-vol4. Resync offset: 87

    DEBUG: 2019/11/13 07:49:18 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/13 07:49:19 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/13 07:49:19 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/13 07:49:19 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:49:19 Volume name & Plex : test-vol6.p2. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:20 Volume "test-vol6" has index "5" in embedded.
    DEBUG: 2019/11/13 07:49:21 Volume: test-vol6. Resync offset: 53

    DEBUG: 2019/11/13 07:49:22 Volume: test-vol6. Resync offset: 55

    DEBUG: 2019/11/13 07:49:22 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/13 07:49:22 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:22 Volume name & Plex : test-vol5.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:23 Volume "test-vol5" has index "4" in embedded.
    DEBUG: 2019/11/13 07:49:24 Volume: test-vol5. Resync offset: 91

    DEBUG: 2019/11/13 07:49:24 Volume: test-vol5. Resync offset: 93

    DEBUG: 2019/11/13 07:49:24 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:24 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/13 07:49:24 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:24 Volume name & Plex : test-vol7.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:26 Volume "test-vol7" has index "6" in embedded.
    DEBUG: 2019/11/13 07:49:26 Volume: test-vol7. Resync offset: 49

    DEBUG: 2019/11/13 07:49:27 Volume: test-vol7. Resync offset: 51

    DEBUG: 2019/11/13 07:49:27 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:27 Wait till resync completion on all volumes
    DEBUG: 2019/11/13 07:49:27 Number of volumes : 8
    DEBUG: 2019/11/13 07:49:27 Checking resync progress on volume : test-vol8
    DEBUG: 2019/11/13 07:49:27 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:49:27 Volume name & Plex : test-vol8.p1. Plex State : Resyncing
    DEBUG: 2019/11/13 07:49:29 Volume "test-vol8" has index "7" in embedded.
    DEBUG: 2019/11/13 07:49:29 Volume: test-vol8. Resync offset: 35

    DEBUG: 2019/11/13 07:49:30 Volume: test-vol8. Resync offset: 36

    DEBUG: 2019/11/13 07:49:30 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:49:30 Resync yet to complete. Sleeping for 30 seconds before checking resync progress.
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol8.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol8.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol8.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 All plexes of volume "test-vol8" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:00 Checking resync progress on volume : test-vol4
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol4.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol4.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol4.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 All plexes of volume "test-vol4" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:00 Checking resync progress on volume : test-vol1
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol1.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol1.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol1.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 All plexes of volume "test-vol1" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:00 Checking resync progress on volume : test-vol2
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol2.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol2.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol2.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 All plexes of volume "test-vol2" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:00 Checking resync progress on volume : test-vol3
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol3.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol3.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 Volume name & Plex : test-vol3.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:00 All plexes of volume "test-vol3" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:00 Checking resync progress on volume : test-vol7
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol7.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol7.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol7.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 All plexes of volume "test-vol7" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:01 Checking resync progress on volume : test-vol6
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol6.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol6.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol6.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 All plexes of volume "test-vol6" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:01 Checking resync progress on volume : test-vol5
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol5.p0. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol5.p1. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 Volume name & Plex : test-vol5.p2. Plex State : InUse
    DEBUG: 2019/11/13 07:50:01 All plexes of volume "test-vol5" are in "InUse" state.
    DEBUG: 2019/11/13 07:50:01 Running verify fio on all volumes across all plexes: 
    DEBUG: 2019/11/13 07:50:02 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:02 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:02 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:04 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:04 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:04 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:04 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/13 07:50:05 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:05 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 0
    DEBUG: 2019/11/13 07:50:06 Running Verify IOs on node : appserv55 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:50:06 Running Verify IOs on node : appserv54 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:50:49 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:50 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:50 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:50 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:51 Running Verify IOs on node : appserv53 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/13 07:50:51 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:52 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:52 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:53 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 1
    DEBUG: 2019/11/13 07:50:53 Running Verify IOs on node : appserv54 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:50:54 Running Verify IOs on node : appserv55 and plex p1
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:52:04 Changing preferred plex of volume: test-vol2. Volume load index: 1.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:05 Changing preferred plex of volume: test-vol5. Volume load index: 4.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:06 Running Verify IOs on node : appserv53 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1
    DEBUG: 2019/11/13 07:52:07 Changing preferred plex of volume: test-vol1. Volume load index: 0.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:07 Changing preferred plex of volume: test-vol3. Volume load index: 2.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:08 Changing preferred plex of volume: test-vol4. Volume load index: 3.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:09 Changing preferred plex of volume: test-vol6. Volume load index: 5.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:10 Changing preferred plex of volume: test-vol7. Volume load index: 6.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:10 Changing preferred plex of volume: test-vol8. Volume load index: 7.New preferred plex: 2
    DEBUG: 2019/11/13 07:52:10 Running Verify IOs on node : appserv55 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:52:11 Running Verify IOs on node : appserv54 and plex p2
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"abcd\"-21 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=4K --direct=1  --name=dev1 --filename=/dev/nvme1n1 --name=dev2 --filename=/dev/nvme2n1 --name=dev3 --filename=/dev/nvme3n1
    DEBUG: 2019/11/13 07:53:24 Detach & delete all volumes: 
[AfterEach] Create mirrored volumes and delete plexes from these volumes and add new plex
  /gocode/main/test/e2e/tests/mirroring.go:2144
    DEBUG: 2019/11/13 07:54:09 END_TEST Mirroring.OnlinePlexDeleteWithAddPlex Time-taken : 883.197748084
    DEBUG: 2019/11/13 07:54:09 Checking stale storage resources
    DEBUG: 2019/11/13 07:54:09 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:54:09 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:54:09 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:883.311 seconds][0m
Mirroring.OnlinePlexDeleteWithAddPlex Daily SM_PlexDelete-1.2
[90m/gocode/main/test/e2e/tests/mirroring.go:2128[0m
  Create mirrored volumes and delete plexes from these volumes and add new plex
  [90m/gocode/main/test/e2e/tests/mirroring.go:2129[0m
    Create mirrored volumes and delete plexes from these volumes and add new plex
    [90m/gocode/main/test/e2e/tests/mirroring.go:2149[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mPod.Basic Management Sanity M_Pod-1.0 M_Pod-1.1 M_Pod-1.2 M_Pod-1.3[0m [90mpod provisioning tests[0m 
  [1mPod create, delete, list api check[0m
  [37m/gocode/main/test/e2e/tests/pod.go:59[0m
[BeforeEach] pod provisioning tests
  /gocode/main/test/e2e/tests/pod.go:48
    DEBUG: 2019/11/13 07:54:09 START_TEST Pod.Basic
    DEBUG: 2019/11/13 07:54:09 Login to cluster
    DEBUG: 2019/11/13 07:54:10 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:54:10 Updating inventory struct
    DEBUG: 2019/11/13 07:54:10 Checking stale storage resources
    DEBUG: 2019/11/13 07:54:10 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:54:10 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:54:10 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:54:18 Creating storage classes
[It] Pod create, delete, list api check
  /gocode/main/test/e2e/tests/pod.go:59
    DEBUG: 2019/11/13 07:54:30 Creating test volume: test-vol1
    DEBUG: 2019/11/13 07:54:31 Create pvc
    DEBUG: 2019/11/13 07:54:31 Created PVC successfully.
    DEBUG: 2019/11/13 07:54:31 Pod with Network
    DEBUG: 2019/11/13 07:54:34 Checking Pod's IP against what was allocated
    DEBUG: 2019/11/13 07:54:34 Pod with CPU shares + Network
    DEBUG: 2019/11/13 07:54:36 Pod with CPU + Memory + Network
    DEBUG: 2019/11/13 07:54:39 Pod with CPU + Memory + Network + Diamanti Volume
    DEBUG: 2019/11/13 07:54:44 List the pods.
    DEBUG: 2019/11/13 07:54:44 Run through and delete all pods.
    DEBUG: 2019/11/13 07:54:44 Deleting pod: testpod101
    DEBUG: 2019/11/13 07:54:57 Deleting pod: testpod102
    DEBUG: 2019/11/13 07:55:11 Deleting pod: testpod103
    DEBUG: 2019/11/13 07:55:25 Deleting pod: testpod104
    DEBUG: 2019/11/13 07:55:41 Make sure that default network usage is zero.
    DEBUG: 2019/11/13 07:55:41 Wait till volume comes into "Available" state: 
    DEBUG: 2019/11/13 07:55:41 Delete PVC: 
    DEBUG: 2019/11/13 07:55:41 Delete test volume.
[AfterEach] pod provisioning tests
  /gocode/main/test/e2e/tests/pod.go:54
    DEBUG: 2019/11/13 07:56:40 END_TEST Pod.Basic Time-taken : 150.49273517
    DEBUG: 2019/11/13 07:56:40 Checking stale storage resources
    DEBUG: 2019/11/13 07:56:40 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:56:40 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:56:40 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:150.606 seconds][0m
Pod.Basic Management Sanity M_Pod-1.0 M_Pod-1.1 M_Pod-1.2 M_Pod-1.3
[90m/gocode/main/test/e2e/tests/pod.go:40[0m
  pod provisioning tests
  [90m/gocode/main/test/e2e/tests/pod.go:42[0m
    Pod create, delete, list api check
    [90m/gocode/main/test/e2e/tests/pod.go:59[0m
[90m------------------------------[0m
[0mNetwork.UniDirectionalQosValidationAcrossBothNics Daily AT_Qos-1.5 AT_Qos-1.6 Qos Multizone[0m [90mValidate Qos on two node with iperfpod[0m 
  [1mNetwork uni-directional port 0 with high QoS and port 2 with medium QoS, QoS should be honoured[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:3502[0m
[BeforeEach] Validate Qos on two node with iperfpod
  /gocode/main/test/e2e/tests/network-pod.go:3488
    DEBUG: 2019/11/13 07:56:40 START_TEST Network.UniDirectionalQosValidationAcrossBothNics
    DEBUG: 2019/11/13 07:56:40 Login to cluster
    DEBUG: 2019/11/13 07:56:40 Checking basic Vnic usage
    DEBUG: 2019/11/13 07:56:40 Updating inventory struct
    DEBUG: 2019/11/13 07:56:41 Checking stale storage resources
    DEBUG: 2019/11/13 07:56:41 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 07:56:41 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 07:56:41 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 07:56:49 Creating storage classes
[It] Network uni-directional port 0 with high QoS and port 2 with medium QoS, QoS should be honoured
  /gocode/main/test/e2e/tests/network-pod.go:3502
    DEBUG: 2019/11/13 07:56:58 Creating perf-tier : template
    DEBUG: 2019/11/13 07:56:59 Creating 2 pairs of iperf client-server pods with template QoS
    DEBUG: 2019/11/13 07:56:59 Creating iperf server pod: iperf-server-template1
    DEBUG: 2019/11/13 07:57:06 Creating service with name: iperf-server-template1
    DEBUG: 2019/11/13 07:57:06 Creating iperf server pod: iperf-server-template2
    DEBUG: 2019/11/13 07:57:08 Creating service with name: iperf-server-template2
    DEBUG: 2019/11/13 07:57:38 Creating iperf Client pod: iperf-client-template1
    DEBUG: 2019/11/13 07:57:41 Creating iperf Client pod: iperf-client-template2
    DEBUG: 2019/11/13 07:57:44 Deleting pods scheduled on nic 0 with template QoS
    DEBUG: 2019/11/13 07:57:44 Deleting pods : 
    DEBUG: 2019/11/13 07:57:57 Deleting pods : 
    DEBUG: 2019/11/13 07:58:01 Creating 4 pairs of iperf client-server pods with high QoS on nic 0
    DEBUG: 2019/11/13 07:58:01 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/13 07:58:16 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/13 07:58:17 Creating iperf server pod: iperf-server-high2
    DEBUG: 2019/11/13 07:58:32 Creating service with name: iperf-server-high2
    DEBUG: 2019/11/13 07:58:32 Creating iperf server pod: iperf-server-high3
    DEBUG: 2019/11/13 07:58:49 Creating service with name: iperf-server-high3
    DEBUG: 2019/11/13 07:58:50 Creating iperf server pod: iperf-server-high4
    DEBUG: 2019/11/13 07:59:04 Creating service with name: iperf-server-high4
    DEBUG: 2019/11/13 07:59:34 Creating iperf Client pod: iperf-client-high1
    DEBUG: 2019/11/13 07:59:37 Creating iperf Client pod: iperf-client-high2
    DEBUG: 2019/11/13 07:59:40 Creating iperf Client pod: iperf-client-high3
    DEBUG: 2019/11/13 07:59:42 Creating iperf Client pod: iperf-client-high4
    DEBUG: 2019/11/13 07:59:46 Getting all pods which are scheduled on nicId 0 
    DEBUG: 2019/11/13 07:59:46 Getting all pods which are scheduled on nicId 0 
    DEBUG: 2019/11/13 07:59:46 Deleting pods scheduled on nic 2 with template QoS
    DEBUG: 2019/11/13 07:59:46 Deleting pods : 
    DEBUG: 2019/11/13 07:59:57 Deleting pods : 
    DEBUG: 2019/11/13 08:00:01 Creating 4 pairs of iperf client-server pods with medium QoS on nic 2
    DEBUG: 2019/11/13 08:00:01 Creating iperf server pod: iperf-server-medium1
    DEBUG: 2019/11/13 08:00:03 Creating service with name: iperf-server-medium1
    DEBUG: 2019/11/13 08:00:03 Creating iperf server pod: iperf-server-medium2
    DEBUG: 2019/11/13 08:00:07 Creating service with name: iperf-server-medium2
    DEBUG: 2019/11/13 08:00:07 Creating iperf server pod: iperf-server-medium3
    DEBUG: 2019/11/13 08:00:10 Creating service with name: iperf-server-medium3
    DEBUG: 2019/11/13 08:00:10 Creating iperf server pod: iperf-server-medium4
    DEBUG: 2019/11/13 08:00:13 Creating service with name: iperf-server-medium4
    DEBUG: 2019/11/13 08:00:43 Creating iperf Client pod: iperf-client-medium1
    DEBUG: 2019/11/13 08:00:45 Creating iperf Client pod: iperf-client-medium2
    DEBUG: 2019/11/13 08:00:48 Creating iperf Client pod: iperf-client-medium3
    DEBUG: 2019/11/13 08:00:51 Creating iperf Client pod: iperf-client-medium4
    DEBUG: 2019/11/13 08:00:53 Getting all pods which are scheduled on nicId 2 
    DEBUG: 2019/11/13 08:00:53 Getting all pods which are scheduled on nicId 2 
    DEBUG: 2019/11/13 08:00:53 Scheduling of all pods on both nics is validated
    DEBUG: 2019/11/13 08:00:53 Sleeping for 180 seconds, so that prometheus database will have some stats.
    DEBUG: 2019/11/13 08:03:53 Validating if bandwidth is honored or not for server pods:
    DEBUG: 2019/11/13 08:03:54 QoS honored for pod: iperf-server-high1
    DEBUG: 2019/11/13 08:03:54 QoS honored for pod: iperf-server-high2
    DEBUG: 2019/11/13 08:03:55 QoS honored for pod: iperf-server-high3
    DEBUG: 2019/11/13 08:03:55 QoS honored for pod: iperf-server-high4
    DEBUG: 2019/11/13 08:03:56 QoS honored for pod: iperf-server-medium1
    DEBUG: 2019/11/13 08:03:56 QoS honored for pod: iperf-server-medium2
    DEBUG: 2019/11/13 08:03:56 QoS honored for pod: iperf-server-medium3
    DEBUG: 2019/11/13 08:03:57 QoS honored for pod: iperf-server-medium4
    DEBUG: 2019/11/13 08:03:57 Validating if bandwidth is honored or not for client pods:
    DEBUG: 2019/11/13 08:03:57 QoS honored for pod: iperf-client-high1
    DEBUG: 2019/11/13 08:03:58 QoS honored for pod: iperf-client-high2
    DEBUG: 2019/11/13 08:03:58 QoS honored for pod: iperf-client-high3
    DEBUG: 2019/11/13 08:03:59 QoS honored for pod: iperf-client-high4
    DEBUG: 2019/11/13 08:03:59 QoS honored for pod: iperf-client-medium1
    DEBUG: 2019/11/13 08:04:00 QoS honored for pod: iperf-client-medium2
    DEBUG: 2019/11/13 08:04:00 QoS honored for pod: iperf-client-medium3
    DEBUG: 2019/11/13 08:04:01 QoS honored for pod: iperf-client-medium4
    DEBUG: 2019/11/13 08:04:01 Measuring throughput. num of links used: 2
    DEBUG: 2019/11/13 08:04:01 Node: appserv53. Expected Throughput: 16200000000. RX Throughput: 18520799324. TX Throughput: 0
    DEBUG: 2019/11/13 08:04:01 Node: appserv54. Expected Throughput: 16200000000. RX Throughput: 0. TX Throughput: 18429517512
    DEBUG: 2019/11/13 08:04:01 Deleting pods :
    DEBUG: 2019/11/13 08:04:01 Deleting pods : 
    DEBUG: 2019/11/13 08:04:35 Deleting pods : 
    DEBUG: 2019/11/13 08:04:59 Deleting perf-tier : template
    DEBUG: 2019/11/13 08:05:00 Deleting services : 
    DEBUG: 2019/11/13 08:05:00 Deleting service(s)
[AfterEach] Validate Qos on two node with iperfpod
  /gocode/main/test/e2e/tests/network-pod.go:3497
    DEBUG: 2019/11/13 08:05:01 END_TEST UniDirectionalQosValidationAcrossBothNics Time-taken : 501.064692853
    DEBUG: 2019/11/13 08:05:01 Checking stale storage resources
    DEBUG: 2019/11/13 08:05:01 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:05:01 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:05:01 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:501.175 seconds][0m
Network.UniDirectionalQosValidationAcrossBothNics Daily AT_Qos-1.5 AT_Qos-1.6 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:3480[0m
  Validate Qos on two node with iperfpod
  [90m/gocode/main/test/e2e/tests/network-pod.go:3481[0m
    Network uni-directional port 0 with high QoS and port 2 with medium QoS, QoS should be honoured
    [90m/gocode/main/test/e2e/tests/network-pod.go:3502[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mRbac.EditView Daily Rbac_Local_Basic-2.0[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view node(s)[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:700[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:528
    DEBUG: 2019/11/13 08:05:01 START_TEST Rbac.EditView
    DEBUG: 2019/11/13 08:05:01 Login to cluster
    DEBUG: 2019/11/13 08:05:01 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:05:01 Updating inventory struct
    DEBUG: 2019/11/13 08:05:02 Checking stale storage resources
    DEBUG: 2019/11/13 08:05:02 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:05:02 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:05:02 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:05:10 Creating storage classes
    DEBUG: 2019/11/13 08:05:20 User Logout
[It] User can edit/view node(s)
  /gocode/main/test/e2e/tests/rbac.go:700
    DEBUG: 2019/11/13 08:05:21 Creating group, user with role(s)
    DEBUG: 2019/11/13 08:05:22 Creating group jacksgroup with node-edit role(s)
    DEBUG: 2019/11/13 08:05:22 Creating jack user in jacksgroup group
    DEBUG: 2019/11/13 08:05:22 Login as jack user
    DEBUG: 2019/11/13 08:05:22 Listing nodes
    DEBUG: 2019/11/13 08:05:22 Performing operations on all nodes
    DEBUG: 2019/11/13 08:05:22 Node : appserv53
    DEBUG: 2019/11/13 08:05:22 Getting node info
    DEBUG: 2019/11/13 08:05:22 Getting network status of node
    DEBUG: 2019/11/13 08:05:22 Getting health status of node
    DEBUG: 2019/11/13 08:05:23 Rediscovering a node
    DEBUG: 2019/11/13 08:05:23 Getting label of a node
    DEBUG: 2019/11/13 08:05:23 Setting label to a node
    DEBUG: 2019/11/13 08:05:23 Node : appserv54
    DEBUG: 2019/11/13 08:05:23 Getting node info
    DEBUG: 2019/11/13 08:05:23 Getting network status of node
    DEBUG: 2019/11/13 08:05:23 Getting health status of node
    DEBUG: 2019/11/13 08:05:23 Rediscovering a node
    DEBUG: 2019/11/13 08:05:23 Getting label of a node
    DEBUG: 2019/11/13 08:05:23 Setting label to a node
    DEBUG: 2019/11/13 08:05:23 Node : appserv55
    DEBUG: 2019/11/13 08:05:23 Getting node info
    DEBUG: 2019/11/13 08:05:23 Getting network status of node
    DEBUG: 2019/11/13 08:05:24 Getting health status of node
    DEBUG: 2019/11/13 08:05:24 Rediscovering a node
    DEBUG: 2019/11/13 08:05:24 Getting label of a node
    DEBUG: 2019/11/13 08:05:24 Setting label to a node
    DEBUG: 2019/11/13 08:05:24 Getting cluster status
    DEBUG: 2019/11/13 08:05:24 Getting cluster non master node
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:539
    DEBUG: 2019/11/13 08:05:24 User Logout
    DEBUG: 2019/11/13 08:05:25 END_TEST Rbac.EditView Time-taken : 24.428702214
    DEBUG: 2019/11/13 08:05:25 Checking stale storage resources
    DEBUG: 2019/11/13 08:05:25 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:05:25 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:05:25 Checking storage stale resources on the node: appserv55

[36m[1mS [SKIPPING] [24.541 seconds][0m
Rbac.EditView Daily Rbac_Local_Basic-2.0
[90m/gocode/main/test/e2e/tests/rbac.go:518[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:520[0m
    [36m[1mUser can edit/view node(s) [It][0m
    [90m/gocode/main/test/e2e/tests/rbac.go:700[0m

    [36m TODO : write function for GetClusterNonQuorumNode because we can't remove Quorum node[0m

    /gocode/main/test/e2e/tests/rbac.go:752
[90m------------------------------[0m
[0mHostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2[0m [90mCreate single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted[0m 
  [1mShould create multiple host-networks with node selector for more than 1 node, check if endpoints got created, delete host network, check if endpoints deleted[0m
  [37m/gocode/main/test/e2e/tests/network.go:524[0m
[BeforeEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:499
    DEBUG: 2019/11/13 08:05:25 START_TEST HostNetwork.Basic
    DEBUG: 2019/11/13 08:05:25 Login to cluster
    DEBUG: 2019/11/13 08:05:26 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:05:26 Updating inventory struct
    DEBUG: 2019/11/13 08:05:27 Checking stale storage resources
    DEBUG: 2019/11/13 08:05:27 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:05:27 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:05:27 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:05:35 Creating storage classes
[It] Should create multiple host-networks with node selector for more than 1 node, check if endpoints got created, delete host network, check if endpoints deleted
  /gocode/main/test/e2e/tests/network.go:524
    DEBUG: 2019/11/13 08:05:45 Creating perf-tier : template
    DEBUG: 2019/11/13 08:05:45 Disabling helm feature
    DEBUG: 2019/11/13 08:05:46 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:05:49 Assigning label to nodes where the host network should get created
    DEBUG: 2019/11/13 08:05:49 Assigned label : label=hostNetwork to node : appserv53
    DEBUG: 2019/11/13 08:05:49 Assigned label : label=hostNetwork to node : appserv54
    DEBUG: 2019/11/13 08:05:49 Updating non default network as host network
    DEBUG: 2019/11/13 08:05:50 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/13 08:05:50 Waiting for allocation of VF to endpoints of network : blue
    DEBUG: 2019/11/13 08:06:10 VF enp129s3f3d2 allocated to endpoint blue.appserv53 successfully
    DEBUG: 2019/11/13 08:06:11 VF enp129s3 allocated to endpoint blue.appserv54 successfully
    DEBUG: 2019/11/13 08:06:11 Updating default network as host network
    DEBUG: 2019/11/13 08:06:11 Vnic allocation for host-network interface is successful
    DEBUG: 2019/11/13 08:06:11 Waiting for allocation of VF to endpoints of network : default
    DEBUG: 2019/11/13 08:06:12 VF enp129s2f4 allocated to endpoint default.appserv53 successfully
    DEBUG: 2019/11/13 08:06:12 VF enp129s1f6d2 allocated to endpoint default.appserv54 successfully
    DEBUG: 2019/11/13 08:06:12 Checking enpoint are provisioned and attached or not for hostnetwork : blue
    DEBUG: 2019/11/13 08:06:13 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/13 08:06:13 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/13 08:06:15 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/13 08:06:45 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/13 08:06:47 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/13 08:06:48 tx_bytes on host-network interface enp129s3 when data transfer starts is : 156745211618 bytes
    DEBUG: 2019/11/13 08:07:18 tx_bytes on host-network interface enp129s3 after waiting time is : 190660457740 bytes
    DEBUG: 2019/11/13 08:07:18 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/13 08:07:18 Deleting pods:
    DEBUG: 2019/11/13 08:07:18 Deleting pods : 
    DEBUG: 2019/11/13 08:07:31 Deleting services: 
    DEBUG: 2019/11/13 08:07:31 Deleting service(s)
    DEBUG: 2019/11/13 08:07:31 Checking enpoint are provisioned and attached or not for hostnetwork : default
    DEBUG: 2019/11/13 08:07:31 Creating 1 pairs of iperf client-server pods with high QoS
    DEBUG: 2019/11/13 08:07:31 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/13 08:07:34 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/13 08:08:04 Creating iperf Client pod: iperf-client-1
    DEBUG: 2019/11/13 08:08:07 Getting host-network interface of a cluster node where client pods scheduled
    DEBUG: 2019/11/13 08:08:07 tx_bytes on host-network interface enp129s1f6d2 when data transfer starts is : 430449529462 bytes
    DEBUG: 2019/11/13 08:08:38 tx_bytes on host-network interface enp129s1f6d2 after waiting time is : 464628441434 bytes
    DEBUG: 2019/11/13 08:08:38 Data is going through host-network interface. Byte direction :  tx_bytes
    DEBUG: 2019/11/13 08:08:38 Deleting pods:
    DEBUG: 2019/11/13 08:08:38 Deleting pods : 
    DEBUG: 2019/11/13 08:08:51 Deleting services: 
    DEBUG: 2019/11/13 08:08:51 Deleting service(s)
    DEBUG: 2019/11/13 08:08:51 Deleting host-network blue
    DEBUG: 2019/11/13 08:08:53 Endpoint blue.appserv53 deleted successfully
    DEBUG: 2019/11/13 08:08:53 Endpoint blue.appserv54 deleted successfully
    DEBUG: 2019/11/13 08:08:53 Endpoints created by host-network got deleted
    DEBUG: 2019/11/13 08:08:53 Deleting host-network default
    DEBUG: 2019/11/13 08:08:57 Endpoint default.appserv53 deleted successfully
    DEBUG: 2019/11/13 08:08:57 Endpoint default.appserv54 deleted successfully
    DEBUG: 2019/11/13 08:08:57 Endpoints created by host-network got deleted
    DEBUG: 2019/11/13 08:08:57 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:08:57 VNICS usage is : 0
    DEBUG: 2019/11/13 08:08:57 After deleting host networks, vnics released by host-network endpoints
    DEBUG: 2019/11/13 08:08:57 Removing hostNetwork label from the nodes : [appserv53 appserv54]
    DEBUG: 2019/11/13 08:08:57 Removed label : label from node : appserv53
    DEBUG: 2019/11/13 08:08:57 Removed label : label from node : appserv54
    DEBUG: 2019/11/13 08:08:57 Creating networks deleted by this TC
    DEBUG: 2019/11/13 08:08:57 Deleting the perf-tier : template
    DEBUG: 2019/11/13 08:08:58 Enabling helm feature.
[AfterEach] Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  /gocode/main/test/e2e/tests/network.go:505
    DEBUG: 2019/11/13 08:09:08 END_TEST HostNetwork.Basic Time-taken : 222.458913432
    DEBUG: 2019/11/13 08:09:08 Checking stale storage resources
    DEBUG: 2019/11/13 08:09:08 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:09:08 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:09:08 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:222.572 seconds][0m
HostNetwork.Basic Daily N_HostNetwork-1.0 N_HostNetwork-1.2 N_HostNetwork-3.0 N_HostNetwork-1.4 N_HostNetwork-1.6 N_HostNetwork-2.0 N_HostNetwork-2.2
[90m/gocode/main/test/e2e/tests/network.go:490[0m
  Create single or multiple host-networks, check if endpoints got created, create pods, ensure data is going through host network interface, delete host network, check if host-network related endpoints got deleted
  [90m/gocode/main/test/e2e/tests/network.go:491[0m
    Should create multiple host-networks with node selector for more than 1 node, check if endpoints got created, delete host network, check if endpoints deleted
    [90m/gocode/main/test/e2e/tests/network.go:524[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mClusterIp.BasicWithHostNetwork Daily CI_basic-1.1[0m [90mCreate nginx & httperf pod, create clusterIp service, Check packets on host-network interface[0m 
  [1mCreate nginx & httperf pod, create clusterIp service, Check packets on host-network interface[0m
  [37m/gocode/main/test/e2e/tests/kubeservices.go:196[0m
[BeforeEach] Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  /gocode/main/test/e2e/tests/kubeservices.go:185
    DEBUG: 2019/11/13 08:09:08 START_TEST ClusterIp.BasiciWithHostNetwork
    DEBUG: 2019/11/13 08:09:08 Login to cluster
    DEBUG: 2019/11/13 08:09:08 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:09:09 Updating inventory struct
    DEBUG: 2019/11/13 08:09:09 Checking stale storage resources
    DEBUG: 2019/11/13 08:09:09 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:09:09 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:09:09 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:09:17 Creating storage classes
[It] Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  /gocode/main/test/e2e/tests/kubeservices.go:196
    DEBUG: 2019/11/13 08:09:27 Updating non-default network blue to host network
    DEBUG: 2019/11/13 08:09:27 Creating replication-controller nginx-rc
    DEBUG: 2019/11/13 08:09:33 List the pods.
    DEBUG: 2019/11/13 08:09:33 Create nginx clusterIp service
    DEBUG: 2019/11/13 08:09:33 Creating httperf pod
    DEBUG: 2019/11/13 08:09:36 Getting interface of node where httperf pod scheduled
    DEBUG: 2019/11/13 08:09:36 Checking rx_packets on host-network interface enp129s4f5
    DEBUG: 2019/11/13 08:09:36 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:10:11 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/13 08:10:11 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:10:45 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/13 08:10:46 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:11:20 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/13 08:11:20 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:11:55 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/13 08:11:55 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:12:29 Data is going through host-network. packet direction:  rx_packets
    DEBUG: 2019/11/13 08:12:29 Checking tx_packets on host-network interface enp129s4f5
    DEBUG: 2019/11/13 08:12:30 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:13:04 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/13 08:13:04 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:13:38 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/13 08:13:39 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:14:13 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/13 08:14:13 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:14:47 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/13 08:14:48 Logging into httperf pod and sending requests to nginx service
    DEBUG: 2019/11/13 08:15:25 Data is going through host-network. packet direction:  tx_packets
    DEBUG: 2019/11/13 08:15:25 Delete rc nginx-rc 
    DEBUG: 2019/11/13 08:15:25 List the pods.
    DEBUG: 2019/11/13 08:15:43 Deleting httperf pod: 
    DEBUG: 2019/11/13 08:16:27 Deleting nginx service: 
    DEBUG: 2019/11/13 08:16:27 Waiting 30 sec for vnic usage to get reduced
    DEBUG: 2019/11/13 08:16:57 Delete the host network.
    DEBUG: 2019/11/13 08:17:03 Create the non-default network.
[AfterEach] Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  /gocode/main/test/e2e/tests/kubeservices.go:191
    DEBUG: 2019/11/13 08:17:03 END_TEST ClusterIp.BasicWithHostNetwork Time-taken : 474.710373823
    DEBUG: 2019/11/13 08:17:03 Checking stale storage resources
    DEBUG: 2019/11/13 08:17:03 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:17:03 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:17:03 Checking storage stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:474.821 seconds][0m
ClusterIp.BasicWithHostNetwork Daily CI_basic-1.1
[90m/gocode/main/test/e2e/tests/kubeservices.go:168[0m
  Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
  [90m/gocode/main/test/e2e/tests/kubeservices.go:169[0m
    Create nginx & httperf pod, create clusterIp service, Check packets on host-network interface
    [90m/gocode/main/test/e2e/tests/kubeservices.go:196[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.IpAddressPoolLimitTests Daily N_Requirements-1.5 N_Requirements-1.6[0m [90mExhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.[0m 
  [1mExhaust ip alloaction pool of network X, Create a pod with network X, Pod must go into pending state.[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:3044[0m
[BeforeEach] Exhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.
  /gocode/main/test/e2e/tests/network-pod.go:3033
    DEBUG: 2019/11/13 08:17:03 START_TEST Network.IpAddressPoolLimitTests
    DEBUG: 2019/11/13 08:17:03 Login to cluster
    DEBUG: 2019/11/13 08:17:03 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:17:03 Updating inventory struct
    DEBUG: 2019/11/13 08:17:04 Checking stale storage resources
    DEBUG: 2019/11/13 08:17:04 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:17:04 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:17:04 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:17:12 Creating storage classes
[It] Exhaust ip alloaction pool of network X, Create a pod with network X, Pod must go into pending state.
  /gocode/main/test/e2e/tests/network-pod.go:3044
    DEBUG: 2019/11/13 08:17:22 Creating test a network with ip pool of 10 addresss: 
    DEBUG: 2019/11/13 08:17:22 Creating pods to exhaust ipaddress pool: 
    DEBUG: 2019/11/13 08:17:22 Creating 10 pods of docker.io/redis:3.0.5 image with network : e2e-test-network and qos : medium
    DEBUG: 2019/11/13 08:17:50 Ensure that network ip pool is exhausted: 
    DEBUG: 2019/11/13 08:17:50 Create one more pod in private network: 
    DEBUG: 2019/11/13 08:17:50 Creating 1 pods of docker.io/redis:3.0.5 image with network : e2e-test-network and qos : medium
    DEBUG: 2019/11/13 08:17:55 Ensure pod is pending state: 
    DEBUG: 2019/11/13 08:17:55 Delete pods: 
    DEBUG: 2019/11/13 08:17:55 Deleting pods : 
    DEBUG: 2019/11/13 08:18:16 Delete test network: 
[AfterEach] Exhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.
  /gocode/main/test/e2e/tests/network-pod.go:3039
    DEBUG: 2019/11/13 08:18:26 END_TEST Network.IpAddressPoolLimitTests : 83.022279499
    DEBUG: 2019/11/13 08:18:26 Checking stale storage resources
    DEBUG: 2019/11/13 08:18:26 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:18:26 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:18:26 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:83.131 seconds][0m
Network.IpAddressPoolLimitTests Daily N_Requirements-1.5 N_Requirements-1.6
[90m/gocode/main/test/e2e/tests/network-pod.go:3025[0m
  Exhaust ip alloaction pool of  a network X, Create a pod with network X, Pod must go into pending state.
  [90m/gocode/main/test/e2e/tests/network-pod.go:3026[0m
    Exhaust ip alloaction pool of network X, Create a pod with network X, Pod must go into pending state.
    [90m/gocode/main/test/e2e/tests/network-pod.go:3044[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mVolume.NameLengthCheck Management Daily M_Volume-1.1[0m [90mwhen volume name is[0m 
  [1mequal to defined character limit[0m
  [37m/gocode/main/test/e2e/tests/volume.go:233[0m
[BeforeEach] when volume name is
  /gocode/main/test/e2e/tests/volume.go:220
    DEBUG: 2019/11/13 08:18:26 START_TEST Volume.NameLengthCheck
    DEBUG: 2019/11/13 08:18:26 Login to cluster
    DEBUG: 2019/11/13 08:18:26 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:18:27 Updating inventory struct
    DEBUG: 2019/11/13 08:18:27 Checking stale storage resources
    DEBUG: 2019/11/13 08:18:27 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:18:27 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:18:27 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:18:35 Creating storage classes
[It] equal to defined character limit
  /gocode/main/test/e2e/tests/volume.go:233
    DEBUG: 2019/11/13 08:18:45 Creating the volume
    DEBUG: 2019/11/13 08:18:45 Getting Volume
    DEBUG: 2019/11/13 08:18:45 Verify that volume status is Available
    DEBUG: 2019/11/13 08:18:45 Deleting the volume
[AfterEach] when volume name is
  /gocode/main/test/e2e/tests/volume.go:228
    DEBUG: 2019/11/13 08:19:08 END_TEST Volume.NameLengthCheck Time-taken : 42.131466103
    DEBUG: 2019/11/13 08:19:08 Checking stale storage resources
    DEBUG: 2019/11/13 08:19:08 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:19:08 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:19:08 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:42.237 seconds][0m
Volume.NameLengthCheck Management Daily M_Volume-1.1
[90m/gocode/main/test/e2e/tests/volume.go:210[0m
  when volume name is
  [90m/gocode/main/test/e2e/tests/volume.go:213[0m
    equal to defined character limit
    [90m/gocode/main/test/e2e/tests/volume.go:233[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRemoteStorage.RebootTarget Daily RS_Reboot-1.3[0m [90mreboot target node after running IO and verify data before and after reboot[0m 
  [1mreboot target node after running IO and verify data before and after reboot[0m
  [37m/gocode/main/test/e2e/tests/volume.go:2575[0m
[BeforeEach] reboot target node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2560
    DEBUG: 2019/11/13 08:19:08 START_TEST RemoteStorage.RebootTarget
    DEBUG: 2019/11/13 08:19:08 Login to cluster
    DEBUG: 2019/11/13 08:19:09 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:19:09 Updating inventory struct
    DEBUG: 2019/11/13 08:19:10 Checking stale storage resources
    DEBUG: 2019/11/13 08:19:10 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:19:10 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:19:10 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:19:17 Creating storage classes
[It] reboot target node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2575
    DEBUG: 2019/11/13 08:19:27 Verifying whether FBM and L1 usage is zero across all nodes
    DEBUG: 2019/11/13 08:19:34 FBM and L1 usage is Zero across all nodes

    DEBUG: 2019/11/13 08:19:34 Creating 4 volumes of random sizes
    DEBUG: 2019/11/13 08:19:34 Mirror Count: 1
    DEBUG: 2019/11/13 08:19:34 Attaching all 4 volumes
    DEBUG: 2019/11/13 08:19:54 Initiator node : appserv53
    DEBUG: 2019/11/13 08:19:54 Nodes to reboot :[appserv54]
    DEBUG: 2019/11/13 08:19:56 Running WRITE fio job on node : appserv53
    DEBUG: 2019/11/13 08:19:56 FIO Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randwrite --numjobs=1 --do_verify=0 --verify_state_save=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --runtime=120 --blocksize=64K --iodepth=16  --time_based  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1

    DEBUG: 2019/11/13 08:21:57 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/13 08:21:57 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=64K --iodepth=16  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/13 08:23:22 Getting cluster quorum nodes
    DEBUG: 2019/11/13 08:23:22 Powering OFF the node appserv54
    DEBUG: 2019/11/13 08:23:22 Node 172.16.6.154 took 0 seconds to power off
    DEBUG: 2019/11/13 08:23:22 Ensuring that appserv54 node is unreachable: 
    DEBUG: 2019/11/13 08:23:22 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:23:31 Polling to check until node: appserv54 goes down
    DEBUG: 2019/11/13 08:25:32 Powering ON the node appserv54
    DEBUG: 2019/11/13 08:25:32 Node 172.16.6.154 took 0 seconds to power on
    DEBUG: 2019/11/13 08:25:32 Checking if node appserv54 is reachable or not: 
    DEBUG: 2019/11/13 08:25:32 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:25:51 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:26:08 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:26:25 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:26:42 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:26:59 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:27:16 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:27:33 Executing ping command: ping  -c 5 -W 5 appserv54
    DEBUG: 2019/11/13 08:27:37 appserv54 is pingable from local machine
    DEBUG: 2019/11/13 08:27:37 Checking ssh port is up or not on node: appserv54
    DEBUG: 2019/11/13 08:28:07 Waiting for the node(s) to come up and rejoin the cluster
    DEBUG: 2019/11/13 08:28:07 Found '3' nodes
    DEBUG: 2019/11/13 08:28:07 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/13 08:28:07 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/13 08:29:08 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/13 08:29:18 After power cycle/reboot, updating timestamp of node : appserv54
    DEBUG: 2019/11/13 08:29:21 Getting cluster quorum nodes
    DEBUG: 2019/11/13 08:30:21 Updating inventory struct
    DEBUG: 2019/11/13 08:30:21 Waiting for nodes to come up, will wait upto 800 seconds
    DEBUG: 2019/11/13 08:30:33 Nodes are up, waiting for armada to start
.
    DEBUG: 2019/11/13 08:31:43 Waiting for the nodes to go into Ready state
    DEBUG: 2019/11/13 08:31:43 Found '3' nodes
    DEBUG: 2019/11/13 08:31:43 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/13 08:31:43 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/13 08:31:44 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/13 08:31:44 Waiting for volumes to come into Attached state after rebooting cluster nodes.
    DEBUG: 2019/11/13 08:31:44 Detaching all 4 volumes
    DEBUG: 2019/11/13 08:31:46 Re-attaching all 4 volumes
    DEBUG: 2019/11/13 08:32:07 Comparing Volume's UUID with nvme id-ns for all volumes
    DEBUG: 2019/11/13 08:32:09 Comparing the device path & uuid on initiator before and after reboot
    DEBUG: 2019/11/13 08:32:13 Running VERIFY IOs on all plexes
    DEBUG: 2019/11/13 08:32:13 Running Verify IOs on node : appserv53 and plex p0
Command : sudo /usr/local/bin/fio --ioengine=libaio --direct=1 --group_reporting --gtod_reduce=1 --clat_percentiles=0 --rw=randread --numjobs=1 --do_verify=1 --verify_state_load=1 --verify=pattern --verify_pattern=0xff%o\"akpz\"-12 --verify_interval=4096 --verify_fatal=1 --verify_dump=1  --blocksize=64K --iodepth=16  --name=job1 --filename=/dev/nvme1n1 --name=job2 --filename=/dev/nvme2n1 --name=job3 --filename=/dev/nvme3n1 --name=job4 --filename=/dev/nvme4n1
    DEBUG: 2019/11/13 08:33:35 Successfully completed Verification on all the volumes
    DEBUG: 2019/11/13 08:33:35 Detach & Delete all volumes
[AfterEach] reboot target node after running IO and verify data before and after reboot
  /gocode/main/test/e2e/tests/volume.go:2570
    DEBUG: 2019/11/13 08:35:10 END_TEST RemoteStorage.RebootTarget Time-taken : 961.592335344
    DEBUG: 2019/11/13 08:35:10 Checking stale storage resources
    DEBUG: 2019/11/13 08:35:10 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:35:10 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:35:10 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:961.699 seconds][0m
RemoteStorage.RebootTarget Daily RS_Reboot-1.3
[90m/gocode/main/test/e2e/tests/volume.go:2553[0m
  reboot target node after running IO and verify data before and after reboot
  [90m/gocode/main/test/e2e/tests/volume.go:2555[0m
    reboot target node after running IO and verify data before and after reboot
    [90m/gocode/main/test/e2e/tests/volume.go:2575[0m
[90m------------------------------[0m
[0mBenchmarking.ReadThroughputLocalVolumes Sanity AT_Benchmark-3.1 Qos Multizone[0m [90mStorage benchmarking, all volumes on local node.[0m 
  [1mStorage benchmarking, all volumes on local node.[0m
  [37m/gocode/main/test/e2e/tests/benchmarking.go:590[0m
[BeforeEach] Storage benchmarking, all volumes on local node.
  /gocode/main/test/e2e/tests/benchmarking.go:579
    DEBUG: 2019/11/13 08:35:10 Login to cluster
    DEBUG: 2019/11/13 08:35:10 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:35:10 Updating inventory struct
    DEBUG: 2019/11/13 08:35:11 Checking stale storage resources
    DEBUG: 2019/11/13 08:35:11 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:35:11 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:35:11 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:35:19 Creating storage classes
    DEBUG: 2019/11/13 08:35:31 START_TEST Benchmarking.ReadThroughputLocalVolumes
[It] Storage benchmarking, all volumes on local node.
  /gocode/main/test/e2e/tests/benchmarking.go:590
    DEBUG: 2019/11/13 08:35:31 Creating 8 Dynamic Persistent Volume Claims (PVCs). Mirror count: 1. Selector: node=node0
    DEBUG: 2019/11/13 08:35:32 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:32 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:32 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:33 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:33 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:33 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:34 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:34 Created PVC successfully.
    DEBUG: 2019/11/13 08:35:36 Creating 8 fio pods: 
    DEBUG: 2019/11/13 08:35:39 Checking if given pods are in Running state
    DEBUG: 2019/11/13 08:35:58 Wait for volumes to move into attached state: 
    DEBUG: 2019/11/13 08:35:59 Waiting for pods move into Completed state
    DEBUG: 2019/11/13 08:36:31 Deleting pods: 
    DEBUG: 2019/11/13 08:36:31 Deleting pods : 
    DEBUG: 2019/11/13 08:36:32 Volume "pvc-00042a03-bd3c-451f-ace4-0d9fdfc27ec4" is in "Down" state. Waiting for it to come in "Available" state.
    DEBUG: 2019/11/13 08:36:36 Volume "pvc-40f1de14-1f5c-47a2-aeef-a07fc5519b30" is in "Attached" state. Waiting for it to come in "Available" state.
    DEBUG: 2019/11/13 08:36:38 Volume "pvc-429294fb-0016-4d0d-8c67-74fcba5c936a" is in "Attached" state. Waiting for it to come in "Available" state.
    DEBUG: 2019/11/13 08:36:38 Volume "pvc-438c7a5d-96aa-4c53-ac7d-c88bdbbcd2b3" is in "Attached" state. Waiting for it to come in "Available" state.
    DEBUG: 2019/11/13 08:36:38 Volume "pvc-7d58af64-5246-42f9-bb9c-04b3070b3842" is in "Down" state. Waiting for it to come in "Available" state.
    DEBUG: 2019/11/13 08:36:38 Volume "pvc-a4599ead-cc40-45bd-bfcd-dc34e55a456d" is in "Down" state. Waiting for it to come in "Available" state.
    DEBUG: 2019/11/13 08:36:38 Volume "pvc-b6911d9e-467c-4c2f-b306-6feea3d436a8" is in "Attached" state. Waiting for it to come in "Available" state.
    DEBUG: 2019/11/13 08:36:38 Creating 8 fio pods: 
    DEBUG: 2019/11/13 08:36:41 Checking if given pods are in Running state
    DEBUG: 2019/11/13 08:37:01 Wait for volumes to move into attached state: 
    DEBUG: 2019/11/13 08:37:01 Waiting for 180 sec., so that Prometheus will have some stats...
    DEBUG: 2019/11/13 08:40:01 Validate qos for every volume: 
    DEBUG: 2019/11/13 08:40:02 Validate read throughput.
    DEBUG: 2019/11/13 08:40:02 Node: appserv53: operation read, Expected IOPs: 500000, Read IOPs: 864121, Write IOPs: 0
    DEBUG: 2019/11/13 08:40:02 Read throughput: 864121.
    DEBUG: 2019/11/13 08:40:02 Deleting pods : 
    DEBUG: 2019/11/13 08:40:13 Wait for volumes to come in Available state: 
    DEBUG: 2019/11/13 08:40:13 Delete PVCs: 
    DEBUG: 2019/11/13 08:40:14 Waiting for volumes to get deleted: 
[AfterEach] Storage benchmarking, all volumes on local node.
  /gocode/main/test/e2e/tests/benchmarking.go:585
    DEBUG: 2019/11/13 08:41:09 END_TEST Benchmarking.ReadThroughputLocalVolumes Time-taken : 338.239017742
    DEBUG: 2019/11/13 08:41:09 Checking stale storage resources
    DEBUG: 2019/11/13 08:41:09 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:41:09 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:41:09 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:359.597 seconds][0m
Benchmarking.ReadThroughputLocalVolumes Sanity AT_Benchmark-3.1 Qos Multizone
[90m/gocode/main/test/e2e/tests/benchmarking.go:562[0m
  Storage benchmarking, all volumes on local node.
  [90m/gocode/main/test/e2e/tests/benchmarking.go:564[0m
    Storage benchmarking, all volumes on local node.
    [90m/gocode/main/test/e2e/tests/benchmarking.go:590[0m
[90m------------------------------[0m
[0mCluster.RemoveNodeWithPods Management Daily M_Cluster-1.5[0m [90mwhen a node is decommissioned and removed from a cluster[0m 
  [1mshould be created, decommissioned, and removed[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:366[0m
[BeforeEach] when a node is decommissioned and removed from a cluster
  /gocode/main/test/e2e/tests/cluster.go:347
    DEBUG: 2019/11/13 08:41:09 START_TEST Cluster.RemoveNodeWithPods
[AfterEach] when a node is decommissioned and removed from a cluster
  /gocode/main/test/e2e/tests/cluster.go:361
    DEBUG: 2019/11/13 08:41:09 END_TEST Cluster.RemoveNodeWithPods Time-taken : 0.000637665
    DEBUG: 2019/11/13 08:41:09 Checking stale storage resources
    DEBUG: 2019/11/13 08:41:10 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:41:10 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:41:10 Checking storage stale resources on the node: appserv54

[36m[1mS [SKIPPING] in Spec Setup (BeforeEach) [0.109 seconds][0m
Cluster.RemoveNodeWithPods Management Daily M_Cluster-1.5
[90m/gocode/main/test/e2e/tests/cluster.go:342[0m
  when a node is decommissioned and removed from a cluster
  [90m/gocode/main/test/e2e/tests/cluster.go:343[0m
    [36m[1mshould be created, decommissioned, and removed [BeforeEach][0m
    [90m/gocode/main/test/e2e/tests/cluster.go:366[0m

    [36mSkipping remove node with pods for now[0m

    /gocode/main/test/e2e/tests/cluster.go:351
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9[0m [90mPing an external IP from from a pod[0m 
  [1mPing external IP from a pod created without using any network[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:601[0m
[BeforeEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:487
    DEBUG: 2019/11/13 08:41:10 START_TEST Network.PingExternalIP
    DEBUG: 2019/11/13 08:41:10 Login to cluster
    DEBUG: 2019/11/13 08:41:10 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:41:10 Updating inventory struct
    DEBUG: 2019/11/13 08:41:11 Checking stale storage resources
    DEBUG: 2019/11/13 08:41:11 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:41:11 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:41:11 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:41:19 Creating storage classes
[It] Ping external IP from a pod created without using any network
  /gocode/main/test/e2e/tests/network-pod.go:601
    DEBUG: 2019/11/13 08:41:29 Creating 1 pods of docker.io/redis:3.0.5 image with network : none and qos : 
    DEBUG: 2019/11/13 08:41:31 Trying to ping google-public-dns-a.google.com from pod e2etest-pod-1
    DEBUG: 2019/11/13 08:41:31 google-public-dns-a.google.com is pingable from pod e2etest-pod-1 (172.20.0.6)
    DEBUG: 2019/11/13 08:41:31 Getting managment inteface from pod host ( appserv54 ) 
    DEBUG: 2019/11/13 08:41:32 Management interface of appserv54 is 172.20.0.1

    DEBUG: 2019/11/13 08:41:32 Matching default gateway of pod e2etest-pod-1 with 172.20.0.1 
    DEBUG: 2019/11/13 08:41:32 Default gateway of e2etest-pod-1 is 172.20.0.1
    DEBUG: 2019/11/13 08:41:32 Deleting the pod: e2etest-pod-1
[AfterEach] Ping an external IP from from a pod
  /gocode/main/test/e2e/tests/network-pod.go:496
    DEBUG: 2019/11/13 08:41:34 END_TEST Network.PingExternalIP Time-taken : 24.718828488
    DEBUG: 2019/11/13 08:41:34 Checking stale storage resources
    DEBUG: 2019/11/13 08:41:34 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:41:34 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:41:34 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:24.844 seconds][0m
Network.PingExternalIP Daily N_Basic-1.5 N_Basic-1.6 N_Basic-1.7 N_Basic-1.8 N_Basic-1.9
[90m/gocode/main/test/e2e/tests/network-pod.go:480[0m
  Ping an external IP from from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:481[0m
    Ping external IP from a pod created without using any network
    [90m/gocode/main/test/e2e/tests/network-pod.go:601[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.Cluster Daily Rbac_Cluster-1.0 Rbac_Cluster-1.1[0m [90mrbac cluster test[0m 
  [1mcluster-edit test[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:276[0m
[BeforeEach] rbac cluster test
  /gocode/main/test/e2e/tests/rbac.go:215
    DEBUG: 2019/11/13 08:41:34 START_TEST Rbac.Cluster
    DEBUG: 2019/11/13 08:41:34 Login to cluster
    DEBUG: 2019/11/13 08:41:35 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:41:35 Updating inventory struct
    DEBUG: 2019/11/13 08:41:36 Checking stale storage resources
    DEBUG: 2019/11/13 08:41:36 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:41:36 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:41:36 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:41:44 Creating storage classes
[It] cluster-edit test
  /gocode/main/test/e2e/tests/rbac.go:276
    DEBUG: 2019/11/13 08:41:55 Create group
    DEBUG: 2019/11/13 08:41:55 Create user
    DEBUG: 2019/11/13 08:41:55 Login as user
    DEBUG: 2019/11/13 08:41:56 Create Network
    DEBUG: 2019/11/13 08:41:56 List Networks
    DEBUG: 2019/11/13 08:41:56 List users
    DEBUG: 2019/11/13 08:41:56 List groups
    DEBUG: 2019/11/13 08:41:56 List roles
    DEBUG: 2019/11/13 08:41:56 List auth-server
    DEBUG: 2019/11/13 08:41:56 Create user
    DEBUG: 2019/11/13 08:41:56 Create group
    DEBUG: 2019/11/13 08:41:56 Create role
    DEBUG: 2019/11/13 08:41:56 Create auth-server
[AfterEach] rbac cluster test
  /gocode/main/test/e2e/tests/rbac.go:222
    DEBUG: 2019/11/13 08:41:58 END_TEST Rbac.Cluster Time-taken : 23.132802921
    DEBUG: 2019/11/13 08:41:58 Checking stale storage resources
    DEBUG: 2019/11/13 08:41:58 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:41:58 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:41:58 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:23.241 seconds][0m
Rbac.Cluster Daily Rbac_Cluster-1.0 Rbac_Cluster-1.1
[90m/gocode/main/test/e2e/tests/rbac.go:207[0m
  rbac cluster test
  [90m/gocode/main/test/e2e/tests/rbac.go:210[0m
    cluster-edit test
    [90m/gocode/main/test/e2e/tests/rbac.go:276[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mVolume.LifeCycle Management Sanity SP_Basic-1.0[0m [90mvolume life cycle test[0m 
  [1mconfirm failure of volume ops in busy state[0m
  [37m/gocode/main/test/e2e/tests/volume.go:762[0m
[BeforeEach] volume life cycle test
  /gocode/main/test/e2e/tests/volume.go:749
    DEBUG: 2019/11/13 08:41:58 START_TEST Volume.LifeCycle
    DEBUG: 2019/11/13 08:41:58 Login to cluster
    DEBUG: 2019/11/13 08:41:58 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:41:58 Updating inventory struct
    DEBUG: 2019/11/13 08:41:59 Checking stale storage resources
    DEBUG: 2019/11/13 08:41:59 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:41:59 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:41:59 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:42:07 Creating storage classes
[It] confirm failure of volume ops in busy state
  /gocode/main/test/e2e/tests/volume.go:762
    DEBUG: 2019/11/13 08:42:17 Creating the volume
    DEBUG: 2019/11/13 08:42:17 Create pvc
    DEBUG: 2019/11/13 08:42:17 Created PVC successfully.
    DEBUG: 2019/11/13 08:42:17 Getting Volume
    DEBUG: 2019/11/13 08:42:17 Verify that volume status is Available
    DEBUG: 2019/11/13 08:42:17 Pod 1 with CPU + Memory + Network + Storage
    DEBUG: 2019/11/13 08:42:27 Wait for volume to become Attached
    DEBUG: 2019/11/13 08:42:27 Deleting Attached volume should fail
    DEBUG: 2019/11/13 08:42:27 Pod 2 with CPU + Memory + Network + Storage
    DEBUG: 2019/11/13 08:48:00 Deleting Pod 2
    DEBUG: 2019/11/13 08:48:00 Deleting Pod 1
    DEBUG: 2019/11/13 08:48:15 Wait for volume to become available
    DEBUG: 2019/11/13 08:48:15 Delete PVC
    DEBUG: 2019/11/13 08:48:15 Deleting the volume
[AfterEach] volume life cycle test
  /gocode/main/test/e2e/tests/volume.go:757
    DEBUG: 2019/11/13 08:49:10 END_TEST Volume.LifeCycle Time-taken : 432.089962869
    DEBUG: 2019/11/13 08:49:10 Checking stale storage resources
    DEBUG: 2019/11/13 08:49:10 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:49:10 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:49:10 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:432.197 seconds][0m
Volume.LifeCycle Management Sanity SP_Basic-1.0
[90m/gocode/main/test/e2e/tests/volume.go:740[0m
  volume life cycle test
  [90m/gocode/main/test/e2e/tests/volume.go:743[0m
    confirm failure of volume ops in busy state
    [90m/gocode/main/test/e2e/tests/volume.go:762[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mNetwork.VFsSchedulingWithBestEffortHighQos Daily AT_Scheduling-3.2 Qos Multizone[0m [90mPods with best-effort, high qos should distribute equally amongst the nicIDs[0m 
  [1mCreate pods with best-effort, high qos and check scheduling on nicID(s)[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1688[0m
[BeforeEach] Pods with best-effort, high qos should distribute equally amongst the nicIDs
  /gocode/main/test/e2e/tests/network-pod.go:1672
    DEBUG: 2019/11/13 08:49:10 START_TEST Network.VFsSchedulingWithBestEffortHighQos
    DEBUG: 2019/11/13 08:49:10 Login to cluster
    DEBUG: 2019/11/13 08:49:10 Checking basic Vnic usage
    DEBUG: 2019/11/13 08:49:10 Updating inventory struct
    DEBUG: 2019/11/13 08:49:11 Checking stale storage resources
    DEBUG: 2019/11/13 08:49:11 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 08:49:11 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 08:49:11 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 08:49:19 Creating storage classes
[It] Create pods with best-effort, high qos and check scheduling on nicID(s)
  /gocode/main/test/e2e/tests/network-pod.go:1688
    DEBUG: 2019/11/13 08:49:29 Creating 50 pods with best-effort qos: 
    DEBUG: 2019/11/13 08:49:29 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/13 08:49:29 Getting node label of appserv53: 
    DEBUG: 2019/11/13 08:49:29 Creating iperf server pod: iperf-best-effort-1
    DEBUG: 2019/11/13 08:49:30 Creating iperf server pod: iperf-best-effort-2
    DEBUG: 2019/11/13 08:49:30 Creating iperf server pod: iperf-best-effort-3
    DEBUG: 2019/11/13 08:49:30 Creating iperf server pod: iperf-best-effort-4
    DEBUG: 2019/11/13 08:49:31 Creating iperf server pod: iperf-best-effort-5
    DEBUG: 2019/11/13 08:49:31 Creating iperf server pod: iperf-best-effort-6
    DEBUG: 2019/11/13 08:49:31 Creating iperf server pod: iperf-best-effort-7
    DEBUG: 2019/11/13 08:49:32 Creating iperf server pod: iperf-best-effort-8
    DEBUG: 2019/11/13 08:49:32 Creating iperf server pod: iperf-best-effort-9
    DEBUG: 2019/11/13 08:49:32 Creating iperf server pod: iperf-best-effort-10
    DEBUG: 2019/11/13 08:49:33 Creating iperf server pod: iperf-best-effort-11
    DEBUG: 2019/11/13 08:49:33 Creating iperf server pod: iperf-best-effort-12
    DEBUG: 2019/11/13 08:49:34 Creating iperf server pod: iperf-best-effort-13
    DEBUG: 2019/11/13 08:49:34 Creating iperf server pod: iperf-best-effort-14
    DEBUG: 2019/11/13 08:49:34 Creating iperf server pod: iperf-best-effort-15
    DEBUG: 2019/11/13 08:49:35 Creating iperf server pod: iperf-best-effort-16
    DEBUG: 2019/11/13 08:49:35 Creating iperf server pod: iperf-best-effort-17
    DEBUG: 2019/11/13 08:49:35 Creating iperf server pod: iperf-best-effort-18
    DEBUG: 2019/11/13 08:49:36 Creating iperf server pod: iperf-best-effort-19
    DEBUG: 2019/11/13 08:49:36 Creating iperf server pod: iperf-best-effort-20
    DEBUG: 2019/11/13 08:49:36 Creating iperf server pod: iperf-best-effort-21
    DEBUG: 2019/11/13 08:49:37 Creating iperf server pod: iperf-best-effort-22
    DEBUG: 2019/11/13 08:49:37 Creating iperf server pod: iperf-best-effort-23
    DEBUG: 2019/11/13 08:49:37 Creating iperf server pod: iperf-best-effort-24
    DEBUG: 2019/11/13 08:49:38 Creating iperf server pod: iperf-best-effort-25
    DEBUG: 2019/11/13 08:49:38 Creating iperf server pod: iperf-best-effort-26
    DEBUG: 2019/11/13 08:49:38 Creating iperf server pod: iperf-best-effort-27
    DEBUG: 2019/11/13 08:49:39 Creating iperf server pod: iperf-best-effort-28
    DEBUG: 2019/11/13 08:49:39 Creating iperf server pod: iperf-best-effort-29
    DEBUG: 2019/11/13 08:49:39 Creating iperf server pod: iperf-best-effort-30
    DEBUG: 2019/11/13 08:49:40 Creating iperf server pod: iperf-best-effort-31
    DEBUG: 2019/11/13 08:49:40 Creating iperf server pod: iperf-best-effort-32
    DEBUG: 2019/11/13 08:49:40 Creating iperf server pod: iperf-best-effort-33
    DEBUG: 2019/11/13 08:49:41 Creating iperf server pod: iperf-best-effort-34
    DEBUG: 2019/11/13 08:49:41 Creating iperf server pod: iperf-best-effort-35
    DEBUG: 2019/11/13 08:49:42 Creating iperf server pod: iperf-best-effort-36
    DEBUG: 2019/11/13 08:49:42 Creating iperf server pod: iperf-best-effort-37
    DEBUG: 2019/11/13 08:49:42 Creating iperf server pod: iperf-best-effort-38
    DEBUG: 2019/11/13 08:49:43 Creating iperf server pod: iperf-best-effort-39
    DEBUG: 2019/11/13 08:49:43 Creating iperf server pod: iperf-best-effort-40
    DEBUG: 2019/11/13 08:49:43 Creating iperf server pod: iperf-best-effort-41
    DEBUG: 2019/11/13 08:49:44 Creating iperf server pod: iperf-best-effort-42
    DEBUG: 2019/11/13 08:49:44 Creating iperf server pod: iperf-best-effort-43
    DEBUG: 2019/11/13 08:49:44 Creating iperf server pod: iperf-best-effort-44
    DEBUG: 2019/11/13 08:49:45 Creating iperf server pod: iperf-best-effort-45
    DEBUG: 2019/11/13 08:49:45 Creating iperf server pod: iperf-best-effort-46
    DEBUG: 2019/11/13 08:49:45 Creating iperf server pod: iperf-best-effort-47
    DEBUG: 2019/11/13 08:49:46 Creating iperf server pod: iperf-best-effort-48
    DEBUG: 2019/11/13 08:49:46 Creating iperf server pod: iperf-best-effort-49
    DEBUG: 2019/11/13 08:49:47 Creating iperf server pod: iperf-best-effort-50
    DEBUG: 2019/11/13 08:49:47 Checking if given pods are in Running state
    DEBUG: 2019/11/13 08:50:01 Creating 10 pods with high qos: 
    DEBUG: 2019/11/13 08:50:01 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/13 08:50:01 Getting node label of appserv53: 
    DEBUG: 2019/11/13 08:50:01 Creating iperf server pod: iperf-high-1
    DEBUG: 2019/11/13 08:50:01 Creating iperf server pod: iperf-high-2
    DEBUG: 2019/11/13 08:50:02 Creating iperf server pod: iperf-high-3
    DEBUG: 2019/11/13 08:50:02 Creating iperf server pod: iperf-high-4
    DEBUG: 2019/11/13 08:50:03 Creating iperf server pod: iperf-high-5
    DEBUG: 2019/11/13 08:50:03 Creating iperf server pod: iperf-high-6
    DEBUG: 2019/11/13 08:50:03 Creating iperf server pod: iperf-high-7
    DEBUG: 2019/11/13 08:50:04 Creating iperf server pod: iperf-high-8
    DEBUG: 2019/11/13 08:50:04 Creating iperf server pod: iperf-high-9
    DEBUG: 2019/11/13 08:50:04 Creating iperf server pod: iperf-high-10
    DEBUG: 2019/11/13 08:50:05 Checking if given pods are in Running state
    DEBUG: 2019/11/13 08:50:10 Deleting all the pods: 
[AfterEach] Pods with best-effort, high qos should distribute equally amongst the nicIDs
  /gocode/main/test/e2e/tests/network-pod.go:1683
    DEBUG: 2019/11/13 09:00:07 END_TEST Network.VFsSchedulingWithBestEffortHighQos Time-taken : 657.34557769
    DEBUG: 2019/11/13 09:00:07 Checking stale storage resources
    DEBUG: 2019/11/13 09:00:07 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:00:07 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:00:07 Checking storage stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:657.460 seconds][0m
Network.VFsSchedulingWithBestEffortHighQos Daily AT_Scheduling-3.2 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1666[0m
  Pods with best-effort, high qos should distribute equally amongst the nicIDs
  [90m/gocode/main/test/e2e/tests/network-pod.go:1667[0m
    Create pods with best-effort, high qos and check scheduling on nicID(s)
    [90m/gocode/main/test/e2e/tests/network-pod.go:1688[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mRbac.AllContainerEditViewLocal Daily Rbac_Local_Basic-2.2[0m [90mUser can edit/view in all namespaces with allcontainer edit/view role[0m 
  [1mUser can edit/view allcontainer(s) in all namespaces[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:1089[0m
[BeforeEach] User can edit/view in all namespaces with allcontainer edit/view role
  /gocode/main/test/e2e/tests/rbac.go:1077
    DEBUG: 2019/11/13 09:00:07 START_TEST Rbac.AllContainerEditViewLocal
    DEBUG: 2019/11/13 09:00:07 Login to cluster
    DEBUG: 2019/11/13 09:00:08 Checking basic Vnic usage
    DEBUG: 2019/11/13 09:00:08 Updating inventory struct
    DEBUG: 2019/11/13 09:00:09 Checking stale storage resources
    DEBUG: 2019/11/13 09:00:09 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:00:09 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:00:09 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:00:16 Creating storage classes
[It] User can edit/view allcontainer(s) in all namespaces
  /gocode/main/test/e2e/tests/rbac.go:1089
    DEBUG: 2019/11/13 09:00:26 Creating group group0 with allcontainer-edit role(s)
    DEBUG: 2019/11/13 09:00:26 Creating group group1 with container-edit/nondefault role(s)
    DEBUG: 2019/11/13 09:00:26 Creating group group2 with volume-edit,volumeclaim-edit/nondefault role(s)
    DEBUG: 2019/11/13 09:00:26 Creating group group3 with volume-edit,volumeclaim-edit/default role(s)
    DEBUG: 2019/11/13 09:00:27 Creating local-user0 user
    DEBUG: 2019/11/13 09:00:27 Creating local-user1 user
    DEBUG: 2019/11/13 09:00:27 Creating local-user2 user
    DEBUG: 2019/11/13 09:00:27 Creating local-user3 user
    DEBUG: 2019/11/13 09:00:27 Login as local-user2 user in nondefault namespace
    DEBUG: 2019/11/13 09:00:42 Create volume
    DEBUG: 2019/11/13 09:00:43 Create pvc in nondefault namespace
    DEBUG: 2019/11/13 09:00:43 Created PVC successfully.
    DEBUG: 2019/11/13 09:00:43 Login as local-user3 user in default namespace
    DEBUG: 2019/11/13 09:00:59 Create volume
    DEBUG: 2019/11/13 09:00:59 Create pvc in default namespace
    DEBUG: 2019/11/13 09:00:59 Created PVC successfully.
    DEBUG: 2019/11/13 09:00:59 Login as local-user0 user
    DEBUG: 2019/11/13 09:01:00 Creating a pair of iperf client-server pod.
    DEBUG: 2019/11/13 09:01:00 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/13 09:01:00 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/13 09:01:30 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/13 09:01:30 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:01:31 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:01:33 Creating fio pod fio-pod in default namespace
    DEBUG: 2019/11/13 09:01:38 Creating a pair of iperf client-server pod.
    DEBUG: 2019/11/13 09:01:38 Creating iperf server pod: iperf-serverhigh1
    DEBUG: 2019/11/13 09:01:38 Creating service with name: iperf-serverhigh1
    DEBUG: 2019/11/13 09:02:08 Creating iperf Client pod: iperf-clienthigh1
    DEBUG: 2019/11/13 09:02:08 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:02:09 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:02:10 Creating fio pod fio-pod in nondefault namespace
    DEBUG: 2019/11/13 09:02:15 Editing group0 group with allcontainer-view role(s)
    DEBUG: 2019/11/13 09:02:15 Login as local-user0 user
    DEBUG: 2019/11/13 09:02:16 List pods from default namepsace: 
    DEBUG: 2019/11/13 09:02:16 List pods from nondefault namepsace: 
    DEBUG: 2019/11/13 09:02:16 List PVCs in default namespace
    DEBUG: 2019/11/13 09:02:16 List Endpoints in default namespace
    DEBUG: 2019/11/13 09:02:16 List Services in default namespace
    DEBUG: 2019/11/13 09:02:16 Try to create the pod in default namespace
    DEBUG: 2019/11/13 09:02:16 Try to create service in default namespace
    DEBUG: 2019/11/13 09:02:17 Try to delete pods in default namespace
    DEBUG: 2019/11/13 09:02:17 Try to delete services in default namespace
    DEBUG: 2019/11/13 09:02:17 Try to delete endpoints in default namespace
    DEBUG: 2019/11/13 09:02:17 List PVCs in nondefault namespace
    DEBUG: 2019/11/13 09:02:17 List Endpoints in nondefault namespace
    DEBUG: 2019/11/13 09:02:17 List Services in nondefault namespace
    DEBUG: 2019/11/13 09:02:17 Try to create the pod in default namespace
    DEBUG: 2019/11/13 09:02:18 Try to create service in default namespace
    DEBUG: 2019/11/13 09:02:18 Try to delete pods in nondefault namespace
    DEBUG: 2019/11/13 09:02:18 Try to delete services in nondefault namespace
    DEBUG: 2019/11/13 09:02:18 Try to delete endpoints in nondefault namespace
    DEBUG: 2019/11/13 09:02:19 Editing group0 group with allcontainer-edit role(s)
    DEBUG: 2019/11/13 09:02:19 Login as local-user0 user
    DEBUG: 2019/11/13 09:02:19 List pods from default namepsace: 
    DEBUG: 2019/11/13 09:02:19 Delete pods in default namespace
    DEBUG: 2019/11/13 09:02:35 List PVCs in default namespace
    DEBUG: 2019/11/13 09:02:35 List Services in default namespace
    DEBUG: 2019/11/13 09:02:35 Delete services in default namespace
    DEBUG: 2019/11/13 09:02:36 List Endpoints in default namespace
    DEBUG: 2019/11/13 09:02:36 List pods from nondefault namepsace: 
    DEBUG: 2019/11/13 09:02:36 Delete pods in nondefault namespace
    DEBUG: 2019/11/13 09:02:47 List PVCs in nondefault namespace
    DEBUG: 2019/11/13 09:02:48 List Services in nondefault namespace
    DEBUG: 2019/11/13 09:02:48 Delete services in nondefault namespace
    DEBUG: 2019/11/13 09:02:48 List Endpoints in nondefault namespace
    DEBUG: 2019/11/13 09:02:48 Login as local-user2 user in nondefault namespace
    DEBUG: 2019/11/13 09:03:04 Delete PVC in nondefault namespace
    DEBUG: 2019/11/13 09:03:04 Login as local-user3 user in default namespace
    DEBUG: 2019/11/13 09:03:19 Delete PVC in default namespace
    DEBUG: 2019/11/13 09:03:19 Wait for volumes to come in Available state
    DEBUG: 2019/11/13 09:03:20 Delete Volume
    DEBUG: 2019/11/13 09:05:11 Delete users
    DEBUG: 2019/11/13 09:05:11 Delete groups
[AfterEach] User can edit/view in all namespaces with allcontainer edit/view role
  /gocode/main/test/e2e/tests/rbac.go:1084
    DEBUG: 2019/11/13 09:05:11 END_TEST Rbac.AllContainerEditViewLocal Time-taken : 304.125249053
    DEBUG: 2019/11/13 09:05:11 Checking stale storage resources
    DEBUG: 2019/11/13 09:05:12 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:05:12 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:05:12 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:304.235 seconds][0m
Rbac.AllContainerEditViewLocal Daily Rbac_Local_Basic-2.2
[90m/gocode/main/test/e2e/tests/rbac.go:1065[0m
  User can edit/view in all namespaces with allcontainer edit/view role
  [90m/gocode/main/test/e2e/tests/rbac.go:1067[0m
    User can edit/view allcontainer(s) in all namespaces
    [90m/gocode/main/test/e2e/tests/rbac.go:1089[0m
[90m------------------------------[0m
[0mNetwork.GatewayPing Daily N_Basic-1.15 N_Basic-1.16 Multizone[0m [90mPing gateway of a network from a pod[0m 
  [1mPing gateway of a network(invalid vlan) from a pod[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:399[0m
[BeforeEach] Ping gateway of a network from a pod
  /gocode/main/test/e2e/tests/network-pod.go:373
    DEBUG: 2019/11/13 09:05:12 START_TEST Network.GatewayPing
    DEBUG: 2019/11/13 09:05:12 Login to cluster
    DEBUG: 2019/11/13 09:05:12 Checking basic Vnic usage
    DEBUG: 2019/11/13 09:05:12 Updating inventory struct
    DEBUG: 2019/11/13 09:05:13 Checking stale storage resources
    DEBUG: 2019/11/13 09:05:13 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:05:13 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:05:13 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:05:21 Creating storage classes
    DEBUG: 2019/11/13 09:05:34 Creating a test pod with docker.io/redis:3.0.5 image, default network and with valid VLAN
[It] Ping gateway of a network(invalid vlan) from a pod
  /gocode/main/test/e2e/tests/network-pod.go:399
    DEBUG: 2019/11/13 09:05:36 Creating test network : networkname1 with invalid vlan
    DEBUG: 2019/11/13 09:05:36 Gateway IP of networkname1 network is 56.12.100.1
    DEBUG: 2019/11/13 09:05:36 Trying to ping the gateway of network with invalid VLAN from pod e2etest-pod
    DEBUG: 2019/11/13 09:05:42 Output : PING 56.12.100.1 (56.12.100.1): 48 data bytes
....--- 56.12.100.1 ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, Error : failed to run commmand 'kubectl exec e2etest-pod -- ping -f -c 5 -W 5 56.12.100.1', output:PING 56.12.100.1 (56.12.100.1): 48 data bytes
....--- 56.12.100.1 ping statistics ---
5 packets transmitted, 0 packets received, 100% packet loss
, error:command terminated with exit code 1

, command terminated with exit code 1

    DEBUG: 2019/11/13 09:05:42 Delete network networkname1
[AfterEach] Ping gateway of a network from a pod
  /gocode/main/test/e2e/tests/network-pod.go:427
    DEBUG: 2019/11/13 09:05:42 Deleting the pod
    DEBUG: 2019/11/13 09:05:55 END_TEST Network.GatewayPing Time-taken : 43.539717006000004
    DEBUG: 2019/11/13 09:05:55 Checking stale storage resources
    DEBUG: 2019/11/13 09:05:55 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:05:55 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:05:55 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:43.653 seconds][0m
Network.GatewayPing Daily N_Basic-1.15 N_Basic-1.16 Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:366[0m
  Ping gateway of a network from a pod
  [90m/gocode/main/test/e2e/tests/network-pod.go:367[0m
    Ping gateway of a network(invalid vlan) from a pod
    [90m/gocode/main/test/e2e/tests/network-pod.go:399[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4[0m [90mPing pod's IP from outside world[0m 
  [1mPing pod's IP from outside world using network having invalid VLAN[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:750[0m
[BeforeEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:700
    DEBUG: 2019/11/13 09:05:55 START_TEST Network.PingPodFromOutside
    DEBUG: 2019/11/13 09:05:55 Login to cluster
    DEBUG: 2019/11/13 09:05:56 Checking basic Vnic usage
    DEBUG: 2019/11/13 09:05:56 Updating inventory struct
    DEBUG: 2019/11/13 09:05:56 Checking stale storage resources
    DEBUG: 2019/11/13 09:05:57 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:05:57 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:05:57 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:06:04 Creating storage classes
[It] Ping pod's IP from outside world using network having invalid VLAN
  /gocode/main/test/e2e/tests/network-pod.go:750
    DEBUG: 2019/11/13 09:06:14 Creating test network : testnetwork with invalid vlan
    DEBUG: 2019/11/13 09:06:14 Creating 1 pods of docker.io/redis:3.0.5 image with network : testnetwork and qos : high
    DEBUG: 2019/11/13 09:06:17 IP address ( 56.12.100.2 ) of e2etest-pod-1 is between 56.12.100.2 and 56.12.100.254

    DEBUG: 2019/11/13 09:06:17 Trying to ping the e2etest-pod-1
    DEBUG: 2019/11/13 09:06:17 Executing ping command: ping  -c 5 -W 5 56.12.100.2
    DEBUG: 2019/11/13 09:06:26 Deleting pods : 
    DEBUG: 2019/11/13 09:06:35 Deleting test network : testnetwork
[AfterEach] Ping pod's IP from outside world
  /gocode/main/test/e2e/tests/network-pod.go:709
    DEBUG: 2019/11/13 09:06:35 END_TEST Network.PingPodFromOutside Time-taken : 40.195428876
    DEBUG: 2019/11/13 09:06:35 Checking stale storage resources
    DEBUG: 2019/11/13 09:06:35 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:06:35 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:06:35 Checking storage stale resources on the node: appserv54

[32mâ€¢ [SLOW TEST:40.302 seconds][0m
Network.PingPodFromOutside Daily N_Basic-1.0 N_Basic-1.1 N_Basic-1.2 N_Basic-1.3 N_Basic-1.4
[90m/gocode/main/test/e2e/tests/network-pod.go:694[0m
  Ping pod's IP from outside world
  [90m/gocode/main/test/e2e/tests/network-pod.go:695[0m
    Ping pod's IP from outside world using network having invalid VLAN
    [90m/gocode/main/test/e2e/tests/network-pod.go:750[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mCluster.Basic Management Sanity Daily M_Cluster-1.0 M_Cluster-1.7 M_Cluster-1.11[0m [90mwhen cluster is created with all nodes[0m 
  [1mshould be created and destroyed[0m
  [37m/gocode/main/test/e2e/tests/cluster.go:61[0m
[BeforeEach] when cluster is created with all nodes
  /gocode/main/test/e2e/tests/cluster.go:48
    DEBUG: 2019/11/13 09:06:35 START_TEST Cluster.Basic
[It] should be created and destroyed
  /gocode/main/test/e2e/tests/cluster.go:61
    DEBUG: 2019/11/13 09:06:35 Login to cluster
    DEBUG: 2019/11/13 09:06:36 Checking basic Vnic usage
    DEBUG: 2019/11/13 09:06:36 Updating inventory struct
    DEBUG: 2019/11/13 09:06:37 Checking stale storage resources
    DEBUG: 2019/11/13 09:06:37 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:06:37 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:06:37 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:06:45 Creating storage classes
    DEBUG: 2019/11/13 09:06:55 Login to cluster
    DEBUG: 2019/11/13 09:06:56 Destroying the cluster: 709335e9-0623-11ea-a994-a4bf01194d67, Master node is appserv53
    DEBUG: 2019/11/13 09:06:56 Checking in a loop for cluster status
    DEBUG: 2019/11/13 09:07:35 Doing sync all nodes.
    DEBUG: 2019/11/13 09:07:35 Doing sync all nodes.
    DEBUG: 2019/11/13 09:07:36 Doing sync all nodes.
    DEBUG: 2019/11/13 09:07:38 Checking for cluster-info.json on node :172.16.6.153
    DEBUG: 2019/11/13 09:07:39 Checking for cluster-info.json on node :172.16.6.154
    DEBUG: 2019/11/13 09:07:39 Checking for cluster-info.json on node :172.16.6.155
    DEBUG: 2019/11/13 09:07:40 Rebooting all nodes.
    DEBUG: 2019/11/13 09:07:40 Doing sync on 172.16.6.153
    DEBUG: 2019/11/13 09:07:41 Doing sync on 172.16.6.154
    DEBUG: 2019/11/13 09:07:42 Doing sync on 172.16.6.155
    DEBUG: 2019/11/13 09:07:43 Waiting for nodes to come up, will wait upto 800 seconds
...............    DEBUG: 2019/11/13 09:10:24 Nodes are up, waiting for armada to start
.....[AfterEach] when cluster is created with all nodes
  /gocode/main/test/e2e/tests/cluster.go:56

    DEBUG: 2019/11/13 09:11:14 END_TEST Cluster.Basic Time-taken : 278.99699525

[32mâ€¢ [SLOW TEST:278.997 seconds][0m
Cluster.Basic Management Sanity Daily M_Cluster-1.0 M_Cluster-1.7 M_Cluster-1.11
[90m/gocode/main/test/e2e/tests/cluster.go:40[0m
  when cluster is created with all nodes
  [90m/gocode/main/test/e2e/tests/cluster.go:43[0m
    should be created and destroyed
    [90m/gocode/main/test/e2e/tests/cluster.go:61[0m
[90m------------------------------[0m
[36mS[0m
[90m------------------------------[0m
[0mPerfTier.Basic Management Sanity QOS_Cli-1.0 QOS_Cli-1.7 QOS_Cli-1.8 QOS_Cli-1.9 QOS_Cli-2.0 QOS_Cli-2.1 QOS_Cli-2.2 QOS_Cli-4.0 QOS_Cli-4.1 QOS_Cli-4.2 Multizone[0m [90mBasic Perf-tier testcases[0m 
  [1mCreate the perf-tier with same name again[0m
  [37m/gocode/main/test/e2e/tests/perf-tier.go:57[0m
[BeforeEach] Basic Perf-tier testcases
  /gocode/main/test/e2e/tests/perf-tier.go:36
    DEBUG: 2019/11/13 09:11:15 Cluster Spec Node list is [appserv53 appserv54 appserv55]
    DEBUG: 2019/11/13 09:11:15 Getting dns domain name
    DEBUG: 2019/11/13 09:11:15 FQDN : autotb7.eng.diamanti.com
    DEBUG: 2019/11/13 09:11:15 Generating certificates for the cluster: (Name: autotb7, VIP: 172.16.19.55, FQDN: autotb7.eng.diamanti.com)
    DEBUG: 2019/11/13 09:11:15 Clean up existing certs if any:
    DEBUG: 2019/11/13 09:11:15 Generate unique CA name with current date
    DEBUG: 2019/11/13 09:11:15 Integrate CA name in file
    DEBUG: 2019/11/13 09:11:15 Generate CA certs
    DEBUG: 2019/11/13 09:11:15 Create a CSR to generate a certificate using FQDN, VIP, Cluster Name for a server certs
    DEBUG: 2019/11/13 09:11:15 Generate server certificate:
    DEBUG: 2019/11/13 09:11:15 Getting CertificateAuthority from /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/ca.pem file
    DEBUG: 2019/11/13 09:11:15 Getting ServerCertificate from /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/server.pem file
    DEBUG: 2019/11/13 09:11:15 Getting ServerPrivateKey from /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/server-key.pem file
    DEBUG: 2019/11/13 09:11:15 Creating the cluster
    DEBUG: 2019/11/13 09:11:31 Please import "/var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/server_certs/ca.pem" certificate to your client machine
    DEBUG: 2019/11/13 09:11:31 Sleeping for 60 sec
    DEBUG: 2019/11/13 09:12:31 Save cluster configuration: 
    DEBUG: 2019/11/13 09:12:32 Login to cluster
    DEBUG: 2019/11/13 09:12:32 Polling for cluster login for 300 seconds.
    DEBUG: 2019/11/13 09:12:32 Checking in a loop for cluster status
    DEBUG: 2019/11/13 09:12:32 Found '3' nodes
    DEBUG: 2019/11/13 09:12:32 Waitting for appserv54 to into "Ready" state.
    DEBUG: 2019/11/13 09:12:32 Waitting for appserv55 to into "Ready" state.
    DEBUG: 2019/11/13 09:12:32 Waitting for appserv53 to into "Ready" state.
    DEBUG: 2019/11/13 09:12:32 Creating network default
    DEBUG: 2019/11/13 09:12:33 Creating network blue
    DEBUG: 2019/11/13 09:12:33 Add default tag to default network
    DEBUG: 2019/11/13 09:12:43 Labeled all nodes with node=node$

    DEBUG: 2019/11/13 09:12:43 Getting cluster ID
    DEBUG: 2019/11/13 09:12:43 Created test cluster: 9984c5cb-0638-11ea-ab81-a4bf01194d67
    DEBUG: 2019/11/13 09:12:43 Deleting all LCVs, volumes, snapshots from previous cluster if any.
    DEBUG: 2019/11/13 09:12:44 Recording timestamp of all services on all nodes
    DEBUG: 2019/11/13 09:12:51 Overwritting e2e parameter : ExpectedBasicVnicUsageCount
    DEBUG: 2019/11/13 09:12:51 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:12:52 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:12:53 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:12:53 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:12:53 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:12:54 Updating inventory struct
    DEBUG: 2019/11/13 09:12:55 Creating storage classes
    DEBUG: 2019/11/13 09:13:05 START_TEST PerfTier.Basic
[It] Create the perf-tier with same name again
  /gocode/main/test/e2e/tests/perf-tier.go:57
    DEBUG: 2019/11/13 09:13:05 Create perf-tier.
    DEBUG: 2019/11/13 09:13:05 Try to create perf-tier with the same name again.
    ERROR: 2019/11/13 09:13:05  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create template4 -b 2G -i 50k', status:&{{Status } {  0} Failure PerformanceTier "template4" already exists AlreadyExists 0xc00016a690 409}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "PerformanceTier \"template4\" already exists",
 "reason": "AlreadyExists",
 "details": {
  "name": "template4",
  "kind": "PerformanceTier"
 },
 "code": 409
}



    DEBUG: 2019/11/13 09:13:05 Delete the perf-tier.
[AfterEach] Basic Perf-tier testcases
  /gocode/main/test/e2e/tests/perf-tier.go:42
    DEBUG: 2019/11/13 09:13:05 END_TEST PerfTier.Basic Time-taken : 0.342414071
    DEBUG: 2019/11/13 09:13:05 Checking stale storage resources
    DEBUG: 2019/11/13 09:13:05 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:13:05 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:13:05 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:110.919 seconds][0m
PerfTier.Basic Management Sanity QOS_Cli-1.0 QOS_Cli-1.7 QOS_Cli-1.8 QOS_Cli-1.9 QOS_Cli-2.0 QOS_Cli-2.1 QOS_Cli-2.2 QOS_Cli-4.0 QOS_Cli-4.1 QOS_Cli-4.2 Multizone
[90m/gocode/main/test/e2e/tests/perf-tier.go:26[0m
  Basic Perf-tier testcases
  [90m/gocode/main/test/e2e/tests/perf-tier.go:27[0m
    Create the perf-tier with same name again
    [90m/gocode/main/test/e2e/tests/perf-tier.go:57[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mNetwork.VFsSchedulingWithCustomQos Daily AT_Scheduling-3.1 Qos Multizone[0m [90mPod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID[0m 
  [1mCreate pods with custom, high qos and check scheduling on nicID(s)[0m
  [37m/gocode/main/test/e2e/tests/network-pod.go:1612[0m
[BeforeEach] Pod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID
  /gocode/main/test/e2e/tests/network-pod.go:1599
    DEBUG: 2019/11/13 09:13:05 START_TEST Network.VFsSchedulingWithCustomQos
    DEBUG: 2019/11/13 09:13:05 Login to cluster
    DEBUG: 2019/11/13 09:13:06 Checking basic Vnic usage
    DEBUG: 2019/11/13 09:13:06 Updating inventory struct
    DEBUG: 2019/11/13 09:13:07 Checking stale storage resources
    DEBUG: 2019/11/13 09:13:07 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:13:07 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:13:07 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:13:15 Creating storage classes
[It] Create pods with custom, high qos and check scheduling on nicID(s)
  /gocode/main/test/e2e/tests/network-pod.go:1612
    DEBUG: 2019/11/13 09:13:25 Create the perf-tier custom with 5G network bandwidth.
    DEBUG: 2019/11/13 09:13:30 Creating 1 pod with custom qos: 
    DEBUG: 2019/11/13 09:13:30 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/13 09:13:30 Getting node label of appserv53: 
    DEBUG: 2019/11/13 09:13:30 Creating iperf server pod: iperf-custom-1
    DEBUG: 2019/11/13 09:13:30 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:13:33 Creating 10 pods with high qos: 
    DEBUG: 2019/11/13 09:13:33 Pick up appserv53 node for scheduling
    DEBUG: 2019/11/13 09:13:33 Getting node label of appserv53: 
    DEBUG: 2019/11/13 09:13:33 Creating iperf server pod: iperf-high-1
    DEBUG: 2019/11/13 09:13:33 Creating iperf server pod: iperf-high-2
    DEBUG: 2019/11/13 09:13:33 Creating iperf server pod: iperf-high-3
    DEBUG: 2019/11/13 09:13:34 Creating iperf server pod: iperf-high-4
    DEBUG: 2019/11/13 09:13:34 Creating iperf server pod: iperf-high-5
    DEBUG: 2019/11/13 09:13:35 Creating iperf server pod: iperf-high-6
    DEBUG: 2019/11/13 09:13:35 Creating iperf server pod: iperf-high-7
    DEBUG: 2019/11/13 09:13:35 Creating iperf server pod: iperf-high-8
    DEBUG: 2019/11/13 09:13:36 Creating iperf server pod: iperf-high-9
    DEBUG: 2019/11/13 09:13:36 Creating iperf server pod: iperf-high-10
    DEBUG: 2019/11/13 09:13:36 Checking if given pods are in Running state
    DEBUG: 2019/11/13 09:13:40 Pod scheduled as expected
    DEBUG: 2019/11/13 09:13:40 Deleting all the pods: 
    DEBUG: 2019/11/13 09:15:21 Delete the perf-tier custom
[AfterEach] Pod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID
  /gocode/main/test/e2e/tests/network-pod.go:1607
    DEBUG: 2019/11/13 09:15:21 END_TEST Network.VFsSchedulingWithCustomQos Time-taken : 136.058317281
    DEBUG: 2019/11/13 09:15:21 Checking stale storage resources
    DEBUG: 2019/11/13 09:15:22 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:15:22 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:15:22 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:136.169 seconds][0m
Network.VFsSchedulingWithCustomQos Daily AT_Scheduling-3.1 Qos Multizone
[90m/gocode/main/test/e2e/tests/network-pod.go:1593[0m
  Pod with custom qos(5G) should schedule on one nicID and pods with high qos should schedule on other nicID
  [90m/gocode/main/test/e2e/tests/network-pod.go:1594[0m
    Create pods with custom, high qos and check scheduling on nicID(s)
    [90m/gocode/main/test/e2e/tests/network-pod.go:1612[0m
[90m------------------------------[0m
[0mRbac.EditView Daily Rbac_Local_Basic-2.0[0m [90mUser can edit/view in it's namespace[0m 
  [1mUser can edit/view perf-tier(s)[0m
  [37m/gocode/main/test/e2e/tests/rbac.go:659[0m
[BeforeEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:528
    DEBUG: 2019/11/13 09:15:22 START_TEST Rbac.EditView
    DEBUG: 2019/11/13 09:15:22 Login to cluster
    DEBUG: 2019/11/13 09:15:22 Checking basic Vnic usage
    DEBUG: 2019/11/13 09:15:22 Updating inventory struct
    DEBUG: 2019/11/13 09:15:23 Checking stale storage resources
    DEBUG: 2019/11/13 09:15:23 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:15:23 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:15:23 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:15:31 Creating storage classes
    DEBUG: 2019/11/13 09:15:43 User Logout
[It] User can edit/view perf-tier(s)
  /gocode/main/test/e2e/tests/rbac.go:659
    DEBUG: 2019/11/13 09:15:44 Creating group, user with role(s)
    DEBUG: 2019/11/13 09:15:45 Creating group jacksgroup with perftier-edit role(s)
    DEBUG: 2019/11/13 09:15:45 Creating jack user in jacksgroup group
    DEBUG: 2019/11/13 09:15:45 Login as jack user
    DEBUG: 2019/11/13 09:15:46 Try to create perf-tier test-perftier with perftier-edit role
    DEBUG: 2019/11/13 09:15:46 Try to list perf-tier.
    DEBUG: 2019/11/13 09:15:46 Editing group role(s)
    DEBUG: 2019/11/13 09:15:46 Editing jacksgroup group with perftier-view role(s)
    DEBUG: 2019/11/13 09:15:46 Login as jack user
    DEBUG: 2019/11/13 09:15:47 Try to list perf-tier.
    DEBUG: 2019/11/13 09:15:47 Try to create perf-tier with perftier-view role.
    ERROR: 2019/11/13 09:15:47  perf-tier.go:59: Perf-tier create command failed: failed to run commmand 'dctl  -o json  perf-tier create test-perftier-1 -b 1G -i 50k', status:&{{Status } {  0} Failure POST on perftiers for "jack" is forbidden: User jack cannot perform POST on perftiers Forbidden 0xc0006085a0 403}, error:{
 "kind": "Status",
 "metadata": {},
 "status": "Failure",
 "message": "POST on perftiers for \"jack\" is forbidden: User jack cannot perform POST on perftiers",
 "reason": "Forbidden",
 "details": {
  "name": "jack",
  "kind": "perftiers"
 },
 "code": 403
}



    DEBUG: 2019/11/13 09:15:47 Editing group role(s)
    DEBUG: 2019/11/13 09:15:47 Editing jacksgroup group with perftier-edit role(s)
    DEBUG: 2019/11/13 09:15:48 Login as jack user
    DEBUG: 2019/11/13 09:15:48 Try to delete perf-tier test-perftier
[AfterEach] User can edit/view in it's namespace
  /gocode/main/test/e2e/tests/rbac.go:539
    DEBUG: 2019/11/13 09:15:48 User Logout
    DEBUG: 2019/11/13 09:15:49 END_TEST Rbac.EditView Time-taken : 27.72322121
    DEBUG: 2019/11/13 09:15:49 Checking stale storage resources
    DEBUG: 2019/11/13 09:15:49 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:15:49 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:15:49 Checking storage stale resources on the node: appserv55

[32mâ€¢ [SLOW TEST:27.837 seconds][0m
Rbac.EditView Daily Rbac_Local_Basic-2.0
[90m/gocode/main/test/e2e/tests/rbac.go:518[0m
  User can edit/view in it's namespace
  [90m/gocode/main/test/e2e/tests/rbac.go:520[0m
    User can edit/view perf-tier(s)
    [90m/gocode/main/test/e2e/tests/rbac.go:659[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0mBenchmarking.NetworkUniDirectionalTwoPorts Daily AT_Benchmark-1.1 Qos Multizone[0m [90mNetwork uni-directional benchmarking with two network ports[0m 
  [1mNetwork uni-directional benchmarking with two network ports, should get 18G bandwidth[0m
  [37m/gocode/main/test/e2e/tests/benchmarking.go:93[0m
[BeforeEach] Network uni-directional benchmarking with two network ports
  /gocode/main/test/e2e/tests/benchmarking.go:79
    DEBUG: 2019/11/13 09:15:49 START_TEST Benchmarking.NetworkUniDirectionalTwoPorts
    DEBUG: 2019/11/13 09:15:49 Login to cluster
    DEBUG: 2019/11/13 09:15:50 Checking basic Vnic usage
    DEBUG: 2019/11/13 09:15:50 Updating inventory struct
    DEBUG: 2019/11/13 09:15:51 Checking stale storage resources
    DEBUG: 2019/11/13 09:15:51 Checking storage stale resources on the node: appserv53
    DEBUG: 2019/11/13 09:15:51 Checking storage stale resources on the node: appserv54
    DEBUG: 2019/11/13 09:15:51 Checking storage stale resources on the node: appserv55
    DEBUG: 2019/11/13 09:15:58 Creating storage classes
[It] Network uni-directional benchmarking with two network ports, should get 18G bandwidth
  /gocode/main/test/e2e/tests/benchmarking.go:93
    DEBUG: 2019/11/13 09:16:08 Creating four pairs of iperf client-server pods
    DEBUG: 2019/11/13 09:16:08 Creating iperf server pod: iperf-server-high1
    DEBUG: 2019/11/13 09:16:11 Creating service with name: iperf-server-high1
    DEBUG: 2019/11/13 09:16:11 Creating iperf server pod: iperf-server-high2
    DEBUG: 2019/11/13 09:16:14 Creating service with name: iperf-server-high2
    DEBUG: 2019/11/13 09:16:14 Creating iperf server pod: iperf-server-high3
    DEBUG: 2019/11/13 09:16:16 Creating service with name: iperf-server-high3
    DEBUG: 2019/11/13 09:16:17 Creating iperf server pod: iperf-server-high4
    DEBUG: 2019/11/13 09:16:19 Creating service with name: iperf-server-high4
    DEBUG: 2019/11/13 09:16:49 Creating iperf Client pod: iperf-client-high1
    DEBUG: 2019/11/13 09:16:52 Creating iperf Client pod: iperf-client-high2
    DEBUG: 2019/11/13 09:16:56 Creating iperf Client pod: iperf-client-high3
    DEBUG: 2019/11/13 09:17:00 Creating iperf Client pod: iperf-client-high4
    DEBUG: 2019/11/13 09:17:09 Sleeping for 180 seconds, so that prometheus database will have some stats.
    DEBUG: 2019/11/13 09:20:09 Validating if bandwidth is honored or not for server pods:
    DEBUG: 2019/11/13 09:20:09 QoS honored for pod: iperf-server-high1
    DEBUG: 2019/11/13 09:20:10 QoS honored for pod: iperf-server-high2
    DEBUG: 2019/11/13 09:20:10 QoS honored for pod: iperf-server-high3
    DEBUG: 2019/11/13 09:20:11 QoS honored for pod: iperf-server-high4
    DEBUG: 2019/11/13 09:20:11 Validating if bandwidth is honored or not for client pods:
    DEBUG: 2019/11/13 09:20:11 QoS honored for pod: iperf-client-high1
    DEBUG: 2019/11/13 09:20:12 QoS honored for pod: iperf-client-high2
    DEBUG: 2019/11/13 09:20:12 QoS honored for pod: iperf-client-high3
    DEBUG: 2019/11/13 09:20:12 QoS honored for pod: iperf-client-high4
    DEBUG: 2019/11/13 09:20:12 Measuring throughput. num of links used: 2
    DEBUG: 2019/11/13 09:20:13 Node: appserv53. Expected Throughput: 16200000000. RX Throughput: 17594673956. TX Throughput: 0
    DEBUG: 2019/11/13 09:20:13 Node: appserv54. Expected Throughput: 16200000000. RX Throughput: 0. TX Throughput: 17640074404
    DEBUG: 2019/11/13 09:20:13 Deleting pods:
    DEBUG: 2019/11/13 09:20:13 Deleting pods : 
    DEBUG: 2019/11/13 09:20:34 Delete services: 
    DEBUG: 2019/11/13 09:20:34 Deleting service(s)
[AfterEach] Network uni-directional benchmarking with two network ports
  /gocode/main/test/e2e/tests/benchmarking.go:88
    DEBUG: 2019/11/13 09:20:34 END_TEST Benchmarking.NetworkUniDirectionalTwoPorts Time-taken : 284.658228626
    DEBUG: 2019/11/13 09:20:34 Checking stale storage resources
    DEBUG: 2019/11/13 09:20:34 Checking storage stale resources on the node: appserv53
    ERROR: 2019/11/13 09:20:34  dwtest_lib.go:451: ResourceNetworkBandwidth is not zero, it is : 500000000
    DEBUG: 2019/11/13 09:20:34 Checking storage stale resources on the node: appserv54
    ERROR: 2019/11/13 09:20:34  dwtest_lib.go:451: ResourceNetworkBandwidth is not zero, it is : 500000000
    DEBUG: 2019/11/13 09:20:34 Checking storage stale resources on the node: appserv55
    ERROR: 2019/11/13 09:20:34  util.go:307: TestFailed Some cluster node(s) may have stale storage resources
[91mCluster has stale storage resources: : Some cluster node(s) may have stale storage resources
Expected error:
    <*errors.errorString | 0xc0003bbf20>: {
        s: "Some cluster node(s) may have stale storage resources",
    }
    Some cluster node(s) may have stale storage resources
not to have occurred
    DEBUG: 2019/11/13 09:20:34 Collecting techsupport log from all test nodes

    DEBUG: 2019/11/13 09:20:34 
Getting techsupport logs for all nodes
    DEBUG: 2019/11/13 09:20:34 Create techsupport log directory : 2019-11-13T09-20-34
    DEBUG: 2019/11/13 09:22:50 Getting tech support for 172.16.6.155 node, to directory 2019-11-13T09-20-34

    DEBUG: 2019/11/13 09:22:51 Getting tech support for 172.16.6.154 node, to directory 2019-11-13T09-20-34

    DEBUG: 2019/11/13 09:23:07 Getting tech support for 172.16.6.153 node, to directory 2019-11-13T09-20-34

    DEBUG: 2019/11/13 09:23:08 Creating pod log directory : pod_description_and_logs
    DEBUG: 2019/11/13 09:23:08 Collecting pod description and logs for all the pods ...
    DEBUG: 2019/11/13 09:23:17 Collected techsupport log location : /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/bin/2019-11-13T09-20-34
    DEBUG: 2019/11/13 09:23:17 Copying e2e log file to directory : /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/bin/2019-11-13T09-20-34
    DEBUG: 2019/11/13 09:23:17 Changing permissions of directory, so that all users can write to : 2019-11-13T09-20-34
    DEBUG: 2019/11/13 09:23:17 [0mExiting as FailFast is set..
+ val=0
+ '[' 0 == 0 ']'
++ grep 'SUCCESS\!' console_ouput.txt
++ awk '{print $6}'
+ pass=
+ '[' ']'
+ val=1
++ grep 'FAIL\!' console_ouput.txt
++ awk '{print $6}'
+ failcnt=
+ '[' ']'
+ echo 'Collecting techsupport from all nodes.'
Collecting techsupport from all nodes.
+ ../scripts/utils/collect_techsupport.sh -n appserv53 appserv54 appserv55


Collecting techsupport from all the nodes in parallel. Please wait ...



Copying tech-support dump from appserv55

+ PROG=/usr/local/bin/dw-techsupport
+ TS_FILE_PREFIX=appserv55-dw-tech-support-
+ TS_FILE_SUFFIX=.tar
+ TS_OUTPUT_FILE=
+ TS_OUTPUT_DIR=/data/techsupport
+ TS_FILE_XZ=.xz
+ TS_PATH_XZ_FILE='/data/techsupport/*.tar.xz'
+ TS_MAX_FILES=30
+ [[ 1 -gt 0 ]]
+ key=--best-effort
+ case $key in
+ BEST_EFFORT=true
+ shift
+ [[ 0 -gt 0 ]]
+ CONDUIT_EMBEDDED_IP=192.168.100.2
+ EMBEDDED_LOG_FILES_DIR=/tmp
+ EMBEDDED_TS_FILE_BACKUP=/mnt/embedded-latest-techsupport.tar.gz
+ EMBEDDED_TS_BACKUP_DIR=/mnt/ts_backup
+ EMBEDDED_PERSISTENT_LOG_DIR=/mnt/logs
++ mktemp -d
+ TEMP_DIR=/tmp/tmp.MI3LKKx2vd
+ TS_DIR_PREFIX=/tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-
+ TS_DIR_HOST=host-files/
+ TS_DIR_EMBEDDED=embedded-files/
+ TS_DIR_DWS=dws/
+ HOST_SERVICES_SYSTEMD='armada bosun convoy apiserver scheduler kubelet 	controller-manager etcd docker docker-storage-setup        	dock postgres ntpd proxy kube-dnsmasq kubedns upgrade.service crio'
+ HOST_SERVICES_PROCESS='armada bosun convoy kube-apiserver kube-scheduler kubelet 	kube-controller etcd dockerd-current container-storage-setup        	dock postgres ntpd kube-proxy dnsmasq-nanny kube-dns upgrade crio'
+ DWS_OBJECTS='node network volume snapshot perf-tier event user endpoint drive feature'
+ K8S_OBJECTS='pv pvc sc volumesnapshots volumesnapshotdata services endpoints'
+ DWS_USER_OBJECTS='group role auth-server'
+ HOST_DATA_DIR=/var/lib/diamanti
+ HOST_CONFIG_DIR=/etc/diamanti
+ HOST_PCYCLE_HISTORY='/usr/lib/firmware/diamanti/.dws*'
+ SSHOPTIONS='-o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no 	-o LogLevel=ERROR -o ConnectTimeout=15'
+ HOST_TIMEOUT=15
+ HOST_TIMEOUT_LONG=180
+ HOST_TIMEOUT_KILL=5
+ EMBEDDED_TIMEOUT=150
+ NS='diamanti-system kube-system'
+ collect_tech_support
+ '[' 0 -ne 0 ']'
++ dctl whoami
+ DUMMY='Name:         admin
Built-In:     true
Local-Auth:   true
Groups:       user-admin, cluster-admin
Roles:        perftier-edit, volume-edit, user-edit, allcontainer-edit, container-edit/default, volumeclaim-edit/default, network-edit, required, node-edit
Namespace:    default'
+ '[' 0 -eq 1 ']'
+ LOGGED_IN=true
+ set_tech_support_timestamp
++ date +%Y.%m.%d-%H.%M.%S
+ ts_timestamp=2019.11.13-09.23.19
+ '[' '' = '' ']'
+ TS_OUTPUT_FILE=appserv55-dw-tech-support-2019.11.13-09.23.19
+ TECH_SUPPORT_FILE=appserv55-dw-tech-support-2019.11.13-09.23.19.tar
+ mkdir -p /data/techsupport
+ '[' 0 -ne 0 ']'
+ '[' '!' -z appserv55-dw-tech-support-2019.11.13-09.23.19 -a appserv55-dw-tech-support-2019.11.13-09.23.19 '!=' ' ' ']'
+ touch /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.in_progress
+ '[' 0 -ne 0 ']'
+ logger Collecting dw-techsupport
+ create_tech_support_dirs
+ ts_dir=/tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/
+ mkdir /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/
+ ts_dir_dws=/tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/dws/
+ mkdir /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/dws/
+ ts_dir_host=/tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ mkdir /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ ts_dir_embedded=/tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ mkdir /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ echo -n 'Collecting firmware tech support information '
+ pb_pid=41251
+ collect_embedded_tech_support
+ embedded_tar_file=embedded-2019.11.13-09.23.19.tar.gz
+ ping -c 1 192.168.100.2
+ progress_bar
+ true
+ echo -n .
+ sleep 5
+ '[' 0 -ne 0 ']'
+ run_embedded_cmd '/dwsscripts/embedded_techsupport.sh embedded-2019.11.13-09.23.19.tar.gz' 'Failed to collect embedded techsupport'
+ timeout 150 ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 root@192.168.100.2 /dwsscripts/embedded_techsupport.sh embedded-2019.11.13-09.23.19.tar.gz
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ '[' 0 -ne 0 ']'
+ get_embedded_file embedded-2019.11.13-09.23.19.tar.gz /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2:/embedded-2019.11.13-09.23.19.tar.gz /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ '[' 0 -eq 0 ']'
+ pushd /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ tar -xvzf embedded-2019.11.13-09.23.19.tar.gz
+ rm -rf embedded-2019.11.13-09.23.19.tar.gz
+ popd
+ run_embedded_cmd 'rm embedded-2019.11.13-09.23.19.tar.gz' 'Failed to remove the tar of embedded logs embedded-2019.11.13-09.23.19.tar.gz'
+ timeout 150 ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 root@192.168.100.2 rm embedded-2019.11.13-09.23.19.tar.gz
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ get_embedded_file /mnt/ts_backup /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2://mnt/ts_backup /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ mkdir /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files//persistent
+ get_embedded_file /mnt/logs /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files//persistent
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2://mnt/logs /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/embedded-files//persistent
+ true
+ echo -n .
+ sleep 5
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ kill -13 41251
+ echo ''
+ echo -n 'Collecting host tech support information '
+ pb_pid=41475
+ collect_host_tech_support
+ collect_logs
+ progress_bar
+ mkdir -p /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//logs
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 cp -rf /var/log/diamanti/core /var/log/diamanti/embedded /var/log/diamanti/kubernetes /var/log/diamanti/plugin /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//logs/
+ collect_interrupts
++ seq 0 4
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ true
+ echo -n .
+ sleep 5
+ collect_system_data
+ timeout -k 5 15 cp /var/log/messages /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /var/log/mcelog /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /var/log/secure /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 dmesg -T
+ timeout -k 5 15 df -h
+ timeout -k 5 15 date
+ timeout -k 5 15 uptime
+ timeout -k 5 15 ls -lrRth /var/log/
+ timeout -k 5 15 ls -lrRth /var/log/diamanti/
+ timeout -k 5 15 ls -lrt /var/lib/cni/networks/mgmt
+ timeout -k 5 15 top -b -n 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 systemd-cgtop -b -n 5
+ timeout -k 5 15 systemd-cgls
+ timeout -k 5 15 systemctl status
+ timeout -k 5 15 systemctl list-unit-files
+ timeout -k 5 15 systemctl list-units
+ timeout -k 5 15 systemctl list-timers --all
+ timeout -k 5 15 ps auxf
+ timeout -k 5 15 cp /proc/cpuinfo /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /proc/meminfo /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /proc/devices /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /proc/mounts /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 lspci -vv
+ timeout -k 5 15 cp /usr/local/.patches/.installed /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//installed-patches.txt
+ timeout -k 5 15 cp /usr/local/.updates/.installed /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//installed-updates.txt
+ timeout -k 5 15 cp /usr/share/diamanti/git-manifest.txt /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//git-manifest.txt
+ timeout -k 5 15 cp /etc/diamanti-release /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//diamanti-release
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.desd /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//desd.txt
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.desl /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//desl.txt
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.deso /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//deso.txt
+ timeout -k 5 15 cp '/usr/lib/firmware/diamanti/.dws*' /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 ipmitool sel elist
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ipmitool sdr
+ timeout -k 5 15 ipmitool fru
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ditool
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ntpq -p
+ timeout -k 5 15 ntpstat
+ timeout -k 5 15 netstat -ontap
+ timeout -k 5 15 netstat -s
+ timeout -k 5 15 rpm -qi diamanti-cx
+ timeout -k 5 15 cat /proc/mounts
+ timeout -k 5 15 ls -l /dev/nvme0
+ timeout -k 5 15 echo -e '\ncommand : dstool -s'
+ timeout -k 5 15 dstool -s
+ timeout -k 5 15 echo -e '\ncommand : dstool -c "mputil -w rs_btp"'
+ timeout -k 5 15 dstool -c 'mputil -w rs_btp'
+ timeout -k 5 15 echo -e '\ncommand : dstool -c "dmesg" | grep "Link is up"'
+ timeout -k 5 15 dstool -c dmesg
+ grep 'Link is up'
+ timeout -k 5 15 cat /etc/fstab
+ timeout -k 5 15 cat /etc/resolv.conf
+ timeout -k 5 15 sysctl -a
+ collect_crash_data
+ mkdir -p /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//crash
+ for dir in '/var/crash/*'
++ basename '/var/crash/*'
+ name='*'
+ timeout -k 5 15 cp '/var/crash/*/vmcore-dmesg.txt' '/tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//crash/*-vmcore-dmesg.txt'
+ collect_config_data
+ timeout -k 5 15 cp /etc/systemd/journald.conf /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /etc/etcd/etcd.conf /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 tar -cvzf /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//data.tar.gz /var/lib/diamanti
+ timeout -k 5 15 tar -cvzf /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//config.tar.gz /etc/diamanti
+ timeout -k 5 15 find /etc/diamanti/ -name '*.conf' -exec cp '{}' /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/ ';'
+ collect_network_data
+ timeout -k 5 15 ifconfig -a
+ timeout -k 5 15 ip route list
+ timeout -k 5 15 arp -an
+ printf '\n\n netstat -anevop -t\n'
+ timeout -k 5 15 netstat -anevop -t
+ printf '\n\n  netstat -ian -t\n'
+ timeout -k 5 15 netstat -ian -t
+ printf '\n\n netstat -s -t\n'
+ timeout -k 5 15 netstat -s -t
+ printf '\n\n iptables -L\n'
+ timeout -k 5 15 iptables -L
+ printf '\n\n iptables -vL -t filter\n'
+ timeout -k 5 15 iptables -vL -t filter
+ printf '\n\n iptables -vL -t nat\n'
+ timeout -k 5 15 iptables -vL -t nat
+ printf '\n\n iptables -vL -t mangle\n'
+ timeout -k 5 15 iptables -vL -t mangle
+ printf '\n\n iptables -vL -t raw\n'
+ timeout -k 5 15 iptables -vL -t raw
+ printf '\n\n iptables -vL -t security\n'
+ timeout -k 5 15 iptables -vL -t security
+ printf '\n\n iptables -S -t nat\n'
+ timeout -k 5 15 iptables -S -t nat
+ true
+ echo -n .
+ sleep 5
+ collect_socket_and_fd_data
+ mkdir -p /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//fds
+ mkdir -p /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//tcp_sockets
+ timeout -k 5 15 lsof
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o armada
+ SVC_PID=22780
+ '[' 22780 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/22780/fd
+ timeout -k 5 15 cat /proc/22780/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o bosun
+ SVC_PID=23762
+ '[' 23762 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23762/fd
+ timeout -k 5 15 cat /proc/23762/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o convoy
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-apiserver
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-scheduler
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kubelet
+ SVC_PID=23734
+ '[' 23734 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23734/fd
+ timeout -k 5 15 cat /proc/23734/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-controller
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o etcd
+ SVC_PID=23453
+ '[' 23453 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23453/fd
+ timeout -k 5 15 cat /proc/23453/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dockerd-current
+ SVC_PID=6975
+ '[' 6975 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/6975/fd
+ timeout -k 5 15 cat /proc/6975/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o container-storage-setup
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dock
+ SVC_PID=22782
+ '[' 22782 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/22782/fd
+ timeout -k 5 15 cat /proc/22782/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o postgres
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o ntpd
+ SVC_PID=10052
+ '[' 10052 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/10052/fd
+ timeout -k 5 15 cat /proc/10052/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-proxy
+ SVC_PID=23761
+ '[' 23761 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23761/fd
+ timeout -k 5 15 cat /proc/23761/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dnsmasq-nanny
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-dns
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o upgrade
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o crio
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ collect_nic_data
+ '[' -d /sys/bus/pci/drivers/cxgb4 ']'
+ timeout -k 5 15 cp /lib/firmware/cxgb4/t5-config.txt /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /lib/firmware/cxgb4/t6-config.txt /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files/
++ ls /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -i mgmtpf1,0
+ printf '\n\n ethtool -k %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -k mgmtpf1,0
+ printf '\n\n ethtool -S %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -S mgmtpf1,0
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -i mgmtpf1,1
+ printf '\n\n ethtool -k %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -k mgmtpf1,1
+ printf '\n\n ethtool -S %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -S mgmtpf1,1
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -i mgmtpf1,2
+ printf '\n\n ethtool -k %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -k mgmtpf1,2
+ printf '\n\n ethtool -S %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -S mgmtpf1,2
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -i mgmtpf1,3
+ printf '\n\n ethtool -k %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -k mgmtpf1,3
+ printf '\n\n ethtool -S %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -S mgmtpf1,3
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4
+ timeout -k 5 15 ethtool -i ens801f4
+ printf '\n\n ethtool -k %s\n' ens801f4
+ timeout -k 5 15 ethtool -k ens801f4
+ printf '\n\n ethtool -S %s\n' ens801f4
+ timeout -k 5 15 ethtool -S ens801f4
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -i ens801f4d1
+ printf '\n\n ethtool -k %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -k ens801f4d1
+ printf '\n\n ethtool -S %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -S ens801f4d1
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -i ens801f4d2
+ printf '\n\n ethtool -k %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -k ens801f4d2
+ printf '\n\n ethtool -S %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -S ens801f4d2
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -i ens801f4d3
+ printf '\n\n ethtool -k %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -k ens801f4d3
+ printf '\n\n ethtool -S %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -S ens801f4d3
+ printf '\n\n cat /sys/kernel/debug/cxgb4/*/tp_stats \n' ens801f4d3
+ timeout -k 5 15 cat /sys/kernel/debug/cxgb4/0000:81:00.4/tp_stats
+ printf '\n\n cat /sys/kernel/debug/cxgb4vf/*/sge_qstats \n' ens801f4d3
+ timeout -k 5 15 cat /sys/kernel/debug/cxgb4vf/0000:81:01.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.7/sge_qstats
+ collect_storage_data
+ printf '\n\n lsblk -l\n'
+ timeout -k 5 15 lsblk -l
+ printf '\n\n lsblk -t\n'
+ timeout -k 5 15 lsblk -t
+ timeout -k 5 15 iostat -x 1 20
+ grep nvme
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ collect_services_data
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status armada
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u armada -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status bosun
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u bosun -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status convoy
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u convoy -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status apiserver
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u apiserver -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status scheduler
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u scheduler -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kubelet
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kubelet -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status controller-manager
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u controller-manager -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status etcd
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u etcd -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status docker
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u docker -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status docker-storage-setup
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u docker-storage-setup -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status dock
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u dock -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status postgres
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u postgres -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status ntpd
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u ntpd -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status proxy
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u proxy -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kube-dnsmasq
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kube-dnsmasq -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kubedns
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kubedns -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status upgrade.service
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u upgrade.service -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status crio
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u crio -b -l
+ timeout -k 5 180 journalctl -b -l
+ collect_kvstore_data
+ timeout -k 5 15 curl --silent -X GET http://127.0.0.1:12345/api/v1/cluster
++ grep master /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/dws//cluster.log
++ awk ' { print $2 } '
++ sed 's/[", ]//g'
++ head -n 1
+ master=appserv53
+ '[' -z appserv53 ']'
++ timeout -k 5 15 curl -k --write-out '%{http_code}' --silent --output /dev/null http://appserv55:2379/v2/members
+ response_code=200
+ test 200 -eq 200
+ timeout -k 5 15 etcdctl -C http://appserv55:2379 cluster-health
+ ETCDCTL_API=3
+ timeout -k 5 15 etcdctl get --endpoints=appserv55:2379 --prefix / --sort-by=KEY
+ sed 's/-----BEGIN RSA PRIVATE KEY-----.*-----END RSA PRIVATE KEY-----/Hidden by Diamanti/g'
+ '[' 0 -eq 0 ']'
+ '[' -s /dev/shm/diamanti-shm ']'
+ timeout -k 5 15 cat /dev/shm/diamanti-shm
+ python -m json.tool
+ collect_container_runtime_data
+ mkdir -p /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//runtime
+ timeout -k 5 15 docker ps -a
++ cat /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//runtime/docker-ps.log
++ grep -v 'CONTAINER ID'
++ cut -d ' ' -f 1
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 3afa91c49676
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect b9e934a9d8fb
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 4591d77be547
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect c245e4ac2d6d
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 8f4bbe16d04f
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 78ff8e20b564
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 76440c4476fb
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 1d21754702de
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect ae6fd5c15bf3
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect f9b6b1fda757
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 5ecf0b839d5a
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 8397aa0b0c7a
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect e234d687442b
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 0ba6dfdb8d72
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 740d7ac791e4
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 90c2663c2b8c
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 0f5db45c6b63
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 5e45a1e8dcba
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect db4ece5f22c0
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 3beb8840cfdd
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 8195843f0f4f
+ timeout -k 5 15 crictl ps -a
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
++ cat /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//runtime/crictl-ps.log
++ grep -v 'CONTAINER ID'
++ cut -d ' ' -f 1
+ for id in '`cat $ts_dir_host/runtime/crictl-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 crictl inspect 'time="2019-11-13T09:24:48-08:00"'
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ collect_kubernetes_data
+ mkdir -p /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//kubernetes
+ for obj in '$K8S_OBJECTS'
+ '[' pv == pvc ']'
+ '[' pv == services ']'
+ '[' pv == endpoints ']'
+ '[' pv == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get pv --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' pvc == pvc ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get pvc --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
No resources found.
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get pvc --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' sc == pvc ']'
+ '[' sc == services ']'
+ '[' sc == endpoints ']'
+ '[' sc == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get sc --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 20 ))
+ kubectl describe sc best-effort
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc high
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc medium
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node1
+ true
+ echo -n .
+ sleep 5
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc snapshot-promoter
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc snapshot-promoter-backup
+ (( i++ ))
+ (( i < 20 ))
+ for obj in '$K8S_OBJECTS'
+ '[' volumesnapshots == pvc ']'
+ '[' volumesnapshots == services ']'
+ '[' volumesnapshots == endpoints ']'
+ '[' volumesnapshots == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get volumesnapshots --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
No resources found.
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get volumesnapshots --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' volumesnapshotdata == pvc ']'
+ '[' volumesnapshotdata == services ']'
+ '[' volumesnapshotdata == endpoints ']'
+ '[' volumesnapshotdata == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ grep -v NAME
++ kubectl get volumesnapshotdata --all-namespaces
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' services == pvc ']'
+ '[' services == services ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get services --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get services --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 12 ))
+ kubectl describe services kubernetes -n default
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services alertmanager-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services collectd-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-attacher -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-provisioner -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-resizer -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-snapshotter -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services prometheus-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services coredns -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services helm-chart -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services metrics-server -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services tiller-deploy -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ for obj in '$K8S_OBJECTS'
+ '[' endpoints == pvc ']'
+ '[' endpoints == services ']'
+ '[' endpoints == endpoints ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get endpoints --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get endpoints --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 14 ))
+ kubectl describe endpoints kubernetes -n default
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints alertmanager-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints collectd-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-attacher -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-provisioner -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-resizer -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-snapshotter -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints prometheus-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints coredns -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints helm-chart -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints kube-controller-manager -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints kube-scheduler -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints metrics-server -n kube-system
+ true
+ echo -n .
+ sleep 5
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints tiller-deploy -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ collect_pod_logs
++ hostname
+ host=appserv55
+ mkdir -p /tmp/tmp.MI3LKKx2vd/appserv55-dw-tech-support-2019.11.13-09.23.19/host-files//pod_logs
+ timeout -k 5 15 kubectl get pods --all-namespaces '-o=jsonpath={range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{",  "}{end}{end}'
+ sort
+ for namespace in '$NS'
++ kubectl get pods --namespace diamanti-system --output=custom-columns=NAME:.metadata.name
++ grep -v NAME
++ grep -v 'No resources found'
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod alertmanager-0 --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-55txt --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n collectd-v0.8-55txt   4/4     Running   0          13m   172.16.6.155   appserv55   <none>           <none> ]]
++ kubectl get pod collectd-v0.8-55txt --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-55txt --container=cadvisor --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-55txt --container=collectd-exporter --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-55txt --container=node-exporter --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-55txt --container=collectd-es --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-67bbn --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-mnw9f --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-6zknb --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n csi-diamanti-driver-6zknb   2/2     Running   1          13m   172.16.6.155   appserv55   <none>           <none> ]]
++ kubectl get pod csi-diamanti-driver-6zknb --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-diamanti-driver-6zknb --container=node-driver-registrar --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-diamanti-driver-6zknb --container=diamanticsidriver --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-rzzjh --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-v7q2s --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-attacher-6bbc9d4bbd-gclkl --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-provisioner-957ff6577-68f2d --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n csi-external-provisioner-957ff6577-68f2d   1/1     Running   0          13m   172.20.0.3   appserv55   <none>           <none> ]]
++ kubectl get pod csi-external-provisioner-957ff6577-68f2d --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-external-provisioner-957ff6577-68f2d --container=csi-external-provisioner --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-resizer-9848cdf68-wrgs8 --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-snapshotter-8c8959567-ds7s4 --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-0 --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n prometheus-v1-0   1/1     Running   0          13m   172.16.6.155   appserv55   <none>           <none> ]]
++ kubectl get pod prometheus-v1-0 --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs prometheus-v1-0 --container=prometheus --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-1 --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-2 --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod provisioner-7b58589b9d-ghzcm --namespace diamanti-system -o wide
++ grep appserv55
+ [[ -n provisioner-7b58589b9d-ghzcm   1/1     Running   0          13m   172.20.0.2   appserv55   <none>           <none> ]]
++ kubectl get pod provisioner-7b58589b9d-ghzcm --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs provisioner-7b58589b9d-ghzcm --container=provisioner --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ grep appserv55
++ kubectl get pod snapshot-controller-66fb5f8fbd-j9qq5 --namespace diamanti-system -o wide
+ [[ -n '' ]]
+ for namespace in '$NS'
++ kubectl get pods --namespace kube-system --output=custom-columns=NAME:.metadata.name
++ grep -v NAME
++ grep -v 'No resources found'
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-6qpcz --namespace kube-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-jtj2f --namespace kube-system -o wide
++ grep appserv55
+ [[ -n coredns-665c8b6494-jtj2f   1/1     Running   0          13m   172.16.6.155   appserv55   <none>           <none> ]]
++ kubectl get pod coredns-665c8b6494-jtj2f --namespace kube-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs coredns-665c8b6494-jtj2f --container=coredns --namespace=kube-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-p2pjv --namespace kube-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod helm-chart-687577f867-hlflj --namespace kube-system -o wide
++ grep appserv55
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod metrics-server-v1-5d46b6d959-zlskw --namespace kube-system -o wide
++ grep appserv55
+ [[ -n metrics-server-v1-5d46b6d959-zlskw   1/1     Running   0          13m   172.16.6.155   appserv55   <none>           <none> ]]
++ kubectl get pod metrics-server-v1-5d46b6d959-zlskw --namespace kube-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs metrics-server-v1-5d46b6d959-zlskw --container=metrics-server --namespace=kube-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod tiller-deploy-5668df8bc4-8nklg --namespace kube-system -o wide
++ grep appserv55
+ [[ -n tiller-deploy-5668df8bc4-8nklg   1/1     Running   0          13m   172.16.179.4   appserv55   <none>           <none> ]]
++ kubectl get pod tiller-deploy-5668df8bc4-8nklg --namespace kube-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs tiller-deploy-5668df8bc4-8nklg --container=tiller --namespace=kube-system -p=true
+ collect_runtime_data
+ '[' true == '' ']'
+ for obj in '$DWS_OBJECTS'
+ '[' node == event ']'
+ timeout -k 5 15 dctl -o json node list
+ for obj in '$DWS_OBJECTS'
+ '[' network == event ']'
+ timeout -k 5 15 dctl -o json network list
+ for obj in '$DWS_OBJECTS'
+ '[' volume == event ']'
+ timeout -k 5 15 dctl -o json volume list
+ for obj in '$DWS_OBJECTS'
+ '[' snapshot == event ']'
+ timeout -k 5 15 dctl -o json snapshot list
+ for obj in '$DWS_OBJECTS'
+ '[' perf-tier == event ']'
+ timeout -k 5 15 dctl -o json perf-tier list
+ for obj in '$DWS_OBJECTS'
+ '[' event == event ']'
+ timeout -k 5 15 dctl -o json event list -l 1000
+ for obj in '$DWS_OBJECTS'
+ '[' user == event ']'
+ timeout -k 5 15 dctl -o json user list
+ for obj in '$DWS_OBJECTS'
+ '[' endpoint == event ']'
+ timeout -k 5 15 dctl -o json endpoint list
+ for obj in '$DWS_OBJECTS'
+ '[' drive == event ']'
+ timeout -k 5 15 dctl -o json drive list
+ for obj in '$DWS_OBJECTS'
+ '[' feature == event ']'
+ timeout -k 5 15 dctl -o json feature list
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get pv --all-namespaces
+ true
+ echo -n .
+ sleep 5
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get pvc --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get sc --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get volumesnapshots --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get volumesnapshotdata --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get services --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get endpoints --all-namespaces
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user group list
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user role list
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user auth-server list
+ kill -13 41475
+ echo ''
+ echo -n 'Packaging tech support information '
+ pb_pid=48215
+ pushd /tmp/tmp.MI3LKKx2vd
+ progress_bar
+ chmod -R a+rX,u+w .
+ true
+ echo -n .
+ sleep 5
+ tar -cvf /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar appserv55-dw-tech-support-2019.11.13-09.23.19
+ xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ popd
+ kill -13 48215
+ echo ''
+ cleanup_tech_support_dirs
+ rm -rf /tmp/tmp.MI3LKKx2vd
+ cleanup_max_techsupport_files
++ wc -l
++ ls -l /data/techsupport/appserv55-dw-tech-support-2019.11.07-15.56.30.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.07-15.59.00.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-11.15.36.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-17.51.26.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-03.42.31.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-06.29.26.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-13.22.03.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-20.00.06.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-05.51.32.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-23.27.42.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-01.50.23.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-06.04.23.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-08.36.28.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-20.24.33.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-05.50.43.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-08.19.22.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-20.48.17.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-00.52.17.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-05.43.54.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar.xz
+ '[' 31 -gt 30 ']'
+ head -1
+ xargs rm -f
+ ls -tr /data/techsupport/appserv55-dw-tech-support-2019.11.07-15.56.30.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.07-15.59.00.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-11.15.36.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-17.51.26.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-03.42.31.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-06.29.26.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-13.22.03.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-20.00.06.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-05.51.32.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-23.27.42.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-01.50.23.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-06.04.23.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-08.36.28.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-20.24.33.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-05.50.43.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-08.19.22.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-20.48.17.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-00.52.17.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-05.43.54.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar.xz
++ wc -l
++ ls -l /data/techsupport/appserv55-dw-tech-support-2019.11.07-15.59.00.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-11.15.36.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.08-17.51.26.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-03.42.31.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-06.29.26.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-13.22.03.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.09-20.00.06.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-05.51.32.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.10-23.27.42.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-01.50.23.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-06.04.23.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-08.36.28.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.11-20.24.33.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-05.50.43.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-08.19.22.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.12-20.48.17.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-00.52.17.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-05.43.54.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar.xz
+ '[' 30 -gt 30 ']'
+ rm -f /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.in_progress
+ '[' '!' -f /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar.xz ']'
+ chown diamanti:diamanti /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar.xz
+ echo 'Tech support collection done, available at: /data/techsupport/appserv55-dw-tech-support-2019.11.13-09.23.19.tar.xz'


Copying tech-support dump from appserv54

+ PROG=/usr/local/bin/dw-techsupport
+ TS_FILE_PREFIX=appserv54-dw-tech-support-
+ TS_FILE_SUFFIX=.tar
+ TS_OUTPUT_FILE=
+ TS_OUTPUT_DIR=/data/techsupport
+ TS_FILE_XZ=.xz
+ TS_PATH_XZ_FILE='/data/techsupport/*.tar.xz'
+ TS_MAX_FILES=30
+ [[ 1 -gt 0 ]]
+ key=--best-effort
+ case $key in
+ BEST_EFFORT=true
+ shift
+ [[ 0 -gt 0 ]]
+ CONDUIT_EMBEDDED_IP=192.168.100.2
+ EMBEDDED_LOG_FILES_DIR=/tmp
+ EMBEDDED_TS_FILE_BACKUP=/mnt/embedded-latest-techsupport.tar.gz
+ EMBEDDED_TS_BACKUP_DIR=/mnt/ts_backup
+ EMBEDDED_PERSISTENT_LOG_DIR=/mnt/logs
++ mktemp -d
+ TEMP_DIR=/tmp/tmp.tgx2rT3Xjn
+ TS_DIR_PREFIX=/tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-
+ TS_DIR_HOST=host-files/
+ TS_DIR_EMBEDDED=embedded-files/
+ TS_DIR_DWS=dws/
+ HOST_SERVICES_SYSTEMD='armada bosun convoy apiserver scheduler kubelet 	controller-manager etcd docker docker-storage-setup        	dock postgres ntpd proxy kube-dnsmasq kubedns upgrade.service crio'
+ HOST_SERVICES_PROCESS='armada bosun convoy kube-apiserver kube-scheduler kubelet 	kube-controller etcd dockerd-current container-storage-setup        	dock postgres ntpd kube-proxy dnsmasq-nanny kube-dns upgrade crio'
+ DWS_OBJECTS='node network volume snapshot perf-tier event user endpoint drive feature'
+ K8S_OBJECTS='pv pvc sc volumesnapshots volumesnapshotdata services endpoints'
+ DWS_USER_OBJECTS='group role auth-server'
+ HOST_DATA_DIR=/var/lib/diamanti
+ HOST_CONFIG_DIR=/etc/diamanti
+ HOST_PCYCLE_HISTORY='/usr/lib/firmware/diamanti/.dws*'
+ SSHOPTIONS='-o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no 	-o LogLevel=ERROR -o ConnectTimeout=15'
+ HOST_TIMEOUT=15
+ HOST_TIMEOUT_LONG=180
+ HOST_TIMEOUT_KILL=5
+ EMBEDDED_TIMEOUT=150
+ NS='diamanti-system kube-system'
+ collect_tech_support
+ '[' 0 -ne 0 ']'
++ dctl whoami
+ DUMMY='Name:         admin
Built-In:     true
Local-Auth:   true
Groups:       user-admin, cluster-admin
Roles:        user-edit, allcontainer-edit, volumeclaim-edit/default, volume-edit, required, container-edit/default, network-edit, node-edit, perftier-edit
Namespace:    default'
+ '[' 0 -eq 1 ']'
+ LOGGED_IN=true
+ set_tech_support_timestamp
++ date +%Y.%m.%d-%H.%M.%S
+ ts_timestamp=2019.11.13-09.23.19
+ '[' '' = '' ']'
+ TS_OUTPUT_FILE=appserv54-dw-tech-support-2019.11.13-09.23.19
+ TECH_SUPPORT_FILE=appserv54-dw-tech-support-2019.11.13-09.23.19.tar
+ mkdir -p /data/techsupport
+ '[' 0 -ne 0 ']'
+ '[' '!' -z appserv54-dw-tech-support-2019.11.13-09.23.19 -a appserv54-dw-tech-support-2019.11.13-09.23.19 '!=' ' ' ']'
+ touch /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.in_progress
+ '[' 0 -ne 0 ']'
+ logger Collecting dw-techsupport
+ create_tech_support_dirs
+ ts_dir=/tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/
+ mkdir /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/
+ ts_dir_dws=/tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/dws/
+ mkdir /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/dws/
+ ts_dir_host=/tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ mkdir /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ ts_dir_embedded=/tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ mkdir /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ echo -n 'Collecting firmware tech support information '
+ pb_pid=43466
+ collect_embedded_tech_support
+ embedded_tar_file=embedded-2019.11.13-09.23.19.tar.gz
+ ping -c 1 192.168.100.2
+ progress_bar
+ true
+ echo -n .
+ sleep 5
+ '[' 0 -ne 0 ']'
+ run_embedded_cmd '/dwsscripts/embedded_techsupport.sh embedded-2019.11.13-09.23.19.tar.gz' 'Failed to collect embedded techsupport'
+ timeout 150 ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 root@192.168.100.2 /dwsscripts/embedded_techsupport.sh embedded-2019.11.13-09.23.19.tar.gz
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ '[' 0 -ne 0 ']'
+ get_embedded_file embedded-2019.11.13-09.23.19.tar.gz /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2:/embedded-2019.11.13-09.23.19.tar.gz /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ '[' 0 -eq 0 ']'
+ pushd /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ tar -xvzf embedded-2019.11.13-09.23.19.tar.gz
+ rm -rf embedded-2019.11.13-09.23.19.tar.gz
+ popd
+ run_embedded_cmd 'rm embedded-2019.11.13-09.23.19.tar.gz' 'Failed to remove the tar of embedded logs embedded-2019.11.13-09.23.19.tar.gz'
+ timeout 150 ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 root@192.168.100.2 rm embedded-2019.11.13-09.23.19.tar.gz
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ get_embedded_file /mnt/ts_backup /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2://mnt/ts_backup /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files/
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ mkdir /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files//persistent
+ get_embedded_file /mnt/logs /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files//persistent
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2://mnt/logs /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/embedded-files//persistent
+ true
+ echo -n .
+ sleep 5
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ kill -13 43466
+ echo ''
+ echo -n 'Collecting host tech support information '
+ pb_pid=43688
+ collect_host_tech_support
+ collect_logs
+ mkdir -p /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//logs
+ progress_bar
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 cp -rf /var/log/diamanti/core /var/log/diamanti/embedded /var/log/diamanti/kubernetes /var/log/diamanti/plugin /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//logs/
+ collect_interrupts
++ seq 0 4
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ true
+ echo -n .
+ sleep 5
+ collect_system_data
+ timeout -k 5 15 cp /var/log/messages /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /var/log/mcelog /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /var/log/secure /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 dmesg -T
+ timeout -k 5 15 df -h
+ timeout -k 5 15 date
+ timeout -k 5 15 uptime
+ timeout -k 5 15 ls -lrRth /var/log/
+ timeout -k 5 15 ls -lrRth /var/log/diamanti/
+ timeout -k 5 15 ls -lrt /var/lib/cni/networks/mgmt
+ timeout -k 5 15 top -b -n 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 systemd-cgtop -b -n 5
+ timeout -k 5 15 systemd-cgls
+ timeout -k 5 15 systemctl status
+ timeout -k 5 15 systemctl list-unit-files
+ timeout -k 5 15 systemctl list-units
+ timeout -k 5 15 systemctl list-timers --all
+ timeout -k 5 15 ps auxf
+ timeout -k 5 15 cp /proc/cpuinfo /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /proc/meminfo /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /proc/devices /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /proc/mounts /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 lspci -vv
+ timeout -k 5 15 cp /usr/local/.patches/.installed /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//installed-patches.txt
+ timeout -k 5 15 cp /usr/local/.updates/.installed /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//installed-updates.txt
+ timeout -k 5 15 cp /usr/share/diamanti/git-manifest.txt /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//git-manifest.txt
+ timeout -k 5 15 cp /etc/diamanti-release /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//diamanti-release
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.desd /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//desd.txt
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.desl /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//desl.txt
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.deso /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//deso.txt
+ timeout -k 5 15 cp '/usr/lib/firmware/diamanti/.dws*' /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 ipmitool sel elist
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ipmitool sdr
+ timeout -k 5 15 ipmitool fru
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ditool
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ntpq -p
+ timeout -k 5 15 ntpstat
+ timeout -k 5 15 netstat -ontap
+ timeout -k 5 15 netstat -s
+ timeout -k 5 15 rpm -qi diamanti-cx
+ timeout -k 5 15 cat /proc/mounts
+ timeout -k 5 15 ls -l /dev/nvme0
+ timeout -k 5 15 echo -e '\ncommand : dstool -s'
+ timeout -k 5 15 dstool -s
+ timeout -k 5 15 echo -e '\ncommand : dstool -c "mputil -w rs_btp"'
+ timeout -k 5 15 dstool -c 'mputil -w rs_btp'
+ timeout -k 5 15 echo -e '\ncommand : dstool -c "dmesg" | grep "Link is up"'
+ timeout -k 5 15 dstool -c dmesg
+ grep 'Link is up'
+ timeout -k 5 15 cat /etc/fstab
+ timeout -k 5 15 cat /etc/resolv.conf
+ timeout -k 5 15 sysctl -a
+ collect_crash_data
+ mkdir -p /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//crash
+ for dir in '/var/crash/*'
++ basename '/var/crash/*'
+ name='*'
+ timeout -k 5 15 cp '/var/crash/*/vmcore-dmesg.txt' '/tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//crash/*-vmcore-dmesg.txt'
+ collect_config_data
+ timeout -k 5 15 cp /etc/systemd/journald.conf /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /etc/etcd/etcd.conf /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 tar -cvzf /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//data.tar.gz /var/lib/diamanti
+ timeout -k 5 15 tar -cvzf /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//config.tar.gz /etc/diamanti
+ timeout -k 5 15 find /etc/diamanti/ -name '*.conf' -exec cp '{}' /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/ ';'
+ collect_network_data
+ timeout -k 5 15 ifconfig -a
+ timeout -k 5 15 ip route list
+ timeout -k 5 15 arp -an
+ printf '\n\n netstat -anevop -t\n'
+ timeout -k 5 15 netstat -anevop -t
+ printf '\n\n  netstat -ian -t\n'
+ timeout -k 5 15 netstat -ian -t
+ printf '\n\n netstat -s -t\n'
+ timeout -k 5 15 netstat -s -t
+ printf '\n\n iptables -L\n'
+ timeout -k 5 15 iptables -L
+ printf '\n\n iptables -vL -t filter\n'
+ timeout -k 5 15 iptables -vL -t filter
+ printf '\n\n iptables -vL -t nat\n'
+ timeout -k 5 15 iptables -vL -t nat
+ printf '\n\n iptables -vL -t mangle\n'
+ timeout -k 5 15 iptables -vL -t mangle
+ printf '\n\n iptables -vL -t raw\n'
+ timeout -k 5 15 iptables -vL -t raw
+ printf '\n\n iptables -vL -t security\n'
+ timeout -k 5 15 iptables -vL -t security
+ printf '\n\n iptables -S -t nat\n'
+ timeout -k 5 15 iptables -S -t nat
+ collect_socket_and_fd_data
+ mkdir -p /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//fds
+ mkdir -p /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//tcp_sockets
+ timeout -k 5 15 lsof
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o armada
+ SVC_PID=22797
+ '[' 22797 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/22797/fd
+ timeout -k 5 15 cat /proc/22797/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o bosun
+ SVC_PID=23779
+ '[' 23779 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23779/fd
+ timeout -k 5 15 cat /proc/23779/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o convoy
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-apiserver
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-scheduler
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kubelet
+ SVC_PID=23753
+ '[' 23753 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23753/fd
+ timeout -k 5 15 cat /proc/23753/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-controller
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o etcd
+ SVC_PID=23479
+ '[' 23479 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23479/fd
+ timeout -k 5 15 cat /proc/23479/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dockerd-current
+ SVC_PID=6965
+ '[' 6965 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/6965/fd
+ timeout -k 5 15 cat /proc/6965/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o container-storage-setup
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dock
+ SVC_PID=22799
+ '[' 22799 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/22799/fd
+ timeout -k 5 15 cat /proc/22799/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o postgres
+ SVC_PID=27806
+ '[' 27806 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/27806/fd
+ timeout -k 5 15 cat /proc/27806/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o ntpd
+ SVC_PID=10032
+ '[' 10032 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/10032/fd
+ timeout -k 5 15 cat /proc/10032/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-proxy
+ SVC_PID=23778
+ '[' 23778 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23778/fd
+ timeout -k 5 15 cat /proc/23778/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dnsmasq-nanny
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-dns
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o upgrade
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o crio
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ collect_nic_data
+ '[' -d /sys/bus/pci/drivers/cxgb4 ']'
+ timeout -k 5 15 cp /lib/firmware/cxgb4/t5-config.txt /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
+ timeout -k 5 15 cp /lib/firmware/cxgb4/t6-config.txt /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files/
++ ls /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -i mgmtpf1,0
+ printf '\n\n ethtool -k %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -k mgmtpf1,0
+ printf '\n\n ethtool -S %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -S mgmtpf1,0
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -i mgmtpf1,1
+ printf '\n\n ethtool -k %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -k mgmtpf1,1
+ printf '\n\n ethtool -S %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -S mgmtpf1,1
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -i mgmtpf1,2
+ printf '\n\n ethtool -k %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -k mgmtpf1,2
+ printf '\n\n ethtool -S %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -S mgmtpf1,2
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -i mgmtpf1,3
+ printf '\n\n ethtool -k %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -k mgmtpf1,3
+ printf '\n\n ethtool -S %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -S mgmtpf1,3
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4
+ timeout -k 5 15 ethtool -i ens801f4
+ printf '\n\n ethtool -k %s\n' ens801f4
+ timeout -k 5 15 ethtool -k ens801f4
+ printf '\n\n ethtool -S %s\n' ens801f4
+ timeout -k 5 15 ethtool -S ens801f4
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -i ens801f4d1
+ printf '\n\n ethtool -k %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -k ens801f4d1
+ printf '\n\n ethtool -S %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -S ens801f4d1
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -i ens801f4d2
+ printf '\n\n ethtool -k %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -k ens801f4d2
+ printf '\n\n ethtool -S %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -S ens801f4d2
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -i ens801f4d3
+ printf '\n\n ethtool -k %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -k ens801f4d3
+ printf '\n\n ethtool -S %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -S ens801f4d3
+ printf '\n\n cat /sys/kernel/debug/cxgb4/*/tp_stats \n' ens801f4d3
+ timeout -k 5 15 cat /sys/kernel/debug/cxgb4/0000:81:00.4/tp_stats
+ printf '\n\n cat /sys/kernel/debug/cxgb4vf/*/sge_qstats \n' ens801f4d3
+ timeout -k 5 15 cat /sys/kernel/debug/cxgb4vf/0000:81:01.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.7/sge_qstats
+ collect_storage_data
+ printf '\n\n lsblk -l\n'
+ timeout -k 5 15 lsblk -l
+ printf '\n\n lsblk -t\n'
+ timeout -k 5 15 lsblk -t
+ timeout -k 5 15 iostat -x 1 20
+ grep nvme
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ collect_services_data
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status armada
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u armada -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status bosun
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u bosun -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status convoy
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u convoy -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status apiserver
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u apiserver -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status scheduler
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u scheduler -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kubelet
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kubelet -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status controller-manager
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u controller-manager -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status etcd
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u etcd -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status docker
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u docker -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status docker-storage-setup
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u docker-storage-setup -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status dock
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u dock -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status postgres
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u postgres -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status ntpd
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u ntpd -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status proxy
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u proxy -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kube-dnsmasq
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kube-dnsmasq -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kubedns
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kubedns -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status upgrade.service
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u upgrade.service -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status crio
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u crio -b -l
+ timeout -k 5 180 journalctl -b -l
+ collect_kvstore_data
+ timeout -k 5 15 curl --silent -X GET http://127.0.0.1:12345/api/v1/cluster
++ grep master /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/dws//cluster.log
++ awk ' { print $2 } '
++ sed 's/[", ]//g'
++ head -n 1
+ master=appserv53
+ '[' -z appserv53 ']'
++ timeout -k 5 15 curl -k --write-out '%{http_code}' --silent --output /dev/null http://appserv54:2379/v2/members
+ response_code=200
+ test 200 -eq 200
+ timeout -k 5 15 etcdctl -C http://appserv54:2379 cluster-health
+ sed 's/-----BEGIN RSA PRIVATE KEY-----.*-----END RSA PRIVATE KEY-----/Hidden by Diamanti/g'
+ ETCDCTL_API=3
+ timeout -k 5 15 etcdctl get --endpoints=appserv54:2379 --prefix / --sort-by=KEY
+ '[' 0 -eq 0 ']'
+ '[' -s /dev/shm/diamanti-shm ']'
+ timeout -k 5 15 cat /dev/shm/diamanti-shm
+ python -m json.tool
+ collect_container_runtime_data
+ mkdir -p /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//runtime
+ timeout -k 5 15 docker ps -a
++ cat /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//runtime/docker-ps.log
++ grep -v 'CONTAINER ID'
++ cut -d ' ' -f 1
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 80438570ac74
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 18234fa4c28b
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect dc805f769436
+ true
+ echo -n .
+ sleep 5
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect a66241317a16
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 471f8880642a
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 11381460afdc
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 3e073f3f3794
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 7d17aed244de
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect def1bc8792ec
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 3eb99c4a202e
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect d093bd9fdacb
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 995c6d8484f4
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 8d5215c048f7
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 24c0863af54f
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 21e9d9658579
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 5d8dceb52211
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 22466bbac66c
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 56174a00def5
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect fea6275e3181
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect ec90db442fc8
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 057a6cda7e52
+ timeout -k 5 15 crictl ps -a
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
++ cat /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//runtime/crictl-ps.log
++ grep -v 'CONTAINER ID'
++ cut -d ' ' -f 1
+ for id in '`cat $ts_dir_host/runtime/crictl-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 crictl inspect 'time="2019-11-13T09:24:51-08:00"'
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ collect_kubernetes_data
+ mkdir -p /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//kubernetes
+ for obj in '$K8S_OBJECTS'
+ '[' pv == pvc ']'
+ '[' pv == services ']'
+ '[' pv == endpoints ']'
+ '[' pv == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get pv --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' pvc == pvc ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get pvc --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
No resources found.
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get pvc --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' sc == pvc ']'
+ '[' sc == services ']'
+ '[' sc == endpoints ']'
+ '[' sc == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get sc --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 20 ))
+ kubectl describe sc best-effort
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc high
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc medium
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc snapshot-promoter
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc snapshot-promoter-backup
+ (( i++ ))
+ (( i < 20 ))
+ for obj in '$K8S_OBJECTS'
+ '[' volumesnapshots == pvc ']'
+ '[' volumesnapshots == services ']'
+ '[' volumesnapshots == endpoints ']'
+ '[' volumesnapshots == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get volumesnapshots --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
No resources found.
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get volumesnapshots --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' volumesnapshotdata == pvc ']'
+ '[' volumesnapshotdata == services ']'
+ '[' volumesnapshotdata == endpoints ']'
+ '[' volumesnapshotdata == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get volumesnapshotdata --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' services == pvc ']'
+ '[' services == services ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get services --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get services --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 12 ))
+ kubectl describe services kubernetes -n default
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services alertmanager-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services collectd-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-attacher -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-provisioner -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-resizer -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-snapshotter -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services prometheus-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services coredns -n kube-system
+ true
+ echo -n .
+ sleep 5
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services helm-chart -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services metrics-server -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services tiller-deploy -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ for obj in '$K8S_OBJECTS'
+ '[' endpoints == pvc ']'
+ '[' endpoints == services ']'
+ '[' endpoints == endpoints ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get endpoints --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get endpoints --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 14 ))
+ kubectl describe endpoints kubernetes -n default
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints alertmanager-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints collectd-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-attacher -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-provisioner -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-resizer -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-snapshotter -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints prometheus-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints coredns -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints helm-chart -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints kube-controller-manager -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints kube-scheduler -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints metrics-server -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints tiller-deploy -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ collect_pod_logs
++ hostname
+ host=appserv54
+ mkdir -p /tmp/tmp.tgx2rT3Xjn/appserv54-dw-tech-support-2019.11.13-09.23.19/host-files//pod_logs
+ timeout -k 5 15 kubectl get pods --all-namespaces '-o=jsonpath={range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{",  "}{end}{end}'
+ sort
+ for namespace in '$NS'
++ kubectl get pods --namespace diamanti-system --output=custom-columns=NAME:.metadata.name
++ grep -v NAME
++ grep -v 'No resources found'
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod alertmanager-0 --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n alertmanager-0   1/1     Running   0          13m   172.16.6.154   appserv54   <none>           <none> ]]
++ kubectl get pod alertmanager-0 --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs alertmanager-0 --container=alertmanager --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-55txt --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-67bbn --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n collectd-v0.8-67bbn   4/4     Running   0          13m   172.16.6.154   appserv54   <none>           <none> ]]
++ kubectl get pod collectd-v0.8-67bbn --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-67bbn --container=cadvisor --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-67bbn --container=collectd-exporter --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-67bbn --container=node-exporter --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-67bbn --container=collectd-es --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-mnw9f --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-6zknb --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-rzzjh --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-v7q2s --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n csi-diamanti-driver-v7q2s   2/2     Running   1          13m   172.16.6.154   appserv54   <none>           <none> ]]
++ kubectl get pod csi-diamanti-driver-v7q2s --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-diamanti-driver-v7q2s --container=node-driver-registrar --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-diamanti-driver-v7q2s --container=diamanticsidriver --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-attacher-6bbc9d4bbd-gclkl --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-provisioner-957ff6577-68f2d --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ grep appserv54
++ kubectl get pod csi-external-resizer-9848cdf68-wrgs8 --namespace diamanti-system -o wide
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ grep appserv54
++ kubectl get pod csi-external-snapshotter-8c8959567-ds7s4 --namespace diamanti-system -o wide
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-0 --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-1 --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n prometheus-v1-1   1/1     Running   0          13m   172.16.6.154   appserv54   <none>           <none> ]]
++ kubectl get pod prometheus-v1-1 --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs prometheus-v1-1 --container=prometheus --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-2 --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod provisioner-7b58589b9d-ghzcm --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod snapshot-controller-66fb5f8fbd-j9qq5 --namespace diamanti-system -o wide
++ grep appserv54
+ [[ -n snapshot-controller-66fb5f8fbd-j9qq5   2/2     Running   0          13m   172.20.0.2   appserv54   <none>           <none> ]]
++ kubectl get pod snapshot-controller-66fb5f8fbd-j9qq5 --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ true
+ echo -n .
+ sleep 5
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs snapshot-controller-66fb5f8fbd-j9qq5 --container=snapshot-controller --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs snapshot-controller-66fb5f8fbd-j9qq5 --container=snapshot-provisioner --namespace=diamanti-system -p=true
+ for namespace in '$NS'
++ kubectl get pods --namespace kube-system --output=custom-columns=NAME:.metadata.name
++ grep -v NAME
++ grep -v 'No resources found'
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-6qpcz --namespace kube-system -o wide
++ grep appserv54
+ [[ -n coredns-665c8b6494-6qpcz   1/1     Running   0          13m   172.16.6.154   appserv54   <none>           <none> ]]
++ kubectl get pod coredns-665c8b6494-6qpcz --namespace kube-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs coredns-665c8b6494-6qpcz --container=coredns --namespace=kube-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-jtj2f --namespace kube-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-p2pjv --namespace kube-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ grep appserv54
++ kubectl get pod helm-chart-687577f867-hlflj --namespace kube-system -o wide
+ [[ -n helm-chart-687577f867-hlflj   1/1     Running   0          13m   172.16.179.5   appserv54   <none>           <none> ]]
++ kubectl get pod helm-chart-687577f867-hlflj --namespace kube-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs helm-chart-687577f867-hlflj --container=helm-chart --namespace=kube-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod metrics-server-v1-5d46b6d959-zlskw --namespace kube-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod tiller-deploy-5668df8bc4-8nklg --namespace kube-system -o wide
++ grep appserv54
+ [[ -n '' ]]
+ collect_runtime_data
+ '[' true == '' ']'
+ for obj in '$DWS_OBJECTS'
+ '[' node == event ']'
+ timeout -k 5 15 dctl -o json node list
+ for obj in '$DWS_OBJECTS'
+ '[' network == event ']'
+ timeout -k 5 15 dctl -o json network list
+ for obj in '$DWS_OBJECTS'
+ '[' volume == event ']'
+ timeout -k 5 15 dctl -o json volume list
+ for obj in '$DWS_OBJECTS'
+ '[' snapshot == event ']'
+ timeout -k 5 15 dctl -o json snapshot list
+ for obj in '$DWS_OBJECTS'
+ '[' perf-tier == event ']'
+ timeout -k 5 15 dctl -o json perf-tier list
+ for obj in '$DWS_OBJECTS'
+ '[' event == event ']'
+ timeout -k 5 15 dctl -o json event list -l 1000
+ for obj in '$DWS_OBJECTS'
+ '[' user == event ']'
+ timeout -k 5 15 dctl -o json user list
+ for obj in '$DWS_OBJECTS'
+ '[' endpoint == event ']'
+ timeout -k 5 15 dctl -o json endpoint list
+ for obj in '$DWS_OBJECTS'
+ '[' drive == event ']'
+ timeout -k 5 15 dctl -o json drive list
+ for obj in '$DWS_OBJECTS'
+ '[' feature == event ']'
+ timeout -k 5 15 dctl -o json feature list
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get pv --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get pvc --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get sc --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get volumesnapshots --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get volumesnapshotdata --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get services --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get endpoints --all-namespaces
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user group list
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user role list
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user auth-server list
+ kill -13 43688
+ echo ''
+ echo -n 'Packaging tech support information '
+ pb_pid=50428
+ pushd /tmp/tmp.tgx2rT3Xjn
+ progress_bar
+ chmod -R a+rX,u+w .
+ true
+ echo -n .
+ sleep 5
+ tar -cvf /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar appserv54-dw-tech-support-2019.11.13-09.23.19
+ xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ popd
+ kill -13 50428
+ echo ''
+ cleanup_tech_support_dirs
+ rm -rf /tmp/tmp.tgx2rT3Xjn
+ cleanup_max_techsupport_files
++ wc -l
++ ls -l /data/techsupport/appserv54-dw-tech-support-2019.11.07-15.56.30.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.07-15.59.00.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-11.15.36.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-17.51.26.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-03.42.31.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-06.29.26.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-13.22.03.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-20.00.06.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-05.51.34.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-23.27.42.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-01.50.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-06.04.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-08.36.28.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-20.24.33.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-05.50.47.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-08.19.54.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-20.48.20.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-00.52.18.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-05.44.00.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar.xz
+ '[' 31 -gt 30 ']'
+ head -1
+ xargs rm -f
+ ls -tr /data/techsupport/appserv54-dw-tech-support-2019.11.07-15.56.30.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.07-15.59.00.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-11.15.36.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-17.51.26.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-03.42.31.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-06.29.26.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-13.22.03.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-20.00.06.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-05.51.34.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-23.27.42.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-01.50.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-06.04.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-08.36.28.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-20.24.33.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-05.50.47.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-08.19.54.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-20.48.20.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-00.52.18.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-05.44.00.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar.xz
++ wc -l
++ ls -l /data/techsupport/appserv54-dw-tech-support-2019.11.07-15.59.00.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-11.15.36.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.08-17.51.26.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-03.42.31.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-06.29.26.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-13.22.03.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.09-20.00.06.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-05.51.34.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.10-23.27.42.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-01.50.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-06.04.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-08.36.28.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.11-20.24.33.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-05.50.47.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-08.19.54.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.12-20.48.20.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-00.52.18.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-05.44.00.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar.xz
+ '[' 30 -gt 30 ']'
+ rm -f /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.in_progress
+ '[' '!' -f /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar.xz ']'
+ chown diamanti:diamanti /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar.xz
+ echo 'Tech support collection done, available at: /data/techsupport/appserv54-dw-tech-support-2019.11.13-09.23.19.tar.xz'


Copying tech-support dump from appserv53

+ PROG=/usr/local/bin/dw-techsupport
+ TS_FILE_PREFIX=appserv53-dw-tech-support-
+ TS_FILE_SUFFIX=.tar
+ TS_OUTPUT_FILE=
+ TS_OUTPUT_DIR=/data/techsupport
+ TS_FILE_XZ=.xz
+ TS_PATH_XZ_FILE='/data/techsupport/*.tar.xz'
+ TS_MAX_FILES=30
+ [[ 1 -gt 0 ]]
+ key=--best-effort
+ case $key in
+ BEST_EFFORT=true
+ shift
+ [[ 0 -gt 0 ]]
+ CONDUIT_EMBEDDED_IP=192.168.100.2
+ EMBEDDED_LOG_FILES_DIR=/tmp
+ EMBEDDED_TS_FILE_BACKUP=/mnt/embedded-latest-techsupport.tar.gz
+ EMBEDDED_TS_BACKUP_DIR=/mnt/ts_backup
+ EMBEDDED_PERSISTENT_LOG_DIR=/mnt/logs
++ mktemp -d
+ TEMP_DIR=/tmp/tmp.uabU2KzSgC
+ TS_DIR_PREFIX=/tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-
+ TS_DIR_HOST=host-files/
+ TS_DIR_EMBEDDED=embedded-files/
+ TS_DIR_DWS=dws/
+ HOST_SERVICES_SYSTEMD='armada bosun convoy apiserver scheduler kubelet 	controller-manager etcd docker docker-storage-setup        	dock postgres ntpd proxy kube-dnsmasq kubedns upgrade.service crio'
+ HOST_SERVICES_PROCESS='armada bosun convoy kube-apiserver kube-scheduler kubelet 	kube-controller etcd dockerd-current container-storage-setup        	dock postgres ntpd kube-proxy dnsmasq-nanny kube-dns upgrade crio'
+ DWS_OBJECTS='node network volume snapshot perf-tier event user endpoint drive feature'
+ K8S_OBJECTS='pv pvc sc volumesnapshots volumesnapshotdata services endpoints'
+ DWS_USER_OBJECTS='group role auth-server'
+ HOST_DATA_DIR=/var/lib/diamanti
+ HOST_CONFIG_DIR=/etc/diamanti
+ HOST_PCYCLE_HISTORY='/usr/lib/firmware/diamanti/.dws*'
+ SSHOPTIONS='-o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no 	-o LogLevel=ERROR -o ConnectTimeout=15'
+ HOST_TIMEOUT=15
+ HOST_TIMEOUT_LONG=180
+ HOST_TIMEOUT_KILL=5
+ EMBEDDED_TIMEOUT=150
+ NS='diamanti-system kube-system'
+ collect_tech_support
+ '[' 0 -ne 0 ']'
++ dctl whoami
+ DUMMY='Name:         admin
Built-In:     true
Local-Auth:   true
Groups:       user-admin, cluster-admin
Roles:        required, allcontainer-edit, network-edit, node-edit, perftier-edit, volume-edit, user-edit, container-edit/default, volumeclaim-edit/default
Namespace:    default'
+ '[' 0 -eq 1 ']'
+ LOGGED_IN=true
+ set_tech_support_timestamp
++ date +%Y.%m.%d-%H.%M.%S
+ ts_timestamp=2019.11.13-09.23.20
+ '[' '' = '' ']'
+ TS_OUTPUT_FILE=appserv53-dw-tech-support-2019.11.13-09.23.20
+ TECH_SUPPORT_FILE=appserv53-dw-tech-support-2019.11.13-09.23.20.tar
+ mkdir -p /data/techsupport
+ '[' 0 -ne 0 ']'
+ '[' '!' -z appserv53-dw-tech-support-2019.11.13-09.23.20 -a appserv53-dw-tech-support-2019.11.13-09.23.20 '!=' ' ' ']'
+ touch /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.in_progress
+ '[' 0 -ne 0 ']'
+ logger Collecting dw-techsupport
+ create_tech_support_dirs
+ ts_dir=/tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/
+ mkdir /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/
+ ts_dir_dws=/tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/dws/
+ mkdir /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/dws/
+ ts_dir_host=/tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ mkdir /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ ts_dir_embedded=/tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files/
+ mkdir /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files/
+ echo -n 'Collecting firmware tech support information '
+ pb_pid=50502
+ collect_embedded_tech_support
+ embedded_tar_file=embedded-2019.11.13-09.23.20.tar.gz
+ ping -c 1 192.168.100.2
+ progress_bar
+ true
+ echo -n .
+ sleep 5
+ '[' 0 -ne 0 ']'
+ run_embedded_cmd '/dwsscripts/embedded_techsupport.sh embedded-2019.11.13-09.23.20.tar.gz' 'Failed to collect embedded techsupport'
+ timeout 150 ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 root@192.168.100.2 /dwsscripts/embedded_techsupport.sh embedded-2019.11.13-09.23.20.tar.gz
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ '[' 0 -ne 0 ']'
+ get_embedded_file embedded-2019.11.13-09.23.20.tar.gz /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files/
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2:/embedded-2019.11.13-09.23.20.tar.gz /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files/
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ '[' 0 -eq 0 ']'
+ pushd /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files/
+ tar -xvzf embedded-2019.11.13-09.23.20.tar.gz
+ rm -rf embedded-2019.11.13-09.23.20.tar.gz
+ popd
+ run_embedded_cmd 'rm embedded-2019.11.13-09.23.20.tar.gz' 'Failed to remove the tar of embedded logs embedded-2019.11.13-09.23.20.tar.gz'
+ timeout 150 ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 root@192.168.100.2 rm embedded-2019.11.13-09.23.20.tar.gz
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ get_embedded_file /mnt/ts_backup /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files/
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2://mnt/ts_backup /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files/
+ true
+ echo -n .
+ sleep 5
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ mkdir /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files//persistent
+ get_embedded_file /mnt/logs /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files//persistent
+ timeout 150 scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=ERROR -o ConnectTimeout=15 -r root@192.168.100.2://mnt/logs /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/embedded-files//persistent
+ val=0
+ '[' 0 -ne 0 ']'
+ return 0
+ kill -13 50502
+ echo ''
+ echo -n 'Collecting host tech support information '
+ pb_pid=50719
+ collect_host_tech_support
+ collect_logs
+ progress_bar
+ mkdir -p /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//logs
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 cp -rf /var/log/diamanti/core /var/log/diamanti/embedded /var/log/diamanti/kubernetes /var/log/diamanti/plugin /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//logs/
+ collect_interrupts
++ seq 0 4
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ for i in '`seq 0 4`'
+ timeout -k 5 15 cat /proc/interrupts
+ printf '\n\n'
+ sleep 1
+ true
+ echo -n .
+ sleep 5
+ collect_system_data
+ timeout -k 5 15 cp /var/log/messages /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 cp /var/log/mcelog /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 cp /var/log/secure /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 dmesg -T
+ timeout -k 5 15 df -h
+ timeout -k 5 15 date
+ timeout -k 5 15 uptime
+ timeout -k 5 15 ls -lrRth /var/log/
+ timeout -k 5 15 ls -lrRth /var/log/diamanti/
+ timeout -k 5 15 ls -lrt /var/lib/cni/networks/mgmt
+ timeout -k 5 15 top -b -n 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 systemd-cgtop -b -n 5
+ timeout -k 5 15 systemd-cgls
+ timeout -k 5 15 systemctl status
+ timeout -k 5 15 systemctl list-unit-files
+ timeout -k 5 15 systemctl list-units
+ timeout -k 5 15 systemctl list-timers --all
+ timeout -k 5 15 ps auxf
+ timeout -k 5 15 cp /proc/cpuinfo /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 cp /proc/meminfo /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 cp /proc/devices /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 cp /proc/mounts /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 lspci -vv
+ timeout -k 5 15 cp /usr/local/.patches/.installed /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//installed-patches.txt
+ timeout -k 5 15 cp /usr/local/.updates/.installed /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//installed-updates.txt
+ timeout -k 5 15 cp /usr/share/diamanti/git-manifest.txt /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//git-manifest.txt
+ timeout -k 5 15 cp /etc/diamanti-release /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//diamanti-release
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.desd /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//desd.txt
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.desl /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//desl.txt
+ timeout -k 5 15 cp /usr/lib/firmware/diamanti/.deso /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//deso.txt
+ timeout -k 5 15 cp '/usr/lib/firmware/diamanti/.dws*' /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 ipmitool sel elist
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ipmitool sdr
+ timeout -k 5 15 ipmitool fru
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ditool
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 ntpq -p
+ timeout -k 5 15 ntpstat
+ timeout -k 5 15 netstat -ontap
+ timeout -k 5 15 netstat -s
+ timeout -k 5 15 rpm -qi diamanti-cx
+ timeout -k 5 15 cat /proc/mounts
+ timeout -k 5 15 ls -l /dev/nvme0
+ timeout -k 5 15 echo -e '\ncommand : dstool -s'
+ timeout -k 5 15 dstool -s
+ timeout -k 5 15 echo -e '\ncommand : dstool -c "mputil -w rs_btp"'
+ timeout -k 5 15 dstool -c 'mputil -w rs_btp'
+ timeout -k 5 15 echo -e '\ncommand : dstool -c "dmesg" | grep "Link is up"'
+ true
+ echo -n .
+ sleep 5
+ timeout -k 5 15 dstool -c dmesg
+ grep 'Link is up'
+ timeout -k 5 15 cat /etc/fstab
+ timeout -k 5 15 cat /etc/resolv.conf
+ timeout -k 5 15 sysctl -a
+ collect_crash_data
+ mkdir -p /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//crash
+ for dir in '/var/crash/*'
++ basename '/var/crash/*'
+ name='*'
+ timeout -k 5 15 cp '/var/crash/*/vmcore-dmesg.txt' '/tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//crash/*-vmcore-dmesg.txt'
+ collect_config_data
+ timeout -k 5 15 cp /etc/systemd/journald.conf /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 cp /etc/etcd/etcd.conf /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 tar -cvzf /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//data.tar.gz /var/lib/diamanti
+ timeout -k 5 15 tar -cvzf /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//config.tar.gz /etc/diamanti
+ timeout -k 5 15 find /etc/diamanti/ -name '*.conf' -exec cp '{}' /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/ ';'
+ collect_network_data
+ timeout -k 5 15 ifconfig -a
+ timeout -k 5 15 ip route list
+ timeout -k 5 15 arp -an
+ printf '\n\n netstat -anevop -t\n'
+ timeout -k 5 15 netstat -anevop -t
+ printf '\n\n  netstat -ian -t\n'
+ timeout -k 5 15 netstat -ian -t
+ printf '\n\n netstat -s -t\n'
+ timeout -k 5 15 netstat -s -t
+ printf '\n\n iptables -L\n'
+ timeout -k 5 15 iptables -L
+ printf '\n\n iptables -vL -t filter\n'
+ timeout -k 5 15 iptables -vL -t filter
+ printf '\n\n iptables -vL -t nat\n'
+ timeout -k 5 15 iptables -vL -t nat
+ printf '\n\n iptables -vL -t mangle\n'
+ timeout -k 5 15 iptables -vL -t mangle
+ printf '\n\n iptables -vL -t raw\n'
+ timeout -k 5 15 iptables -vL -t raw
+ printf '\n\n iptables -vL -t security\n'
+ timeout -k 5 15 iptables -vL -t security
+ printf '\n\n iptables -S -t nat\n'
+ timeout -k 5 15 iptables -S -t nat
+ collect_socket_and_fd_data
+ mkdir -p /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//fds
+ mkdir -p /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//tcp_sockets
+ timeout -k 5 15 lsof
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o armada
+ SVC_PID=22865
+ '[' 22865 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/22865/fd
+ timeout -k 5 15 cat /proc/22865/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o bosun
+ SVC_PID=24426
+ '[' 24426 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24426/fd
+ timeout -k 5 15 cat /proc/24426/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o convoy
+ SVC_PID=24039
+ '[' 24039 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24039/fd
+ timeout -k 5 15 cat /proc/24039/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-apiserver
+ SVC_PID=24040
+ '[' 24040 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24040/fd
+ timeout -k 5 15 cat /proc/24040/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-scheduler
+ SVC_PID=24042
+ '[' 24042 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24042/fd
+ timeout -k 5 15 cat /proc/24042/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kubelet
+ SVC_PID=24050
+ '[' 24050 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24050/fd
+ timeout -k 5 15 cat /proc/24050/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-controller
+ SVC_PID=24041
+ '[' 24041 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24041/fd
+ timeout -k 5 15 cat /proc/24041/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o etcd
+ SVC_PID=23795
+ '[' 23795 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/23795/fd
+ timeout -k 5 15 cat /proc/23795/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dockerd-current
+ SVC_PID=6974
+ '[' 6974 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/6974/fd
+ timeout -k 5 15 cat /proc/6974/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o container-storage-setup
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dock
+ SVC_PID=22866
+ '[' 22866 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/22866/fd
+ timeout -k 5 15 cat /proc/22866/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o postgres
+ SVC_PID=24650
+ '[' 24650 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24650/fd
+ timeout -k 5 15 cat /proc/24650/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o ntpd
+ SVC_PID=10042
+ '[' 10042 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/10042/fd
+ timeout -k 5 15 cat /proc/10042/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-proxy
+ SVC_PID=24293
+ '[' 24293 '!=' '' ']'
+ timeout -k 5 15 ls -al /proc/24293/fd
+ timeout -k 5 15 cat /proc/24293/net/tcp
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o dnsmasq-nanny
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o kube-dns
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o upgrade
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ for svc in '$HOST_SERVICES_PROCESS'
++ pgrep -x -o crio
+ SVC_PID=
+ '[' '' '!=' '' ']'
+ collect_nic_data
+ '[' -d /sys/bus/pci/drivers/cxgb4 ']'
+ timeout -k 5 15 cp /lib/firmware/cxgb4/t5-config.txt /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
+ timeout -k 5 15 cp /lib/firmware/cxgb4/t6-config.txt /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files/
++ ls /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.0/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -i mgmtpf1,0
+ printf '\n\n ethtool -k %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -k mgmtpf1,0
+ printf '\n\n ethtool -S %s\n' mgmtpf1,0
+ timeout -k 5 15 ethtool -S mgmtpf1,0
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.1/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -i mgmtpf1,1
+ printf '\n\n ethtool -k %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -k mgmtpf1,1
+ printf '\n\n ethtool -S %s\n' mgmtpf1,1
+ timeout -k 5 15 ethtool -S mgmtpf1,1
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.2/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -i mgmtpf1,2
+ printf '\n\n ethtool -k %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -k mgmtpf1,2
+ printf '\n\n ethtool -S %s\n' mgmtpf1,2
+ timeout -k 5 15 ethtool -S mgmtpf1,2
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.3/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -i mgmtpf1,3
+ printf '\n\n ethtool -k %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -k mgmtpf1,3
+ printf '\n\n ethtool -S %s\n' mgmtpf1,3
+ timeout -k 5 15 ethtool -S mgmtpf1,3
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -i /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ printf '\n\n ethtool -k %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -k /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ printf '\n\n ethtool -S %s\n' /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ timeout -k 5 15 ethtool -S /sys/bus/pci/drivers/cxgb4/0000:81:00.4/net:
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4
+ timeout -k 5 15 ethtool -i ens801f4
+ printf '\n\n ethtool -k %s\n' ens801f4
+ timeout -k 5 15 ethtool -k ens801f4
+ printf '\n\n ethtool -S %s\n' ens801f4
+ timeout -k 5 15 ethtool -S ens801f4
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -i ens801f4d1
+ printf '\n\n ethtool -k %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -k ens801f4d1
+ printf '\n\n ethtool -S %s\n' ens801f4d1
+ timeout -k 5 15 ethtool -S ens801f4d1
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -i ens801f4d2
+ printf '\n\n ethtool -k %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -k ens801f4d2
+ printf '\n\n ethtool -S %s\n' ens801f4d2
+ timeout -k 5 15 ethtool -S ens801f4d2
+ for i in '`ls /sys/bus/pci/drivers/cxgb4/*/net`'
+ printf '\n\n ethtool -i %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -i ens801f4d3
+ printf '\n\n ethtool -k %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -k ens801f4d3
+ printf '\n\n ethtool -S %s\n' ens801f4d3
+ timeout -k 5 15 ethtool -S ens801f4d3
+ printf '\n\n cat /sys/kernel/debug/cxgb4/*/tp_stats \n' ens801f4d3
+ timeout -k 5 15 cat /sys/kernel/debug/cxgb4/0000:81:00.4/tp_stats
+ printf '\n\n cat /sys/kernel/debug/cxgb4vf/*/sge_qstats \n' ens801f4d3
+ timeout -k 5 15 cat /sys/kernel/debug/cxgb4vf/0000:81:01.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:01.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:02.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:03.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:04.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:05.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:06.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:07.7/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.0/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.1/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.2/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.3/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.4/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.5/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.6/sge_qstats /sys/kernel/debug/cxgb4vf/0000:81:08.7/sge_qstats
+ collect_storage_data
+ printf '\n\n lsblk -l\n'
+ timeout -k 5 15 lsblk -l
+ printf '\n\n lsblk -t\n'
+ timeout -k 5 15 lsblk -t
+ timeout -k 5 15 iostat -x 1 20
+ grep nvme
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ collect_services_data
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status armada
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u armada -b -l
+ true
+ echo -n .
+ sleep 5
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status bosun
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u bosun -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status convoy
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u convoy -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status apiserver
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u apiserver -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status scheduler
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u scheduler -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kubelet
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kubelet -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status controller-manager
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u controller-manager -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status etcd
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u etcd -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status docker
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u docker -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status docker-storage-setup
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u docker-storage-setup -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status dock
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u dock -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status postgres
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u postgres -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status ntpd
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u ntpd -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status proxy
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u proxy -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kube-dnsmasq
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kube-dnsmasq -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status kubedns
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u kubedns -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status upgrade.service
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u upgrade.service -b -l
+ for service in '$HOST_SERVICES_SYSTEMD'
+ timeout -k 5 15 systemctl -l status crio
+ printf '\n\n'
+ timeout -k 5 180 journalctl -u crio -b -l
+ timeout -k 5 180 journalctl -b -l
+ collect_kvstore_data
+ timeout -k 5 15 curl --silent -X GET http://127.0.0.1:12345/api/v1/cluster
++ grep master /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/dws//cluster.log
++ awk ' { print $2 } '
++ sed 's/[", ]//g'
++ head -n 1
+ master=appserv53
+ '[' -z appserv53 ']'
++ timeout -k 5 15 curl -k --write-out '%{http_code}' --silent --output /dev/null http://appserv53:2379/v2/members
+ response_code=200
+ test 200 -eq 200
+ timeout -k 5 15 etcdctl -C http://appserv53:2379 cluster-health
+ sed 's/-----BEGIN RSA PRIVATE KEY-----.*-----END RSA PRIVATE KEY-----/Hidden by Diamanti/g'
+ ETCDCTL_API=3
+ timeout -k 5 15 etcdctl get --endpoints=appserv53:2379 --prefix / --sort-by=KEY
+ '[' 0 -eq 0 ']'
+ '[' -s /dev/shm/diamanti-shm ']'
+ timeout -k 5 15 cat /dev/shm/diamanti-shm
+ python -m json.tool
+ collect_container_runtime_data
+ mkdir -p /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//runtime
+ timeout -k 5 15 docker ps -a
++ cat /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//runtime/docker-ps.log
++ grep -v 'CONTAINER ID'
++ cut -d ' ' -f 1
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 6278bca0b311
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 27864534b2d9
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 72786d3afcb2
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 8fb23946e3a6
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect b18e4a7fe369
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect fdfd6335e4b2
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 0e5a328d1a21
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 60b402984740
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 3630cddccd4e
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 2de446ade96b
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect e064bcfb4891
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect e28f75150f09
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect f617e3bcad47
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 6441e56f6a99
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 9f31fba5af02
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 55ce55e3bf36
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect 37a9db19e05e
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect dee189d43ed6
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect e8635b623e26
+ for id in '`cat $ts_dir_host/runtime/docker-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 docker inspect b857663495e1
+ timeout -k 5 15 crictl ps -a
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
++ cat /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//runtime/crictl-ps.log
++ grep -v 'CONTAINER ID'
++ cut -d ' ' -f 1
+ for id in '`cat $ts_dir_host/runtime/crictl-ps.log | grep -v  "CONTAINER ID" | cut -d " " -f 1`'
+ timeout -k 5 15 crictl inspect 'time="2019-11-13T09:24:52-08:00"'
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ collect_kubernetes_data
+ mkdir -p /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//kubernetes
+ for obj in '$K8S_OBJECTS'
+ '[' pv == pvc ']'
+ '[' pv == services ']'
+ '[' pv == endpoints ']'
+ '[' pv == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get pv --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' pvc == pvc ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get pvc --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
No resources found.
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get pvc --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' sc == pvc ']'
+ '[' sc == services ']'
+ '[' sc == endpoints ']'
+ '[' sc == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get sc --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 20 ))
+ kubectl describe sc best-effort
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc high
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc medium
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-best-effort-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-high-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node0
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node1
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-1-node2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-2
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc sc-medium-ext4-3
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc snapshot-promoter
+ (( i++ ))
+ (( i < 20 ))
+ kubectl describe sc snapshot-promoter-backup
+ (( i++ ))
+ (( i < 20 ))
+ for obj in '$K8S_OBJECTS'
+ '[' volumesnapshots == pvc ']'
+ '[' volumesnapshots == services ']'
+ '[' volumesnapshots == endpoints ']'
+ '[' volumesnapshots == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get volumesnapshots --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
No resources found.
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get volumesnapshots --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' volumesnapshotdata == pvc ']'
+ '[' volumesnapshotdata == services ']'
+ '[' volumesnapshotdata == endpoints ']'
+ '[' volumesnapshotdata == volumesnapshots ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get volumesnapshotdata --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
No resources found.
+ (( i = 0 ))
+ (( i < 0 ))
+ for obj in '$K8S_OBJECTS'
+ '[' services == pvc ']'
+ '[' services == services ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get services --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get services --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 12 ))
+ kubectl describe services kubernetes -n default
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services alertmanager-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services collectd-svc -n diamanti-system
+ true
+ echo -n .
+ sleep 5
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-attacher -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-provisioner -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-resizer -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services csi-external-snapshotter -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services prometheus-svc -n diamanti-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services coredns -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services helm-chart -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services metrics-server -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ kubectl describe services tiller-deploy -n kube-system
+ (( i++ ))
+ (( i < 12 ))
+ for obj in '$K8S_OBJECTS'
+ '[' endpoints == pvc ']'
+ '[' endpoints == services ']'
+ '[' endpoints == endpoints ']'
+ nameList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $2}'`)
++ kubectl get endpoints --all-namespaces
++ grep -v NAME
++ awk '{print $2}'
+ nsList=(`kubectl get $obj --all-namespaces | grep -v "NAME" | awk '{print $1}'`)
++ kubectl get endpoints --all-namespaces
++ grep -v NAME
++ awk '{print $1}'
+ (( i = 0 ))
+ (( i < 14 ))
+ kubectl describe endpoints kubernetes -n default
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints alertmanager-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints collectd-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-attacher -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-provisioner -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-resizer -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints csi-external-snapshotter -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints prometheus-svc -n diamanti-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints coredns -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints helm-chart -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints kube-controller-manager -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints kube-scheduler -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints metrics-server -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ kubectl describe endpoints tiller-deploy -n kube-system
+ (( i++ ))
+ (( i < 14 ))
+ collect_pod_logs
++ hostname
+ host=appserv53
+ mkdir -p /tmp/tmp.uabU2KzSgC/appserv53-dw-tech-support-2019.11.13-09.23.20/host-files//pod_logs
+ timeout -k 5 15 kubectl get pods --all-namespaces '-o=jsonpath={range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{",  "}{end}{end}'
+ sort
+ for namespace in '$NS'
++ kubectl get pods --namespace diamanti-system --output=custom-columns=NAME:.metadata.name
++ grep -v NAME
++ grep -v 'No resources found'
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod alertmanager-0 --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-55txt --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-67bbn --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod collectd-v0.8-mnw9f --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n collectd-v0.8-mnw9f   4/4     Running   0          13m   172.16.6.153   appserv53   <none>           <none> ]]
++ kubectl get pod collectd-v0.8-mnw9f --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-mnw9f --container=cadvisor --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-mnw9f --container=collectd-exporter --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-mnw9f --container=node-exporter --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs collectd-v0.8-mnw9f --container=collectd-es --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-6zknb --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-rzzjh --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n csi-diamanti-driver-rzzjh   2/2     Running   2          13m   172.16.6.153   appserv53   <none>           <none> ]]
++ kubectl get pod csi-diamanti-driver-rzzjh --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-diamanti-driver-rzzjh --container=node-driver-registrar --namespace=diamanti-system -p=true
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-diamanti-driver-rzzjh --container=diamanticsidriver --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-diamanti-driver-v7q2s --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-attacher-6bbc9d4bbd-gclkl --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n csi-external-attacher-6bbc9d4bbd-gclkl   1/1     Running   0          13m   172.20.0.4   appserv53   <none>           <none> ]]
++ kubectl get pod csi-external-attacher-6bbc9d4bbd-gclkl --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-external-attacher-6bbc9d4bbd-gclkl --container=csi-attacher --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-provisioner-957ff6577-68f2d --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-resizer-9848cdf68-wrgs8 --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n csi-external-resizer-9848cdf68-wrgs8   1/1     Running   0          13m   172.20.0.3   appserv53   <none>           <none> ]]
++ kubectl get pod csi-external-resizer-9848cdf68-wrgs8 --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-external-resizer-9848cdf68-wrgs8 --container=csi-external-resizer --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod csi-external-snapshotter-8c8959567-ds7s4 --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n csi-external-snapshotter-8c8959567-ds7s4   1/1     Running   0          13m   172.20.0.2   appserv53   <none>           <none> ]]
++ kubectl get pod csi-external-snapshotter-8c8959567-ds7s4 --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ true
+ echo -n .
+ sleep 5
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs csi-external-snapshotter-8c8959567-ds7s4 --container=csi-external-snapshotter --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-0 --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-1 --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod prometheus-v1-2 --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n prometheus-v1-2   1/1     Running   0          13m   172.16.6.153   appserv53   <none>           <none> ]]
++ kubectl get pod prometheus-v1-2 --namespace diamanti-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs prometheus-v1-2 --container=prometheus --namespace=diamanti-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod provisioner-7b58589b9d-ghzcm --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod snapshot-controller-66fb5f8fbd-j9qq5 --namespace diamanti-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for namespace in '$NS'
++ kubectl get pods --namespace kube-system --output=custom-columns=NAME:.metadata.name
++ grep -v NAME
++ grep -v 'No resources found'
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-6qpcz --namespace kube-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-jtj2f --namespace kube-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod coredns-665c8b6494-p2pjv --namespace kube-system -o wide
++ grep appserv53
+ [[ -n coredns-665c8b6494-p2pjv   1/1     Running   0          13m   172.16.6.153   appserv53   <none>           <none> ]]
++ kubectl get pod coredns-665c8b6494-p2pjv --namespace kube-system -o 'jsonpath={range .spec.containers[*]}{.name}{" "}{end}'
+ for container in '$(kubectl get pod $pod --namespace $namespace -o jsonpath='\''{range .spec.containers[*]}{.name}{" "}{end}'\'')'
+ timeout -k 5 15 kubectl logs coredns-665c8b6494-p2pjv --container=coredns --namespace=kube-system -p=true
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod helm-chart-687577f867-hlflj --namespace kube-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ grep appserv53
++ kubectl get pod metrics-server-v1-5d46b6d959-zlskw --namespace kube-system -o wide
+ [[ -n '' ]]
+ for pod in '$(kubectl get pods --namespace $namespace --output=custom-columns=NAME:.metadata.name | grep -v NAME | grep -v '\''No resources found'\'')'
++ kubectl get pod tiller-deploy-5668df8bc4-8nklg --namespace kube-system -o wide
++ grep appserv53
+ [[ -n '' ]]
+ collect_runtime_data
+ '[' true == '' ']'
+ for obj in '$DWS_OBJECTS'
+ '[' node == event ']'
+ timeout -k 5 15 dctl -o json node list
+ for obj in '$DWS_OBJECTS'
+ '[' network == event ']'
+ timeout -k 5 15 dctl -o json network list
+ for obj in '$DWS_OBJECTS'
+ '[' volume == event ']'
+ timeout -k 5 15 dctl -o json volume list
+ for obj in '$DWS_OBJECTS'
+ '[' snapshot == event ']'
+ timeout -k 5 15 dctl -o json snapshot list
+ for obj in '$DWS_OBJECTS'
+ '[' perf-tier == event ']'
+ timeout -k 5 15 dctl -o json perf-tier list
+ for obj in '$DWS_OBJECTS'
+ '[' event == event ']'
+ timeout -k 5 15 dctl -o json event list -l 1000
+ for obj in '$DWS_OBJECTS'
+ '[' user == event ']'
+ timeout -k 5 15 dctl -o json user list
+ for obj in '$DWS_OBJECTS'
+ '[' endpoint == event ']'
+ timeout -k 5 15 dctl -o json endpoint list
+ for obj in '$DWS_OBJECTS'
+ '[' drive == event ']'
+ timeout -k 5 15 dctl -o json drive list
+ for obj in '$DWS_OBJECTS'
+ '[' feature == event ']'
+ timeout -k 5 15 dctl -o json feature list
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get pv --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get pvc --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get sc --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get volumesnapshots --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get volumesnapshotdata --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get services --all-namespaces
+ for obj in '$K8S_OBJECTS'
+ timeout -k 5 15 kubectl -o json get endpoints --all-namespaces
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user group list
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user role list
+ for obj in '$DWS_USER_OBJECTS'
+ timeout -k 5 15 dctl -o json user auth-server list
+ kill -13 50719
+ echo ''
+ echo -n 'Packaging tech support information '
+ pb_pid=57288
+ pushd /tmp/tmp.uabU2KzSgC
+ progress_bar
+ chmod -R a+rX,u+w .
+ true
+ echo -n .
+ sleep 5
+ tar -cvf /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar appserv53-dw-tech-support-2019.11.13-09.23.20
+ xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ true
+ echo -n .
+ sleep 5
+ popd
+ kill -13 57288
+ echo ''
+ cleanup_tech_support_dirs
+ rm -rf /tmp/tmp.uabU2KzSgC
+ cleanup_max_techsupport_files
++ wc -l
++ ls -l /data/techsupport/appserv53-dw-tech-support-2019.11.06-19.48.02.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-00.07.40.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-05.58.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-12.13.31.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-12.16.14.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-15.56.30.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-15.58.59.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.09-13.22.02.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.09-20.00.07.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-05.51.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-01.50.24.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-06.04.23.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-08.36.27.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-20.24.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-05.50.43.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-08.19.22.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-20.48.17.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-00.52.17.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-05.43.54.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar.xz
+ '[' 31 -gt 30 ']'
+ head -1
+ xargs rm -f
+ ls -tr /data/techsupport/appserv53-dw-tech-support-2019.11.06-19.48.02.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-00.07.40.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-05.58.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-12.13.31.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-12.16.14.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-15.56.30.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-15.58.59.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.09-13.22.02.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.09-20.00.07.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-05.51.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-01.50.24.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-06.04.23.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-08.36.27.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-20.24.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-05.50.43.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-08.19.22.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-20.48.17.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-00.52.17.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-05.43.54.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar.xz
++ wc -l
++ ls -l /data/techsupport/appserv53-dw-tech-support-2019.11.07-00.07.40.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-05.58.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-12.13.31.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-12.16.14.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-15.56.30.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-15.58.59.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.07-20.02.27.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-05.56.53.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-09.45.01.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.08-10.55.03.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.09-13.22.02.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.09-20.00.07.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-05.51.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-12.28.31.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-12.31.29.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.10-20.04.22.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-01.50.24.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-06.04.23.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-08.36.27.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-08.39.34.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-18.23.24.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.11-20.24.32.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-05.50.43.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-08.19.22.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-19.24.12.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.12-20.48.17.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-00.52.17.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-05.43.54.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.20.36.tar.xz /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar.xz
+ '[' 30 -gt 30 ']'
+ rm -f /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.in_progress
+ '[' '!' -f /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar.xz ']'
+ chown diamanti:diamanti /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar.xz
+ echo 'Tech support collection done, available at: /data/techsupport/appserv53-dw-tech-support-2019.11.13-09.23.20.tar.xz'


***** Tech support logs are stored in /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/bin/jenkins/2019-11-13T09-11-17 directory *****

DCTL_CONFIG  was exported.
Taking .dctl.d from /var/lib/jenkins/sanity/auto_tb7 
Creating file :  config-files-2019.11.13-09.25.50.tar.gz


***** Config files of dctl and kubectl are stored in /var/lib/jenkins/workspace/build_sanity/P17-GA-2.3.0-NYNJ-e2e/diamanti-test-pkg/bin/jenkins/2019-11-13T09-11-17 directory *****

Collecting pod description and logs for all the pods ...

***** Copied pod description and logs from all pods in pod_description_and_logs.tar.gz file *****



Copying e2e logs to 2019-11-13T09-11-17 directory 



Copying logs to testserver3 

Permission denied, please try again.
Permission denied, please try again.
Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
lost connection

Logs are copied on testserver3 at /scratch/bugs/2019-11-13T09-11-17 location

+ echo 'Updating google sheets with the results'
Updating google sheets with the results
+ cd /var/lib/jenkins/workspace/build_sanity/Update_Spreadsheets_Project/python_spreadsheets/app/
+ ./get_latest_log_file.sh P17-GA-2.3.0-NYNJ-e2e
